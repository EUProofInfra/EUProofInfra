\paragraph{State of the art.}
Formal proofs and systems manipulating such proofs have become
central tools both in safety and security, as shown by several major
successes: the correctness proof of the Paris metro line 14, the NASA
detect-and-avoid system for unmanned aircraft system proved correct in
PVS, the proved operating system seL4 \cite{Klein09}, and the proved C
compiler CompCert \cite{Leroy06}.  Also in the realm of pure mathematics,
formal proofs have convinced us of the
correctness of several complex proofs, such as those of the Feit-Thompson
theorem \cite{Gonthier13}, Hales' theorem
\cite{Hales17}, etc.  The development of these formal proofs has led
to the construction of huge libraries, totalling millions of hours of
work, that cover a significant part of mankind's mathematical knowledge.

Software development has always been accompanied with the definition
of standards, that make systems interoperable and data
sustainable. For example, web browsers are interoperable and websites
are sustainable because they all comply with web standards such as HTML
and JavaScript. The
area of formal proofs is however an exception. So, while we had in the
past an (informal) proof of Pythagoras' theorem or Fermat's little
theorem, the same proof now has different formalizations in {\sc Coq},
{\sc Matita}, {\sc HOL Light}, etc. This lack of standards is the
major weakness of this area, as it jeopardizes the usability and the
sustainability of these libraries. Indeed, each library is specific to
a proof system, or often even to a specific version of this
system. In general, a library developed in one system cannot be used
in another, and when the system is no longer maintained, the library may
disappear. Being a major weakness, designing a standard for formal
proofs is also a major challenge.

On more philosophical grounds, this lack of a standard jeopardizes the
universality of logical truth, just like non-Euclidean geometries
did in the 19th century, as some statements could be true in one
geometry, but false in others.  This lack of universality severely
limits the spreading of formal proofs in non-specialist
communities. For instance, teaching formal proving to undergraduate
students in a logic course is difficult, as it
requires the choice of a specific language, a specific theory and a specific system that
orients the course towards this language, theory, and
system. The same is true for the use of formal proofs in
industry or by working mathematicians. So another goal of this project
is to make formal proofs accessible to a much larger community, as
standards often do.

At the beginning of the 20th century, a solution was found to
the crisis of non-Euclidean geometries: the definition of the various
geometries in predicate logic \cite{HilbertAckermann}
restore the universality of mathematical truth, allowing to determine
which axiom was used in which proof and which theorem held in
which geometry.

In the same way, we defend the thesis that the interoperability of
proof systems can only be achieved if we are able to express the theories
implemented in these proof systems in a common logical framework.

In 1928, predicate logic, the first logical framework in the history
of logic, was a huge success, since three important theories used at
that time (geometry, arithmetic and set theory) could be expressed in
it. But it also has limitations, which explains that another of the major
theories used at that time (Russell's type theory, from The Principia
Mathematica) can not be expressed in it. Since then, several other
versions of type theory, such as Church's type theory \cite{Church40},
Martin-L\"of's type Theory \cite{Martin-Lof84}, and the Calculus of
constructions \cite{CoquandHuet88}, have also been defined as
autonomous theories. These theories are the ones implemented
in most of the current proof systems, yet they can not be expressed
in predicate logic.

This failure has led, in the field of proof systems, to the abandonment
predicate logic, and even the concept of logical framework: the
theories implemented in {\sc Coq}, {\sc Matita}, {\sc HOL Light},
etc. are often defined as autonomous systems, and not in a logical
framework.

However, a different line of research has attempted to understand the
limitations of predicate logic and to propose other logical frameworks
repairing them. The most prominent limitations of predicate logic are
the lack of function symbols binding variables, the lack of a syntax
for proof terms, the lack of a notion of computation, the lack of a
notion of cut for axiomatic theories, and the impossibility to express
constructive proofs. These limitations have led to the development of
logical frameworks such as $\lambda$-Prolog \cite{NadathurMiller88,
  MillerNadathur12}, Isabelle \cite{Paulson90}, the $\lambda
\Pi$-calculus \cite{HarperHonsellPlotkin91} (also called the
``Edinburgh logical framework''), Deduction modulo theory
\cite{DowekHardinKirchner03, DowekWerner03}, Pure Type Systems
\cite{Berardi88,Terlouw89}, and ecumenical logics
\cite{Prawitz15,Dowek15,PereiraRodriguez17}. The $\lambda
\Pi$-calculus modulo theory \cite{CousineauDowek07}, implemented in
the system {\sc Dedukti} \cite{Assaf16}, is a synthesis of these
frameworks. It not only allows the expression of geometry, arithmetic
and set theory, but also that of Russell's type theory, Church's type
theory, Martin-L\"of's type theory, and the Calculus of constructions.

In the years 2010--2015, it was shown that theories implemented in {\sc
HOL Light} \cite{Assaf12}, {\sc Matita} \cite{Assaf15}, and {\sc
FoCaLiZe} \cite{Cauderlier16} could be expressed in {\sc Dedukti}, and
that the libraries of these systems could be translated to {\sc
Dedukti}, as well as the proofs produced with the automated theorem
proving systems {\sc iProver} \cite{Burel10} and {\sc Zenon}
\cite{CauderlierHalmagrand15}, and by the SMT solver {\sc Archsat}
\cite{Bury19}. In particular, this allowed {\sc Atelier B} proofs
produced by {\sc Zenon} to be expressed in {\sc Dedukti}.

Just like for non-Euclidean geometries, it is not
sufficient to express the theories implemented in {\sc Matita}, {\sc
Hol Light}, etc.  in a common logical framework and to translate the
proofs originally defined in these systems to {\sc Dedukti}: we must
also analyze each proof to understand on which features of the theory
it relies and in which other theories it can be used, a domain traditionally called
``reverse mathematics'' \cite{Friedman76,Simpson09,Dowek17}.


In the years 2015--2020, we started to focus on the translation of
proofs from one library to another \cite{Dowek17,Thire18}. This led us
to propose an on-line system-independent encyclopedia of formal proofs
{\sc Logipedia} ({\tt http://logipedia.science}) in which each proof
is labeled with the axioms and computation rules it uses, indicating
the systems in which it can be used. In particular, we
have showed that the arithmetic library of {\sc Matita} can be
translated into five other, significantly different, systems: {\sc HOL
Light}, {\sc Isabelle}, {\sc PVS}, {\sc Coq}, and {\sc Lean},
preserving the readability of the statements of the theorems.

These successes have convinced us that we are now ready to scale up
and develop this encyclopedia, with the objective to include a
significant part of all formal proofs in four years time, and
in twenty years all of them.

\paragraph{Concept}

The main notions articulated in this project are thus those of
\begin{itemize}
\item logical framework,
\item theory,
\item instrumentation,
\item reverse mathematics,
\item concept alignmenent,
\item and structuring.
\end{itemize}

Suppose we start with a proof developed in a system $X$. The first step
is to express, in the logical framework {\sc Dedukti}, the theory
$D[X]$ implemented in the system $X$. Then, we must instrument the
system $X$ so that the proof can be exported from it, as a piece of
data, expressed as a proof in $D[X]$ and included in {\sc
Logipedia}. Next, we can analyze this proof in order to understand
which features of $D[X]$ it is uses and thus in which alternative
theories it can be expressed.  Finally, we must align its concepts with
the definitions already pressent in {\sc Logipedia} and decide where
it fits in the general structure of the encyclopedia.

Our goals require the expertise of computer scientists,
logicians, mathematicians who are experts on one
theory or one system, and specialists on logical
frameworks, reverse mathematics, and concept alignment.

Such an infrastructure is new in the European Strategy on Research
Infrastructures. We can even say that the idea to structure a research
effort around an infrastructure, such as {\em Software heritage}, is
relatively new in computer science and mathematics.

Depending on the system $X$, we consider more of less research needs
to be done, for instance to design the theory $D[X]$. Among the 15
systems we focus on, we plan to reach level 5 or 6 for 10 of them.
For the others, more research needs to be done and we plan to reach
level 3 only.

\paragraph{Methodology}

The methodology is the same for integrating any library to {\sc Logipedia},
but due to a difference of readiness of the various systems we focus on,
our priorities are different.

\begin{enumerate}
\item We already know to express most of the theories of {\sc Atelier B},
{\sc Coq}, {\sc FoCaLiZe}, {\sc HOL Light}, {\sc HOL4}, {\sc
Isabelle}, {\sc Matita}, and {\sc Rodin} in {\sc Dedukti}. We propose
to instrument these systems so that they can produce {\sc Dedukti}
proofs, that we can include in {\sc Logipedia}.

\item
For other theories, such as those of {\sc Abella}, {\sc Agda}, {\sc
Lean}, {\sc Minlog}, {\sc Mizar}, {\sc PVS}, {\sc TLA+}, the work is
in progress, or not yet started.  So we must first understand how they
can be expressed in {\sc Dedukti}.

\item
Besides the standard libraries of these systems, large libraries
have been developed: the Isabelle Archive of formal Proofs,
Flyspeck, MathComp, CompCert...  We aim to include some of
these libraries in {\sc Logipedia}.
  
\item
Besides proof systems, we also want to include proofs coming from
automated theorem provers, SAT solvers, SMT solvers, and model
checkers.  So we must instrument some of these systems so that they
can produce {\sc Dedukti} proofs, that we can include in {\sc
Logipedia}.

\item
We want to develop algorithms to analyze which symbol, axiom, rewrite
rule is used in each proof, and consequently in which system each proof
can be used.

\item
Each library imported in {\sc Logipedia} will come with its own
definition of natural numbers, real numbers, etc. We want to develop
``concept alignments algorithms'' to transport theorems from one
structure to another isomorphic one.

\item 
Besides data, we propose to include in {\sc Logipedia}, metadata and
an inner structure.
\end{enumerate}


\paragraph{Readiness of the project}

This idea of building such a standard for proofs has already been
investigated in the past, such as in the Qed manifesto \cite{Qed94}, but
have produced limited results.

Our thesis is that, since the
Qed project, the situation has radically changed, because after
thirty years of research, we have an empirical evidence that most of
the formal proofs developed in one of these systems can also be
developed in another, we understand the relationship between the
theories implemented in these systems much better, we have developed
several logical frameworks, extending predicate logic, in which these
theories can be expressed, and we have developed reverse mathematics
algorithms to analyze which axioms and rules are used in each proofs
and algorithms, such as constructivization algorithms, to translate
proofs from one theory to another.


%%% Local Variables:
%%%   mode: latex
%%%   ispell-local-dictionary: "english"
%%% TeX-master: "main"
%%% End:
