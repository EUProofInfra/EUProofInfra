\subsection{Concept}

To make a formal proof, developed in some system $X$, accessible to
the users of other systems, the first step is to express the theory
$D[X]$, implemented in the system $X$, in the logical framework {\sf
  Dedukti}.  Then, we must instrument the system $X$ so that the proof
can be exported from it, as a piece of data, expressed as a proof in
$D[X]$ and included in {\sf Logipedia}. Next, we need to analyze this
proof in order to determine which symbols, axioms and rewrite rules of
$D[X]$ it actually uses and, thus, in which alternative theories it
can be expressed.  Finally, we must align its concepts with the
definitions already present in {\sf Logipedia} and decide where it
fits in the general structure of the encyclopedia.

Depending on the system $X$ we consider, more or less research needs
to be done, for instance to design the theory $D[X]$. Among the 15
systems we focus on, we plan to reach level 5 or 6 for 10 of them.
For the others, more research needs to be done and we plan to reach
level 3 only.

These goals require the expertise of computer scientists, logicians,
mathematicians who are experts on one theory or one system, and
specialists on logical frameworks, reverse mathematics, and concept
alignment.

\subsubsection{Description of work package 1: Instrumentation}

Consider a system $X$ at LIL level 2, so the basic theory $D[X]$ has
already been encoded in Dedukti. The next step is to implement a
method to automatically translate proofs from system $X$ to its
encoding in Dedukti, and make these proofs available in Logipedia so
they can be exported to other systems. This step is called the
\emph{instrumentation} of system $X$ and is the main goal of this work
package.

The systems considered in this work package fall into three broad
classes:
\begin{description}

  \item[Systems based on dependent type theory.] Agda, Coq, and Matita
  are theorem provers based on Martin-L\"of's intuitionistic type
  theory.

  \begin{itemize}
    \item Coq is an interactive theorem prover developed at Inria
    since the 1984.  It is based on Type Theory and was used to
    formally verify the correctness of both industrially relevant
    software such as the CompCert C compiler and complex mathematical
    proofs such as the one of the Four Color theorem and the one of
    the Odd Order theorem. In 2013 Coq received the ACM system award.

    \item Matita is an interactive theorem prover developed at the
    University of Bologna and used for teaching logic courses and to
    verify software and mathematical proofs, with special attention to
    predicative foundations. The first generation of the system (up to
    version 0.5.9) was born as a by-product of the MoWGLI FET-Open
    Project, it was compatible with the logic of Coq and it could
    re-use its libraries. It was an important test-bench for the
    integration of Mathematical Knowledge Management techniques with
    Interactive Theorem Proving, featuring for example a library of
    theorems distributed over multiple servers, innovative indexing
    and search techniques and automatic translation of proofs between
    declarative and procedural styles. The second generation of the
    system (up to the current version 0.99.3) was a re-implementation
    from scratch that departed from the logic of Coq and that
    experimented with the most concise ways to implement an efficient
    theorem prover. Several ideas later migrated into Coq. The
    currently available largest library is the formal certification of
    a complexity-preserving and cost-model-inducing compiler from C to
    MCS-51 machine code, developed in the FET project CerCo (Certified
    Complexity).

    \item Agda is a dependently typed programming language and
    interactive proof assistant developed at Chalmers University of
    Technology as well as other places in Europe. The theory of Agda
    is similar to Coq and Matita, but is more focused on interactive
    development and direct manipulation of proof terms (in contrast to
    using a tactic language to generate the proof terms). To support
    the construction of proof terms, Agda provides powerful features
    such as dependent pattern and copattern matching, eta equality for
    functions and record types, first-class universe polymorphism, and
    definitional proof irrelevance. In addition, Agda provides an
    experimental option for extending the language with user-defined
    rewrite rules, which are very similar to the rewrite rules
    provided by Dedukti.
  \end{itemize}

  \item[Systems based on higher-order logic.] (HOL4, Isabelle) We
  still need some text to go here!

  The HOL4 proof assistant is home to a few medium to large scale
  specifications and associated proof developments that have value
  outside of HOL4. These specifications include the formal semantics
  of the CakeML language (and its verified compiler) and an extensive
  specification of the ARM instruction set architecture (ISA) as
  formalised by Anthony Fox at the University of Cambridge.

  Isabelle as a logical framework \cite{paulson700} is an intermediate
  between Type-Theory provers (like Coq or Agda) and classic LCF-style
  systems (like HOL Light or HOL4).

  \item[Systems based on set theory and first-order logic.] Atelier B,
  Rodin and ProB are platforms or tools to develop models written
  using the B method, a first-order language based on set theory. A
  model in these systems encodes a state machine constrained by
  invariant properties. Verification of the model correctness implies
  to verify some proof obligations produced by a weakest precondition
  calculus. For example, spanning trees algorithms, distributed
  algorithms, access control policies have been formalized
  respectivement in EventB and B method.

  The development process for the B method is based on formal proof:
  proof obligations are automatically generated and must be proven by
  automatic or interactive provers. This can include external solvers
  such as SMT solvers, and (in the case of ProB) constraint solvers
  such as Sicstus Prolog. The difference between Atelier B and Rodin
  lies mainly in the refinement process they implement: in the B
  method used by Atelier B refinement is done by deriving a program,
  while in Event-B used by Rodin refinement is done by defining a
  model of the system and iteratively introducing details. Finally,
  ProB is an animator and model checker: it helps users to gain
  confidence in their specifications, and it can be used as a
  disprover to discover counter-examples to proof obligations.

\end{description}
Despite the large differences between these three classes of provers,
there are several common points between them where solutions that work
for one system can also be applied to other systems.
%
Concretely, in this work package we plan to tackle the following
research challenges:
\begin{description}

  \item[Improved encodings] Since Dedukti is designed to be a small
  logical framework with a minimal number of features, there are many
  features in the systems we consider that are not supported in
  Dedukti. Instead, such features need to be encoded in some way. In
  this workpackage, we consider systems for which it is known how to
  encode all features \emph{in theory} (see WP2 where we also consider
  systems for which this is not yet the case). However, depending on
  the choice of encoding, the instrumentation may be very hard or even
  unfeasible in practice. For example:
  \begin{itemize}

    \item Many Agda libraries (as well as some Coq libraries) rely
    heavily on type-directed conversion rules such as eta-equality for
    functions and record types, or definitional proof
    irrelevance. Dedukti does not provide type-directed conversion
    directly, so instead these rules have to be encoded by adding
    additional type information to the proof terms. This can hence
    lead to a large blow-up in the size of those proof terms, and thus
    greatly increase the cost of typechecking.

    \item Coq, Agda, and Matita all provide support for coinductive
    (infinite) structures, which are not supported natively in
    Dedukti. These structures can be encoded by rewrite rules in
    Dedukti, but this may cause the typechecker to go into an infinite
    loop.

    \item A few of the systems rely on AC rewriting (i.e.~rewriting
    modulo associativity and commutativity of certain operations), for
    example to encode universe polymorphism in dependent type
    theories. While Dedukti has support for AC rewriting, the current
    implementation is very slow, which makes typechecking the proofs
    in Dedukti unfeasible.

  \end{itemize}
  Finding better ways to encode such features could have a great
  impact on the quality and speed of the translation process.  We plan
  to investigate two possible approaches to improve the encodings of
  common features of proof assistants in Dedukti. First, we will
  investigate whether some information in the current encodings is
  redundant and can thus be omitted. Second, if this is not possible
  we will investigate what minimal changes need to be made to Dedukti
  itself in order to overcome these limitations.

  \item[Reconstructing transient proof components] In all theorem
  provers, there is some information that is created during the
  process of checking a proof that is not stored in the final proof
  term. In systems such as Coq, Agda, and Matita, this concerns for
  example the types of each subexpression. In other systems such as
  HOL4, Isabelle, Atelier B, and Rodin, even more information about
  the proof is discarded and only the high-level proof steps remain.
  However, the translation from the system to Dedukti might rely on
  this transient information. Developing a method to reconstruct this
  transient information is thus crucial to the goals of this work
  package.

  In order to instrument systems with transient proof information, the
  proof terms need to be complemented with this transient information
  by either logging or re-synthesizing it on demand. Both approaches
  may be used, depending on the the tradeoff between computation time
  and space for storage.

  However, this approach is insufficent for systems that rely on
  external provers such as Isabelle or Event B. In order to handle
  these cases, we will instrument these external provers so they can
  produce the missing parts of the proof terms (possible interaction
  with WP4).

  \item[Producing more compact proofs] When translating a proof from
  some system $X$ to Dedukti, we want to produce proof terms that are
  as small as possible, so it is easy to store them, recheck them, or
  export them to a different system. However, there are at least two
  reasons why proof terms might become large. First, a typical proof
  in system $X$ might be very large (because big parts of it are
  constructed automatically). Second, translating a proof from $X$ to
  its encoding in Dedukti might greatly increase the size of the
  proof, for example by normalizing it or by adding type annotations.
  This means translating the proofs can take a very long time, and
  doing anything with the translated proof will be difficult. This
  also has an obvious impact on the exporting of complete libraries to
  Dedukti, which is the goal of WP3.

  To reduce the size of proof terms produced by the translation to
  Dedukti, we plan to investigate how to avoid unneccessary
  normalization or duplication of (parts of) proofs. We will also
  investigate what parts of each proof can be safely omitted because
  they can be inferred from the rest of the proof.
\end{description}

For each of the systems considered in this work package, we can
identify some preliminary work on translating proofs to Dedukti. Some
of these tools are in a preliminary stage and can only handle toy
examples, while others have been used to export whole libraries but
have not been updated to work with the latest version of the
system. Concretely, we plan to build upon the previous work done on
the following tools:
\begin{itemize}
  \item The standard and arithmetic libraries of Matita has been the
  first libraries to be exported to Logipedia using Krajono, a fork of
  Matita. The forked system is also actually the only one able to
  import Logipedia proofs. The choice of Matita as a test-bench for
  Logipedia is easily understood considering that the implementation
  of the 0.99.x series was aimed at obtaining a well-documented,
  minimal but fast implementation of a theorem prover, two order of
  magnitudes smaller than Coq.
  \item CoqInE is a prototype tool that can translate Coq proofs to
  Dedukti, which currently covers a small subset of the type theory of
  Coq. However, since Coq is an actively developed system that is
  constantly evolving, CoqInE is now quite outdated and no longer
  works with the latest version of Coq.
  \item In the summer of 2019, Guillaume Genestier worked together
  with Jesper Cockx on the implementation of an experimental
  translator from Agda to Dedukti during a research visit at Chalmers
  University in Sweden. This translator is still work in progress, but
  it is already able to translate 142 modules of the Agda standard
  library (about 25\%) to a form that can be checked in Dedukti. This
  exploratory work uncovered several challenges and opportunities for
  further work (see research challenges above).
  \item The inference kernel of Isabelle has already been instrumented
  to output proofs as $\lambda$-terms that can be understood by
  Dedukti. However, this has so far been only used for small examples
  \cite{Berghofer-Nipkow:2000:TPHOL}. The challenge is to make
  Isabelle proof terms work robustly for the basic libraries and
  reasonably big applications.  Preliminary work by Wenzel (2019) has
  demonstrated the feasibility for relatively small parts of
  Isabelle/HOL, but this requires scaling up.
  \item HOL4 has support for exporting proofs to the OpenTheory proof
  exchange format, and there has been some work on importing
  OpenTheory proofs into Dedukti.
  \item In the context of the BWare project, an encoding of the set
  theory of the B method has been provided as a theory modulo, i.e. a
  rewrite system rather than a set of axioms. This encoding is used by
  the automatic prover Zenon modulo which features a backend to
  Dedukti. Thus, as a first step through instrumentation of Atelier B
  and Rodin, proof obligations coming from Atelier B can be proved by
  Zenon modulo producing Dedukti proofs, hence providing a better
  confidence in the proofs produced by the native proof tools of
  Atelier B \cite{Bware}.
\end{itemize}


\subsubsection{Description of the work package 3}

\subsubsection{Description of the work package 4}

Automatic Theorem Proving (ATP) has a key role in this project.

{\em Per se}, many formal proofs nowadays rely on the use of automatic
provers. They cover various domains and various kinds of application,
{\em e.g.} combinatorial
mathematics~\cite{DBLP:journals/ai/KonevL15,DBLP:conf/sat/HeuleKM16},
where they are expected to solve one large propositional problem, or
proof of
programs~\cite{DBLP:conf/esop/FilliatreP13,DBLP:journals/pacmpl/ProtzenkoZRRWBD17},
where they are given thousands of small problems in a combination of
quantified theories.

In a complementary way, ATPs will be used to automatically make a
coherent whole out of {\sf Logipedia}. A fruitful interaction between
formal proofs requires low-level glue which falls in the scope of ATPs.
For instance, they will be employed to fill the holes that appear when
considering provers with various granularity, to reduce the gaps between
proof systems, and to discharge proofs of concept alignment (in close
interaction with the work package 6).

Finally, ATPs will also benefit from {\sf Logipedia}. Obviously, {\sf
  Logipedia} will be an extensive library of formal statements that can
be used and combined by ATPs in their proof search. This is not trivial
though: lemma selection is crucial to avoid an overhead. It can also be
a source of benchmarks to evaluate their expressivity and automation.
Lastly, {\sf Logipedia} and {\sf Dedukti} will form a framework to make
ATPs cooperate with each other and with other tools, in a safe way.


\paragraph{From ATPs to {\sf Dedukti}}

As for Interactive Theorem Provers, the consistency of this work
strongly relies on the ability for ATPs to export statements and proofs
in some theory in {\sf Dedukti}. We divide this aspect in two tasks.

\dots Pascal: T1 and T2


\paragraph{From {\sf Dedukti} to ATPs}

In the other direction, ATPs will benefit from {\sf Logipedia}.

\dots Chantal and Pascal: T3, T4 and T7


\paragraph{ATPs to increase {\sf Logipedia} readiness}

\dots Chantal: T5 and T6


\subsubsection{Description of the work package 7}

\subsubsection{Description of the work package 2}

\subsubsection{Description of the work package 5 and 6}
>>>>>>> refs/remotes/origin/master

\subsection{Methodology}

The methodology is the same for integrating any library to {\sf Logipedia},
but due to a difference of readiness of the various systems we focus on,
our priorities are different.

\begin{enumerate}
\item We already know how to express most of the theories of {\sf Atelier B},
{\sf Coq}, {\sf FoCaLiZe}, {\sf HOL Light}, {\sf HOL4}, {\sf
Isabelle}, {\sf Matita}, and {\sf Rodin} in {\sf Dedukti}. We propose
to instrument these systems so that they can produce {\sf Dedukti}
proofs that we can include in {\sf Logipedia}.

\item
For other theories, such as those of {\sf Abella}, {\sf Agda}, {\sf
Lean}, {\sf Minlog}, {\sf Mizar}, {\sf PVS}, {\sf TLA+}, the work is
in progress, or not yet started.  So we must first understand how they
can be expressed in {\sf Dedukti}.

\item Besides the standard libraries of these systems, large libraries
  have been developed: the {\sf Isabelle Archive of formal Proofs} \cite{AFP},
  {\sf Flyspeck}\cite{Flyspeck}, {\sf MathComp}\cite{Mathcomp}, 
  {\sf CompCert} \cite{Compcert}, {\sf CakeML} \cite{CakeML}, ...  We aim to include
  some of these libraries in {\sf Logipedia}.
  
\item
Besides proof systems, we also want to include proofs coming from
automated theorem provers, SAT solvers, SMT solvers, and model
checkers.  So we must instrument some of these systems so that they
can produce {\sf Dedukti} proofs that we can include in {\sf
Logipedia}.

\item
We want to develop algorithms to analyze which symbol, axiom, rewrite
rule is used in each proof, and consequently in which system each proof
can be used. We also want to develop algorithms to eliminate some of the 
symbols, axioms, and rewrite rules used in a proof in order to be able to 
use it in more systems.


\item
Each library imported in {\sf Logipedia} will come with its own
definition of natural numbers, real numbers, etc. We want to develop
``concept alignments algorithms'' to transport theorems from one
structure to another isomorphic one.

\item 
Besides data, we propose to include in {\sf Logipedia}, metadata and
an inner structure.
\end{enumerate}

\subsubsection{Methodology of work package 1: instrumentation}

We know how to express in Dedukti the theories implemented in Matita,
HOL4, Coq, Agda, Isabelle and Atelier B, and some of these systems
have been partially instrumented to export proofs that can be checked
in Dedukti. Our first work package is to complete this instrumentation
to be able to export most of the proofs of these systems. As a
consequence, the work includes a strong practical component.

Three methods have to be used here: some of the systems (Automath
style), such as Coq and Agda already have proof-terms that can be
output, thus the main task is to translate these proofs into the
Dedukti format. Others (LCF style), such as Isabelle and HOL4, have an
inference kernel that can be instrumented, the main task here is to
transform the internal proof-object into an external
proof-term. Others, such as Atelier B are slightly more difficult to
address. For those, we need to use the water ford method: extract an
incomplete trace (a sequence of lemmas) and fill the gap using
automated theorem proving, as experimented with Atelier B and Zenon.

% A technological hurdle in the instrumentation of the systems under
% consideration is that all of them are actively developed systems
% that are constantly evolving.

\subsubsection{Methodology of the work package 3} 

\subsubsection{Methodology of the work package 4} 

\subsubsection{Methodology of the work package 7} 

\subsubsection{Methodology of the work package 2} 

\subsubsection{Methodology of the work package 5 + 6} 

\subsubsection{Methodology of the work package 1}

\subsubsection{Methodology of the work package 3}

\subsubsection{Methodology of the work package 4}

\subsubsection{Methodology of the work package 7}

\subsubsection{Methodology of the work package 2}

\subsubsection{Methodology of the work package 5 and 6}


\subsection{Readiness of the project}

This idea of building such a standard for proofs has already been
investigated in the past, such as in the Qed manifesto \cite{Qed94}, but
has produced limited results.

Our thesis is that, since the
Qed project, the situation has radically changed. After
thirty years of research, we have an empirical evidence that most of
the formal proofs developed in one of these systems can also be
developed in another. We understand the relationship between the
theories implemented in these systems much better. We have developed
several logical frameworks, extending predicate logic, in which these
theories can be expressed. And we have developed reverse mathematics
algorithms to analyze which axioms and rules are used in each proofs
and algorithms, such as constructivization algorithms, to translate
proofs from one theory to another.


%%% Local Variables:
%%%   mode: latex
%%%   ispell-local-dictionary: "english"
%%% TeX-master: "propB"
%%% End:
