{\bf \Large (a) Concept}

\begin{framed}
\begin{center}
{\bf \Large How do proofs contribute to safety and security of software?}
\end{center}
  
Imagine the following casino game. At the beginning, a player is given
eleven euros. At each round, she throws a dice. If the results is a
six, then game is over.  If it is a five, a four, a three, or a two,
she is given twice the amount of money she already has. If it is a one
she is taken two euros, if she has at least two.  When the game is
over, the player wins the money she got, except if she has zero, in
which case she looses one million.

This game can be modeled by program $p$
%import random 
\begin{verbatim}
n = 11
stay = True;
while stay: 
    roll = random.randint(1,6)
    if roll == 6:
        stay = False
    else:
        if roll >= 2:
            n = n + 2 * n
        else:
            if (n >= 2):
                n = n - 2
print(n)
\end{verbatim}

To be on the safe side, the player wants to be sure, before starting
playing, that she will never finish with zero.  And indeed, in all runs,
the content of the variable {\tt n} is always an odd number, thus it
cannot be zero. This property is a consequence of two simple
theorems of arithmetic
$$\forall x~(\mbox{\it odd}(x) \Rightarrow \mbox{\it odd}(x + 2 * x))$$
$$\forall x~(\mbox{\it odd}(x) \Rightarrow \mbox{\it odd}(x - 2))$$
%$$\forall x~(\mbox{\it odd}(x) \Rightarrow \mbox{\it odd}(x))$$
Hence, verifying the safety of this program, that is that the
proposition 
${\mbox{\it safe}}(p)$ holds, 
amounts to prove these three theorems of arithmetic.

A tiny bug in the program, for instance replacing the {\tt 2} by a
{\tt 3} in the the line {\tt n = n + 2 * n} makes the program unsafe
as shown by the sequence $11, 9, 7, 5, 3, 1, 4, 2, 0$. Yet, testing
this program will, most likely, not reveal this bug, that manifests
very rarely.  In contrast, attempting to prove the correctness of this
program will
reveal the bug as it is impossible to prove the proposition
$$\forall x~(\mbox{\it odd}(x) \Rightarrow \mbox{\it odd}(x + 3 * x))$$
\end{framed}
\pagebreak
\begin{framed}
  \begin{center}
    {\bf \Large What is a formal proof? What is a proof system?}
    \end{center}
        
Since Antiquity, we have known that
proofs, both purely mathematical ones, as in Euclid's elements or the
recent proof of the Kepler conjecture by Thomas Hales, and proofs used
to establish the safety and security of software, can be built with a
limited number of rules, for example
\begin{itemize}
\item From $A \Rightarrow B$ and $A$, deduce $B$.
\item From $A$, deduce $A~\mbox{\it or}~B$.
\item ...
\end{itemize}
Yet, through history, most mathematical proofs have been written in
pidgin of natural language and mathematical formulas. When proofs are
very long (as it is often of the proofs used in safety and security,
but also of some proofs in pure mathematics), mistakes in proofs are
very difficult to detect. For instance dozen of wrong proofs of
parallel postulate have been given through history, sometimes by the
best mathematicians such as Ptolémée, Proclus, al-Haytam, Tacket,
Clairaut, Legendre...

In the 1960s, Robin Milner and Nicolaas De Bruijn noticed that the
correctness of a mathematical proof could be checked by a
computer. This led to the development of the two first proof systems
in history: Milner's LCF or De Bruijn's Automath.  From
the axioms
$$\forall x~(\mbox{\em philosopher}(x) \Rightarrow \mbox{\em human}(x))$$
$$\forall x~(\mbox{\em human}(x) \Rightarrow \mbox{\em mortal}(x))$$
we can deduce
$$\forall x~(\mbox{\em philosopher}(x) \Rightarrow \mbox{\em mortal}(x))$$
In the language implemented in Automath, this proof is written
$$\lambda x \lambda h~(g~x~(f~x~h))$$
\end{framed}

\begin{framed}
\begin{center}
{\bf \Large What is a theory?}
\end{center}

Deduction rules such as ``From $A \Rightarrow B$ and $A$, conclude
$B$'' are universal, but building proofs require more rules, that are
often specific to a domain of knowledge and are called
``axiom''. Examples are the axioms of geometry, the axioms or
arithmetic. These axioms constitute a theory.

At the beginning of the 20th century an axiomatic theory, {\em set
  theory}, has been proposed to express all mathematical proofs. In
the first half of the 20th century a few variants of set theory, and a
few alternatives have been proposed (such as Simple type theory).
But, because these theories had not been designed for being
implemented on a computer, the rise of computer checked formal proofs
has led to a multiplication of such alternative theories. Each proof system,
such as Coq, Isabelle/HOL, Mizar, Atelier B,
etc. implements its own theory.

This is the major obstacle to interoperability.
\end{framed}

\bigskip

\noindent
{\bf \Large Logical Frameworks}

We had, in the past, an (informal) proof of Pythagoras’ theorem
or Fermat’s little theorem. The same proof now has different
formalizations in PVS, Isabelle/HOL, Coq, etc., jeoprardizing the
universality of logical truth.

In the 19th century, the universality of logical truth has been
jeopardized in a similar way, by non-Euclidean geometries, as some
statements could be true in one geometry, but false in others.  But,
at the beginning of the 20th century, a solution was found.  The
definition of the various geometries in predicate logic
\cite{HilbertAckermann} restored the universality of logical truth,
allowing to determine which axiom was used in which proof and which
theorem held in which geometry.

Predicate logic is not a theory in itself, but it is a framework in
which one can express theories, as sets of axioms: {\em a logical
  framework}.  Expressing the various geometries in predicate logic
allowed to analyze which proof contained which axiom, and hence which
proof could be expressed in which theory.

In 1928, predicate logic was a huge success, since three important
theories used at that time (geometry, arithmetic and set theory) could
be expressed in it. But it also has limitations, which explains that
another of the major theories used at that time (Russell's type
theory, from The Principia Mathematica) has not been expressed in
it. Since then, several other theories, such as Church's type theory
\cite{Church40}, Martin-L\"of's type Theory \cite{Martin-Lof84}, and
the Calculus of constructions \cite{CoquandHuet88}, have also been
defined as autonomous theories. These theories are those implemented
in most of the current proof systems, yet they cannot be expressed in
predicate logic.

This failure has led, in the field of proof systems, to the
abandonment of predicate logic, and even of the concept of logical
framework: the theories implemented in Coq, Matita, HOL Light,
etc. are often defined as autonomous systems, and not in a logical
framework.

However, a different line of research has attempted to understand the
limitations of predicate logic and to propose other logical frameworks
repairing them. The most prominent limitations of predicate logic are
the lack of function symbols binding variables, the lack of a syntax
for proof terms, the lack of a notion of computation, the lack of a
notion of cut for axiomatic theories, and the impossibility to express
constructive proofs. These limitations have led to the development of
logical frameworks such as $\lambda$-Prolog \cite{NadathurMiller88,
  MillerNadathur12}, Isabelle \cite{Paulson90}, the $\lambda
\Pi$-calculus (also called the ``Edinburgh logical framework'')
\cite{HarperHonsellPlotkin91}, Deduction modulo theory
\cite{DowekHardinKirchner03, DowekWerner03}, Pure Type Systems
\cite{Berardi88,Terlouw89}, and ecumenical logics
\cite{Prawitz15,Dowek15,PereiraRodriguez17}.

All these logical frameworks have been unified in the $\lambda
\Pi$-calculus modulo theory \cite{CousineauDowek07}, implemented in
the system Dedukti \cite{Assaf16}
on which Logipedia is based. Geometry, arithmetic and set
theory, but also Russell's type theory, Church's type theory,
Martin-L\"of's type theory, and the Calculus of constructions can be
expressed in this framework.

For instance, although the details are not important, Church's type
theory can be expressed in Dedukti , with 8 déclarations and 3
rewrite rules.
\begin{framed}
$\begin{array}{rcl}
type&:&Type\\
\eta&:&type \rightarrow Type\\
o&:&type\\
\mbox{\it nat}&:&type\\
\mbox{\it arrow}&:&type \rightarrow type \rightarrow type\\
\varepsilon&:&(\eta~o) \rightarrow Type\\
\Rightarrow&:&(\eta~o) \rightarrow (\eta~o) \rightarrow (\eta~o)\\
\forall&:&\Pi a:type~(((\eta~a) \rightarrow (\eta~o)) \rightarrow (\eta~o))\\
\\
(\eta~(\mbox{\it arrow}~x~y)) &\longrightarrow& (\eta~x) \rightarrow (\eta~y)\\
(\varepsilon~(\Rightarrow~x~y)) &\longrightarrow& (\varepsilon~x) \rightarrow (\
\varepsilon~y)\\
(\varepsilon~(\forall~x~y)) &\longrightarrow& \Pi z:(\eta~x)~(\varepsilon~(y~z))
\end{array}$
\end{framed}

So the theories implemented in various systems can all be expressed in
Dedukti and the proofs developed in these systems can be translated to
Dedukti. Just like in the case of non Euclidean geometry, this allows
to analyse the symbols and rewrite rules used in each proof
\cite{Thire18,Dowek17} (a domain traditionally called ``reverse
mathematics'' \cite{Friedman76,Simpson09}) and to deduce in which
systems each proof can be used.  This analysis is the basis of the
interoperability between proof systems.


\bigskip

\noindent
{\bf \Large The Logipedia integration levels}

\medskip

Thus, to make a formal proof, developed in some system $X$, accessible to
the users of other systems, the first step is to express the theory
$D[X]$, implemented in the system $X$, in the logical framework
  Dedukti.  Then, we must instrument the system $X$ so that the proof
can be exported from it, as a piece of data, expressed as a proof in
$D[X]$ and included in Logipedia. Next, we need to analyze this
proof in order to determine which symbols, axioms and rewrite rules of
$D[X]$ it actually uses and, thus, in which alternative theories it
can be expressed.  Finally, we must align its concepts with the
definitions already present in Logipedia and decide where it
fits in the general structure of the encyclopedia.

To measure the level of integration of an existing proof system and
associated proof library in Logipedia, we introduce a metric:
{\em the Logipedia integration levels} (Figure \ref{lil}) that
counts six levels.  

\begin{framed}
\begin{center}
{\bf The Logipedia integration levels (LIL)\label{lil}}
\end{center}

\begin{itemize}
\item[LIL 1:]
The theory implemented in the system has been defined in
the $\lambda\Pi$-calculus modulo theory and in Dedukti.

\item[LIL 2:]
The system has been instrumented so some of its proofs can be exported
and checked in Dedukti.

\item[LIL 3:] A significant part of the library of the system has been
  exported and checked in Dedukti.

\item[LIL 4:] A significant part of the library of the system have
  been made available in Logipedia.

\item[LIL 5:]
A tool has been defined to analyze the Dedukti proofs for the system,
detect those that can be expressed in a theory weaker than that of the
system, and translate those proofs into a weaker logic.

\item[LIL 6:]
All proofs of the system have been exported, translated,
and made available in Logipedia.
\end{itemize}
\end{framed}

The sixteen systems addressed in this project currently have different
integration levels, and we have different targeted levels for them.. 

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
System & current level & targeted level\\
\hline
Matita & 5 & 6\\
\hline
HOL Light & 3 & 5\\
\hline
FoCaLiZe & 3 & 5\\
\hline
Coq & 3 & 5\\
\hline
Agda & 2 & 4\\
\hline
Atelier B & 1 & 5\\
\hline
ProB & 1 & 5\\
\hline
Isabelle & 2 & 5\\
\hline
HOL4 & 1 & 5\\
\hline
TSTP & 1 & {\color{red} ???}\\
\hline
Minlog & 0 & 4\\
\hline
PVS & 0 & 2\\
\hline
Abella & 0 & 2\\
\hline
Mizar & 0 & 4\\
\hline
Smart & 0 & 3\\
\hline
TLA+ & 0 & 2\\
\hline
 Why3 & 0 & {\color{red} ???}\\
\hline
LFSC & 0 & {\color{red} ???}\\
\hline
\end{tabular}
\end{center}

These systems can be roughly divided into two groups. Those in the
first half of the array (Matita, HOL Light, 
  FoCaLize, Coq, Agda, Atelier B, ProB, Isabelle,
HOL4, and TSTP) for which we have preliminary results and
that we plan to bring to a very high level of integration. Those in
the second half (Minlog, PVS, Abella, Mizar,
Smart, TLA+, SMT-Lib, Why3, and LFSC) with which we
are starting and for which our goal are less ambitious.

These two groups of systems will be addressed in different
work packages, but both are key to the project. The first ones will
constitute Logipedia in 2024 and the second ones prepare the
long term future of the infrastructure.

\bigskip

\noindent
{\bf \Large Instrumentation}

\medskip

Consider a system $X$ at LIL 1, so the basic theory $D[X]$ has
already been expressed in Dedukti. The next step is to implement a
method to automatically translate proofs from system $X$ to its
expression in Dedukti, and make these proofs available in Logipedia so
they can be exported to other systems. This step is called the
\emph{instrumentation} of system $X$.

The systems considered in this project fall into three broad classes:
\begin{description}

  \item[Systems based on dependent type theory.] Agda, Coq, and Matita
  are theorem provers based on Martin-L\"of's intuitionistic type
  theory.

  \begin{itemize}
    \item Coq is an interactive theorem prover developed at Inria
    since the 1984.  It is based on Type Theory and was used to
    formally verify the correctness of both industrially relevant
    software such as the CompCert C compiler and complex mathematical
    proofs such as the one of the Four Color theorem and the one of
    the Odd Order theorem. In 2013 Coq received the ACM system award.

    \item Matita is an interactive theorem prover developed at the
    University of Bologna and used for teaching logic courses and to
    verify software and mathematical proofs, with special attention to
    predicative foundations. The first generation of the system (up to
    version 0.5.9) was born as a by-product of the MoWGLI FET-Open
    Project, it was compatible with the logic of Coq and it could
    re-use its libraries. It was an important test-bench for the
    integration of Mathematical Knowledge Management techniques with
    Interactive Theorem Proving, featuring for example a library of
    theorems distributed over multiple servers, innovative indexing
    and search techniques and automatic translation of proofs between
    declarative and procedural styles. The second generation of the
    system (up to the current version 0.99.3) was a re-implementation
    from scratch that departed from the logic of Coq and that
    experimented with the most concise ways to implement an efficient
    theorem prover. Several ideas later migrated into Coq. The
    currently available largest library is the formal certification of
    a complexity-preserving and cost-model-inducing compiler from C to
    MCS-51 machine code, developed in the FET project CerCo (Certified
    Complexity).

    \item Agda is a dependently typed programming language and
    interactive proof assistant developed at Chalmers University of
    Technology as well as other places in Europe. The theory of Agda
    is similar to Coq and Matita, but is more focused on interactive
    development and direct manipulation of proof terms (in contrast to
    using a tactic language to generate the proof terms). To support
    the construction of proof terms, Agda provides powerful features
    such as dependent pattern and copattern matching, eta equality for
    functions and record types, first-class universe polymorphism, and
    definitional proof irrelevance. In addition, Agda provides an
    experimental option for extending the language with user-defined
    rewrite rules, which are very similar to the rewrite rules
    provided by Dedukti.
  \end{itemize}

  \item[Systems based on simple type theory.] (HOL4, Isabelle)
  \ednote{We still need some text to go here!}

  The HOL4 proof assistant is home to a few medium to large scale
  specifications and associated proof developments that have value
  outside of HOL4. These specifications include the formal semantics
  of the CakeML language (and its verified compiler) and an extensive
  specification of the ARM instruction set architecture (ISA) as
  formalised by Anthony Fox at the University of Cambridge.

  Isabelle as a logical framework \cite{paulson700} is an intermediate
  between Type-Theory provers (like Coq or Agda) and classic LCF-style
  systems (like HOL Light or HOL4).

\item[Systems based on set theory and first-order logic.] Atelier B,
  Rodin and ProB are platforms or tools to develop models written
  using the B method, a version of set theory expressed in predicate
  logic.  A model in these systems encodes a state machine constrained
  by invariant properties. Verification of the model correctness
  implies to verify some proof obligations produced by a weakest
  precondition calculus. For example, spanning trees algorithms,
  distributed algorithms, access control policies have been formalized
  respectivement in EventB and B method.

  The development process for the B method is based on formal proof:
  proof obligations are automatically generated and must be proven by
  automatic or interactive provers. This can include external solvers
  such as SMT solvers, and (in the case of ProB) constraint solvers
  such as Sicstus Prolog. The difference between Atelier B and Rodin
  lies mainly in the refinement process they implement: in the B
  method used by Atelier B refinement is done by deriving a program,
  while in Event-B used by Rodin refinement is done by defining a
  model of the system and iteratively introducing details. Finally,
  ProB is an animator and model checker: it helps users to gain
  confidence in their specifications, and it can be used as a
  disprover to discover counter-examples to proof obaligations.

  Smart is based on a classical first-order logic extended with rank-1
  polymorphism, algebraic datatypes, recursive definitions and
  inductive predicates. ProvenTools will turn models implemented in
  Smart to proof obligations which can be proved in the system using a
  mix of interactive and automated proving. The resulting proofs are
  reviewable objects, akin to proof traces built of the various atomic
  proof rules supported by ProvenTools' kernel, such as definition
  unfolding, case analysis or equality propagation.
\end{description}
Despite the large differences between these three classes of provers,
there are several common points between them where solutions that work
for one system can also be applied to other systems.
%
Concretely, we plan to tackle the following
research challenges:
\begin{description}

\item[Improving encodings.] Since Dedukti is designed to be a small
  logical framework with a minimal number of features, there are many
  features in the systems we consider that are not supported in
  Dedukti. Instead, such features need to be encoded in some way. In
  this work package, we consider systems for which it is known how to
  encode all features \emph{in theory}. However, depending on the
  choice of encoding, the instrumentation may be very hard or even
  unfeasible in practice. For example:
  \begin{itemize}

    \item Many Agda libraries (as well as some Coq libraries) rely
    heavily on type-directed conversion rules such as eta-equality for
    functions and record types, or definitional proof
    irrelevance. Dedukti does not provide type-directed conversion
    directly, so instead these rules have to be encoded by adding
    additional type information to the proof terms. This can hence
    lead to a large blow-up in the size of those proof terms, and thus
    greatly increase the cost of typechecking.

    \item Coq, Agda, and Matita all provide support for coinductive
    (infinite) structures, which are not supported natively in
    Dedukti. These structures can be encoded by rewrite rules in
    Dedukti, but this may cause the typechecker to go into an infinite
    loop.

    \item A few of the systems rely on AC rewriting (i.e.~rewriting
    modulo associativity and commutativity of certain operations), for
    example to encode universe polymorphism in dependent type
    theories. While Dedukti has support for AC rewriting, the current
    implementation is very slow, which makes typechecking the proofs
    in Dedukti unfeasible.

  \end{itemize}
  Finding better ways to encode such features could have a great
  impact on the quality and speed of the translation process.  We plan
  to investigate two possible approaches to improve the encodings of
  common features of proof assistants in Dedukti. First, we will
  investigate whether some information in the current encodings is
  redundant and can thus be omitted. Second, if this is not possible
  we will investigate what minimal changes need to be made to Dedukti
  itself in order to overcome these limitations.

  \item[Reconstructing transient proof components.] In all theorem
  provers, there is some information that is created during the
  process of checking a proof that is not stored in the final proof
  term. In systems such as Coq, Agda, and Matita, this concerns for
  example the types of each subexpression. In other systems such as
  HOL4, Isabelle, Atelier B, and Rodin, even more information about
  the proof is discarded and only the high-level proof steps remain.
  However, the translation from the system to Dedukti might rely on
  this transient information. Developing a method to reconstruct this
  transient information is thus crucial to the goals of this work
  package.

  In order to instrument systems with transient proof information, the
  proof terms need to be complemented with this transient information
  by either logging or re-synthesizing it on demand. Both approaches
  may be used, depending on the the tradeoff between computation time
  and space for storage.

  However, this approach is insufficent for systems that rely on
  external provers such as Isabelle or Event B. In order to handle
  these cases, we will instrument these external provers so they can
  produce the missing parts of the proof terms. 

  \item[Producing more compact proofs.] When translating a proof from
  some system $X$ to Dedukti, we want to produce proof terms that are
  as small as possible, so it is easy to store them, recheck them, or
  export them to a different system. However, there are at least two
  reasons why proof terms might become large. First, a typical proof
  in system $X$ might be very large (because big parts of it are
  constructed automatically). Second, translating a proof from $X$ to
  its encoding in Dedukti might greatly increase the size of the
  proof, for example by normalizing it or by adding type annotations.
  This means translating the proofs can take a very long time, and
  doing anything with the translated proof will be difficult. This
  also has an obvious impact on the exporting of complete libraries to
  Dedukti, which is the goal of WP3.

  To reduce the size of proof terms produced by the translation to
  Dedukti, we plan to investigate how to avoid unneccessary
  normalization or duplication of (parts of) proofs. We will also
  investigate what parts of each proof can be safely omitted because
  they can be inferred from the rest of the proof.
\end{description}

For each of the systems considered in this work package, we can
identify some preliminary work on translating proofs to Dedukti. Some
of these tools are in a preliminary stage and can only handle toy
examples, while others have been used to export whole libraries but
have not been updated to work with the latest version of the
system. Concretely, we plan to build upon the previous work done on
the following tools:
\begin{itemize}
\item The standard and arithmetic libraries of Matita has been the
  first libraries to be exported to Logipedia using Krajono, a fork of
  Matita. The forked system is also actually the only one able to
  import Logipedia proofs. The choice of Matita as a test-bench for
  Logipedia is easily understood considering that the implementation
  of the 0.99.x series was aimed at obtaining a well-documented,
  minimal but fast implementation of a theorem prover, two order of
  magnitudes smaller than Coq.  We plan to make this translation a
  part of the code of Matita itself so that it is maintained with the
  rest of the system.
\item CoqInE is a prototype tool that can translate Coq proofs to
  Dedukti. Recent work to include advanced features of Coq, such as
  universe polymorphism, has dramaticaly increased the coverage of
  this translation. We plan to make this translation a part of the
  code of Coq itself so that it is maintained with the rest of the
  system.
  \item In the summer of 2019, Guillaume Genestier worked together
  with Jesper Cockx on the implementation of an experimental
  translator from Agda to Dedukti during a research visit at Chalmers
  University in Sweden. This translator is still work in progress, but
  it is already able to translate 142 modules of the Agda standard
  library (about 25\%) to a form that can be checked in Dedukti. This
  exploratory work uncovered several challenges and opportunities for
  further work (see research challenges above).
  \item The inference kernel of Isabelle has already been instrumented
  to output proofs as $\lambda$-terms that can be understood by
  Dedukti. However, this has so far been only used for small examples
  \cite{Berghofer-Nipkow:2000:TPHOL}. The challenge is to make
  Isabelle proof terms work robustly for the basic libraries and
  reasonably big applications.  Preliminary work by Wenzel (2019) has
  demonstrated the feasibility for relatively small parts of
  Isabelle/HOL, but this requires scaling up.
  \item HOL4 has support for exporting proofs to the OpenTheory proof
  exchange format, and there has been some work on importing
  OpenTheory proofs into Dedukti.
  \item In the context of the BWare project, an encoding of the set
  theory of the B method has been provided as a theory modulo, i.e. a
  rewrite system rather than a set of axioms. This encoding is used by
  the automatic prover Zenon modulo which features a backend to
  Dedukti. Thus, as a first step through instrumentation of Atelier B
  and Rodin, proof obligations coming from Atelier B can be proved by
  Zenon modulo producing Dedukti proofs, hence providing a better
  confidence in the proofs produced by the native proof tools of
  Atelier B \cite{Bware}.
\end{itemize}

\bigskip
\noindent
{\bf \Large Automatic theorem provers, SAT solvers, SMT solvers, etc.}

\medskip

We have discussed, in the last section, the instrumentation of proof
systems, that is interactive systems, where the users build formal
proofs with the help of the machine. Automatic Theorem Proving are
another type of systems, where the machine build proofs without any
human intervention. The proofs built by these systems are often
expressed in simpler theories than those developed using proof
systems, but they are not of a different nature. We will also include
in Logipedia proofs built by such automatic systems, whether they are
called automatic theorem provers, SAT solvers, SMT solvers, etc.

Several reasons explain the importance of these proofs for Logipedia.
{\em Per se}, many formal proofs nowadays rely on the use of automatic
provers. They cover various domains and various kinds of application,
{\em e.g.} combinatorial
mathematics~\cite{DBLP:journals/ai/KonevL15,DBLP:conf/sat/HeuleKM16},
where they are expected to solve one large propositional problem, or
proof of
programs~\cite{DBLP:conf/esop/FilliatreP13,DBLP:journals/pacmpl/ProtzenkoZRRWBD17},
where they are given thousands of small problems in a combination of
quantified theories.

In a complementary way, automatic theorem provers will be used to automatically make a
coherent whole out of Logipedia. A fruitful interaction between
formal proofs requires low-level glue which falls in the scope of automatic theorem provers.
For instance, they will be employed to fill the holes that appear when
considering provers with various granularity, to reduce the gaps between
proof systems, and to discharge proofs of concept alignment (in close
interaction with the work package 6).

Finally, automatic theorem provers will also benefit from
Logipedia. Obviously, Logipedia will be an extensive library of formal
statements that can be used and combined by automatic theorem provers
in their proof search. This is not trivial though: lemma selection is
crucial to avoid an overhead. It can also be a source of benchmarks to
evaluate their expressivity and automation.  Lastly, Logipedia and
Dedukti will form a framework to make automatic theorem provers
cooperate with each other and with other tools, in a safe way.

PF: TODO: Talk somewhere above about the various automatic theorem
proving tools, their specificity and why they are each of them useful
or would benefit from Logipedia

\medskip

\noindent
{\bf \large From automatic theorem provers to Dedukti}

Similarly to Interactive Theorem Provers, connecting ATPs to the Logipedia
infrastructure strongly relies on the ability for ATPs to import statements from
{\sf Dedukti} and export proofs in some theory in {\sf Dedukti}.

\begin{description}
\item[Instrumenting ATPs to produce proof traces.] The first step for connecting ATPs to the Logipedia infrastructure, and the library of proofs, is that ATPs should actually output some kind of proof, without enforcing strong requirement on the format or even the level of granularity of those proofs.  SAT (Satisfiability) solvers for propositional logic do have a perfectly well specified format for proof traces~\cite{TODO} and most SAT solvers are actually able to produce proof traces in that format.  So, considering SAT solvers, the current status is actually meeting the Logipedia needs on this aspect.

  Considering other automated reasoners however, it is mandatory to significantly improve the picture.  Some SMT (Satisifiability Modulo Theories) solvers do produce some kind of proof trace, but the format is specific to the solver, and the proofs are sometimes difficult to replay due to their granularity.  Most mainstream first-order theorem provers output proofs in a standardized language~\cite{TODO}, but this language does not clearly specify the semantics of the proof steps.  Model-checkers do not provide proof traces per se, although such traces would be useful for various usages and notably certification.

  Our goals are to improve reasoning tools to demonstrate the feasibility to
  produce sufficiently detailed proofs for connecting to Logipedia, and to
  design a set of theoretical methods and practical tools that can be used to
  further connect Logipedia to the other existing automated reasoners of the
  same kind.  We will work on three SMT solvers (alt-ergo, CVC4, veriT), two
  first-order provers (the E-prover, Zipperposition), and one model-checker
  (cubicle).  We will also consider more specific reasoning tools, with the aim
  to demonstrate that this approach also applies to more specific reasoners.  We
  envision to experiment the approach on a Coherent Logic reasoner.  These
  reasoners are particularly useful for geometry reasoning, and as such, they
  are quite complementary to the other considered automatic reasoners.  They are
  also very suitable as as a first experiment, since their proofs are fairly
  straightforward to interpret.  They thus constitute a very interesting low
  hanging fruit.
  
  It is not possible, and not even desirable, to require all tools to directly
  talk in the language of Logipedia.  Indeed, proof trace languages that are
  specific to one kind of reasoning tool are more appropriate than Dedukti for
  instrumenting already large pieces of software, enabling quick output, and
  allowing proof post-processing at the right level of abstraction on the
  produced proof traces.  Furthermore, provided that those proofs are detailed
  enough, translation of traces to Dedukti will not be a difficult task, and a
  the work necessary to translate proof traces for a myriad of very different
  reasoners will be implemented in a unique tool (Ekstrakto, see below) to take
  advantage of the fact that reasoning techniques, and thus proof methods, are
  themselves pretty shared among the solvers.

\item[Translate ATP traces into Dedukti] As pointed out above, it is easier to
  instrument provers to make them output traces instead of directly provide
  Dedukti proofs. The second step to connect ATPs to the Logipedia
  infrastructure is to reconstruct the proof traces in order to build Dedukti
  proofs from them. The proposed process is the following: each step of the
  trace is transformed into an independent subproblem; each of these subproblems
  is given to a prover that can output Dedukti proofs; proofs of the subproblems
  are then combined to produce a global proof of the original problem.  Since
  subproblems correspond to atomic steps of the proof trace, they are relatively
  simple, so that we are confident that the prover producing Dedukti proofs will
  not struggle to find a proof. This process is quite similar to what is done by
  the hammer tools of interactive theorem provers (Sledgehammer in Isabelle/HOL,
  HOLyHammer for HOL4, etc.) which reconstruct proofs from traces produced by
  automated theorem provers.

  This scheme has already been prototyped in a tool called
  \href{https://github.com/Deducteam/ekstrakto}{Ekstrakto}. Ekstrakto takes a
  TSTP file, as can be produced by e.g.\ the provers E and Zipperposition, and
  it uses Zenon Modulo and ArchSAT to prove the subproblems. Ekstrakto was
  designed to be agnostic w.r.t.\ the prover producing the trace; in particular
  it does not depend on the specific set of inference rules of the prover. It
  was also designed to be agnostic w.r.t.\ the prover used to prove the
  subproblems; it is only required that the prover can output a Dedukti proof in
  the correct encoding of first-order logic.

  Although Ekstrakto has already shown that it is a valuable approach, it is a
  work in progress. In particular, the following issues will be addressed in the
  project:

\begin{compactenum}
  % extension to other proof trace formats
\item Up to now, Ekstrakto can only understand traces in TSTP format as
  input. We plan to make it accept traces in other formats, notably traces
  from SMT solvers, as well as all formats that will appear when instrumenting
  other ATPs.

  % unprovable steps

\item  Some steps in the proof traces are not provable: their conclusion is
  not a logical consequence of their premises. However, they preserve
  provability: the original problem has a proof if and only if the
  problem with the conclusion of the step also has a proof. This is the
  case for instance of the Skolemization step in first-order automated
  theorem provers, of the introduction of new definitions, as well as
  the RAT property in traces produced by SAT solvers. The approach of
  Ekstrakto cannot be used here, because the subproblem corresponding to
  the step cannot be proved. However, since provability is preserved, it
  should be possible to transform a
  proof using the conclusion of the step into a proof using its
  premises. Such a transformation depends on the nature of the step that
  has been used. We plan to include in Ekstrakto a way to handle
  Skolemization and definition introduction, which are the two step
  families that are missing to be able to manage all traces from the
  major first-order theorem provers.

  % specialization for theories

\item  Dedukti-producing provers used by Ekstrakto, namely Zenon Modulo and
  ArchSAT, are meant for pure first-order logic. However, we would like
  to deal with proof traces that use some specialized theory,
  e.g.\ arithmetic or bit-vectors, as could be output by SMT
  solvers. Although such theories could be presented as a set of axioms
  in first-order logic, it is almost certain that neither Zenon Modulo
  nor ArchSAT could be able to find non-trivial proofs using these
  axioms. Here, the idea would be to develop small provers dedicated to
  a particular theory, and outputting Dedukti proofs. Such provers would
  be called when a step in the trace relies on said theory. These
  provers need not be very optimized, since trace steps are relatively
  small; this should help producing Dedukti traces. A way to achieve
  this could be to extend Zenon Modulo: indeed, Zenon modulo can find
  proofs modulo arithmetic, but it is not able to produce a Dedukti
  proof yet.

\end{compactenum}
  
\end{description}

\noindent
{\bf \large From Dedukti to automatic theorem provers}
%\label{concept:wp4:deduktitoatp}

In the other direction, Logipedia will constitute a source of
knowledge for automatic theorem provers. For this to be affordable, this workpackage will
study the following challenges.

\begin{description}
\item[Translate Dedukti statements into automatic theorem provers
  inputs] Automatic theorem provers are mostly based on (parts of)
  first-order logic.  Logipedia theorems, which mostly come from
  interactive provers, will be expressed in the Dedukti encodings of
  much more expressive logics, such as dependent type theory or simple
  type theory.  Theorem statements thus need to be encoded again to be
  manipulable by automatic theorem provers.

  Encodings from expressive logics to first order logic already exist,
  and are used for instance in {\em hammers} for using automatic theorem provers into
  interactive theorem
  provers~\cite{DBLP:conf/lpar/PaulsonB10,DBLP:journals/jar/CzajkaK18}.
  In these works, the encodings are specific to one system to be
  affordable. In Logipedia, we have to combine and take benefit
  from statements coming from the encodings of different systems based
  on different logics. The key challenge here are thus:
  \begin{itemize}
  \item to avoid the loss of meaning coming from a succession of
    encodings; and
  \item to encompass statements coming from different systems.
  \end{itemize}
  We plan to investigate a new approach where, instead of hammers, the
  encoding is a succession of fine-grained encodings dedicated to one
  aspect. These ``small'' encodings will offer the possibility to be
  activated independently depending on the origin of the statement. They
  give the other advantages of being modular, easily extensible, and
  more reliable: each encoding is simple, and may output proofs, {\em
    e.g.} using Ekstrakto. It will also crucially rely on the concept
  alignment provided by the workpackage 6.

  It is common in the automatic theorem proving community to evaluate the performance on
  sets of benchmarks. As a side effect of this task, a new set of
  benchmarks will be extracted from Logipedia to measure the
  performance of automatic theorem provers on problems coming from different logic, and from
  a combination of these logics. In our case, it will not only allow to
  compare automatic theorem provers but also to measure the success of this task by evaluated
  the quality of encodings, and comparing our ``small'' encodings by
  activating them or not.

\item[Logipedia as a source of knowledge for automatic theorem provers] Pascal

\end{description}


\noindent
{\bf \large Large scale application: formal verification of C code}

The use of formal methods in the industry requires to have approaches
that apply to a wide variety of cases. For the verification of C code,
the Frama-C platform features numerous techniques. One of them relies on
automatic solvers: Frama-C-WP. Since automatic theorem provers are built by making many
choices such as which heuristics or which algorithms to use, they
display blind spots where they are less efficient to solve some cases.
In order to overcome that, it is necessary to use the wide variety of
solvers which exist in a portfolio manner. The Why3 tool features the
ability to send problems to lots of provers in a uniform way. However
with so many tools written by different teams involved, the meaning of
the same concept in the different tools could be different. It would
lead to errors in the overall verification results. Moreover in order to
be applied more widely, formal methods must handle more concepts such as
floating points where their exact meaning is less clear that
mathematical integers. That leads to more opportunities for two tools to
interpret differently the same concept. Finally, an industrial user
sometimes needs to add some reasoning or simplifications for their
particular concept so that it is better handled by automatic solvers. It
is very easy to make an error in those simplifications.

In order to overcome these problems, we propose to gain insurances in
the interaction of these different tools and to speed the addition of
new features to handle new cases by using proof objects:
\begin{itemize}
\item Where the industrial user needs to define the concepts he wants to
  verify in its C code, we will add the possibility to import concepts
  from Logipedia. The user gain time by not having to define them
  himself and it ensures that we have proofs for the accompanying
  lemmas.
\item Where the simplification rules written for the industrial case are
  executed in Frama-C-WP, we will instrument it to generate Ekstrakto
  input in order to produce a proof of the correction of these
  simplifications.
\item Where the problems are transformed to fit the different provers in
  Why3, we will inspire from the fine-grained encodings
  (Sec.~\ref{concept:wp4:deduktitoatp}) to also generate Ekstrakto
  proof.
\item In the end, we will gather the Dedukti or Ekstrakto proof of the
  provers, the encodings, and the simplification rules, in order to
  assemble them in a coherent whole.
\end{itemize}
This part thus combines and validates most of the previous aspects of
this workpackage, but also constitutes the challenge to instrument a
large-scale formal tool.

\medskip

\noindent
{\bf \large Automatic theorem provers to increase Logipedia readiness}

\dots Chantal: T5 and T6

\subsubsection{Description of the work package 3}

\subsubsection{Description of the work package 7}

\subsubsection{Description of the work package 9}

\subsubsection{Description of the work package 2}

\subsubsection{Description of the work package 5 and 6}

{\bf (b) Methodology}

The methodology is the same for integrating any library to Logipedia,
but due to a difference of readiness of the various systems we focus on,
our priorities are different.

\begin{enumerate}
\item We already know how to express most of the theories of Atelier B,
Coq, FoCaLiZe, HOL Light, HOL4, 
Isabelle, Matita, and Rodin in  Dedukti. We propose
to instrument these systems so that they can produce Dedukti
proofs that we can include in Logipedia.

\item For other theories, such as those of Abella, Agda, Lean, Minlog,
  Mizar, PVS, TLA+, the work is in progress, or not yet started.  So
  we must first understand how they can be expressed in Dedukti.

\item Besides the standard libraries of these systems, large libraries
  have been developed: the Isabelle Archive of formal Proofs \cite{AFP},
   Flyspeck \cite{Flyspeck}, MathComp\cite{Mathcomp}, 
  CompCert \cite{Compcert},  CakeML \cite{CakeML}, ...  We aim to include
  some of these libraries in Logipedia.
  
\item
Besides proof systems, we also want to include proofs coming from
automated theorem provers, SAT solvers, SMT solvers, and model
checkers.  So we must instrument some of these systems so that they
can produce Dedukti proofs that we can include in Logipedia.

\item
We want to develop algorithms to analyze which symbol, axiom, rewrite
rule is used in each proof, and consequently in which system each proof
can be used. We also want to develop algorithms to eliminate some of the 
symbols, axioms, and rewrite rules used in a proof in order to be able to 
use it in more systems.


\item
Each library imported in Logipedia will come with its own
definition of natural numbers, real numbers, etc. We want to develop
``concept alignments algorithms'' to transport theorems from one
structure to another isomorphic one.

\item 
Besides data, we propose to include in Logipedia, metadata and
an inner structure.
\end{enumerate}

\subsubsection{Methodology of work package 1: instrumentation}

We know how to express in Dedukti the theories implemented in Matita,
HOL4, Coq, Agda, Isabelle and Atelier B, and some of these systems
have been partially instrumented to export proofs that can be checked
in Dedukti. Our first work package is to complete this instrumentation
to be able to export most of the proofs of these systems. As a
consequence, the work includes a strong practical component.

Three methods have to be used here: some of the systems (Automath
style), such as Coq and Agda already have proof-terms that can be
output, thus the main task is to translate these proofs into the
Dedukti format. Others (LCF style), such as Isabelle and HOL4, have an
inference kernel that can be instrumented, the main task here is to
transform the internal proof-object into an external
proof-term. Others, such as Atelier B are slightly more difficult to
address. For those, we need to use the water ford method: extract an
incomplete trace (a sequence of lemmas) and fill the gap using
automated theorem proving, as experimented with Atelier B and Zenon.

% A technological hurdle in the instrumentation of the systems under
% consideration is that all of them are actively developed systems
% that are constantly evolving.

\subsubsection{Methodology of the work package 4}

\subsubsection{Methodology of the work package 3}

\subsubsection{Methodology of the work package 7}

\subsubsection{Methodology of the work package 9}

\subsubsection{Methodology of the work package 2}

\subsubsection{Methodology of the work package 5 + 6}

\subsection{Readiness of the project}

This idea of building such a standard for proofs has already been
investigated in the past, such as in the Qed manifesto \cite{Qed94}, but
has produced limited results.

Our thesis is that, since the
Qed project, the situation has radically changed. After
thirty years of research, we have an empirical evidence that most of
the formal proofs developed in one of these systems can also be
developed in another. We understand the relationship between the
theories implemented in these systems much better. We have developed
several logical frameworks, extending predicate logic, in which these
theories can be expressed. And we have developed reverse mathematics
algorithms to analyze which axioms and rules are used in each proofs
and algorithms, such as constructivization algorithms, to translate
proofs from one theory to another.


%%% Local Variables:
%%%   mode: latex
%%%   ispell-local-dictionary: "english"
%%% TeX-master: "propB"
%%% End:
