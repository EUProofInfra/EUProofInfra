\thispagestyle{empty}

Bugs kill, and although testing may reveal some bugs, only formal
modeling and verification can guarantee their absence (Figure
\ref{safety}).  Thus, we should never fly in an autonomous plane driven by a
piece of software that has not been formally verified.

Today, the trust in critical systems relies on formal verification, in
particular formal proofs (Figure \ref{formal}), that guarantee the
safety of the people using transportation systems (autonomous cars,
subways, trains, planes, etc.), health systems (robotic surgery,
etc.), energy provided by nuclear plants, financial applications,
e-governance, etc. This crucial role of formal proof is highlighted by
several successes, like the correctness proofs of the automatic Paris
metro line 14 \cite{metro14}, the detect-and-avoid system for unmanned
aircraft system developed by NASA \cite{Munoz16}, the operating system
seL4 \cite{Klein09}, or the C compiler CompCert \cite{Leroy06}.  It
has been empirically observed that compilers and operating systems
often have bugs, but proved ones, such as seL4 or CompCert have none
or much fewer.  This is why certification processes require proofs,
and not only testing, at the highest Evaluation Assurance Levels of the 
Common Criteria security evaluation international standard, 
in effect since 1999. 

Because formal methods are crucial in the development of the
information society, because people can die and companies go bankrupt
because of a bug, it is crucial for Europe to master this technology
and its evolution.

\begin{figure}
\begin{framed}
Imagine the following casino game. At the beginning, a player is given
eleven euros. At each round, she throws a dice. If the results is a
six, then game is over.  If it is a five, a four, a three, or a two,
she is given twice the amount of money she already has. If it is a one
she is taken two euros, if she has at least two.  When the game is
over, the player wins the money she got, except if she has zero, in
which case she looses one million.

This game can be modeled by program $p$
%import random 
\begin{verbatim}
n = 11
stay = True;
while stay: 
    roll = random.randint(1,6)
    if roll == 6:
        stay = False
    else:
        if roll >= 2:
            n = n + 2 * n
        else:
            if (n >= 2):
                n = n - 2
print(n)
\end{verbatim}

To be on the safe side, the player wants to be sure, before starting
playing, that she will never finish with zero.  And indeed, in all runs,
the content of the variable {\tt n} is always an odd number, thus it
cannot be zero. This property is a consequence of two simple
theorems of arithmetic
$$\forall x~(\mbox{\it odd}(x) \Rightarrow \mbox{\it odd}(x + 2 * x))$$
$$\forall x~(\mbox{\it odd}(x) \Rightarrow \mbox{\it odd}(x - 2))$$
%$$\forall x~(\mbox{\it odd}(x) \Rightarrow \mbox{\it odd}(x))$$
Hence, verifying the safety of this program, that is that the
proposition 
${\mbox{\it safe}}(p)$ holds, 
amounts to prove these three theorems of arithmetic.

A tiny bug in the program, for instance replacing the {\tt 2} by a
{\tt 3} in the the line {\tt n = n + 2 * n} makes the program unsafe
as shown by the sequence $11, 9, 7, 5, 3, 1, 4, 2, 0$. Yet, testing
this program will, most likely, not reveal this bug, that manifests
very rarely.  In contrast, attempting to prove the correctness of this
program will
reveal the bug as it is impossible to prove the proposition
$$\forall x~(\mbox{\it odd}(x) \Rightarrow \mbox{\it odd}(x + 3 * x))$$
\caption{How do proofs contribute to safety and security of software?\label{safety}}
\end{framed}
\end{figure}

\begin{figure}
\begin{framed}
Since Antiquity, we have known that
proofs, both purely mathematical ones, as in Euclid's elements or the
recent proof of the Kepler conjecture by Thomas Hales, and proofs used
to establish the safety and security of software, can be built with a
limited number of rules, for example
\begin{itemize}
\item From $A \Rightarrow B$ and $A$, deduce $B$.
\item From $A$, deduce $A~\mbox{\it or}~B$.
\item ...
\end{itemize}
Yet, through history, most mathematical proofs have been written in
pidgin of natural language and mathematical formulas. When proofs are
very long (as it is often of the proofs used in safety and security,
but also of some proofs in pure mathematics), mistakes in proofs are
very difficult to detect. For instance dozen of wrong proofs of
parallel postulate have been given through history, sometimes by the
best mathematicians such as al-Hayyam, at-Tusi, 
Wallis, Saccheri, Lambert, Legendre...

In the 1960s, Robin Milner and Nicolaas De Bruijn noticed that the
correctness of a mathematical proof could be checked by a
computer. This led to the development of the two first proof systems
in history: Milner's {\sf LCF} or De Bruijn's {\sf Automath}.  From
the axioms
$$\forall x~(\mbox{\em philosopher}(x) \Rightarrow \mbox{\em human}(x))$$
$$\forall x~(\mbox{\em human}(x) \Rightarrow \mbox{\em mortal}(x))$$
we can deduce
$$\forall x~(\mbox{\em philosopher}(x) \Rightarrow \mbox{\em mortal}(x))$$
In the language implemented in {\sf Automath}, this proof is written
$$\lambda x \lambda h~(g~x~(f~x~h))$$

\caption{What is a formal proof? What is a proof system?\label{formal}}
\end{framed}
\end{figure}

\begin{figure}
\begin{framed}
Deduction rules such as ``From $A \Rightarrow B$ and $A$, conclude
$B$'' are universal, but building proofs require more rules, that are
often specific to a domain of knowledge and are called
``axiom''. Examples are the axioms of geometry, the axioms or
arithmetic. These axioms constitute a theory.

At the beginning of the 20th century an axiomatic theory, {\em set
  theory}, has been proposed to express all mathematical proofs. In
the first half of the 20th century a few variants of set theory, and a
few alternatives have been proposed (such as Simple type theory).
But, because these theories had not been designed for being
implemented on a computer, the rise of computer checked formal proofs
has led to a multiplication of such alternative theories. Each proof system,
such as {\sf Coq}, {\sf Isabelle/HOL}, {\sf Mizar}, {\sf Atelier B},
etc. implements its own theory.

This is the major obstacle to interoperability.
\caption{What is a theory?\label{theory}}
\end{framed}
\end{figure}

A lot of formal proofs developed for one application could be used in
another.  Unfortunately, the development of formal methods is slowed
down by the large number of proof systems (and sometimes the large
number of versions, over time, of one single system) and the lack of a
common theory (Figure \ref{theory}) used by these systems.  For
instance, the Paris metro line 14 has been proved correct in {\sf
  Atelier B}, while the Nasa detect-and-avoid system for unmanned
aircraft system has been proved correct in {\sf PVS}, the {\sf seL4}
operating system has been proved correct in {\sf Isabelle/HOL}, and
the compiler {\sf CompCert} has been proved correct in {\sf Coq}.
Some projects, such as the proof of Hales' theorem (Kepler's
conjecture) \cite{Hales17}, have been started in different systems and
required significant integration effort for obtaining the overall
result.

These proof checking systems are infrastuctures and these
infrastuctures have not been integrated yet.  Because of this lack of
integration, each small community is centered around one theory and
one system. Each library is specific to a proof system, or often even
to a specific version of this system. In general, a library developed
in one system cannot be used in another, and when the system is no
longer maintained, the library may disappear.  Thus, interoperability
(the possibility for one user to use a proof developed in another
system), sustainability (the possibility to use a proof decades after
it has been developed), and cross-verification (the possibility to
have a higher assurance in the correctness of some statement by
verifying its proof in a system different from that in which it has
been constructed) are restricted, even if some provers sometimes call
external tools to help building some proofs. The fragmentation of
systems of formal proof also hinders productivity because foundational
work (for instance, developing a calculus library with theorems about
the sinus and cosinus functions, derivatives, etc.) has to be repeated
instead of being reused.

This lack of standards limits the spreading of formal proofs in
non-specialist communities. For instance, teaching formal proving to
undergraduate students in a logic course is difficult, as it requires
the choice of a specific language, a specific theory and a specific
system that orients the course towards this language, theory, and
system, instead of orienting it to fundamental principles that are
useful everywhere. The same is true for the use of formal proofs in
industry or by working mathematicians. So another goal of this project
is to make formal proofs accessible to a much larger community, as
standards often do.

On more philosophical grounds,
while we had in the past an (informal) proof of Pythagoras' theorem or
Fermat's little theorem, the same proof now has different
formalizations in {\sf PVS}, {\sf Isabelle/HOL}, {\sf Coq}, etc.
Thus, the universality of logical truth itself is jeopardized, just
like it has been jeopardized in the 19th century
by non-Euclidean geometries, as some
statements could be true in one geometry, but false in others.

Some twenty major proof assistants exist in the world (Figure
\ref{systems}), that are used both for proving properties of software
and results in pure mathematics, such as the Feit-Thompson theorem
\cite{Gonthier13} or Hales' theorem (Kepler's conjecture)
\cite{Hales17}.  Making these systems interoperable would avoid
duplication of work, reduce development time, and enable
cross-verification.  After three decades dedicated to the development
of these systems, allowing such a cooperation between these systems,
going in the direction of designing a standard for proofs, is the next
step in the development of the formal proof technology.

\begin{figure}
\begin{framed}
\begin{center}
\begin{tabular}{l@{\hspace{3cm}}l}
{\sf \underline{Abella}}    & {\sf Acl2}\\
{\sf \underline{Agda}}      & {\sf \underline{HOL Light}}\\
{\sf \underline{Atelier B}} & {\sf IMPS}\\
{\sf \underline{Coq}}       & {\sf Lean}\\
{\sf \underline{FoCaliZe}}  & {\sf \underline{LFSC}}\\
{\sf \underline{HOL4}}      & {\sf Nuprl}\\
{\sf \underline{Isabelle}}  & {\sf \underline{PVS}}\\
{\sf \underline{Matita}}    & {\sf \underline{TSTP}}\\
{\sf \underline{Minlog}}\\  
{\sf \underline{Mizar}}\\
{\sf Rodin}\\
{\sf \underline{SMT-Lib}}\\
{\sf \underline{TLA+}}\\
{\sf \underline{Why3}}\\
\end{tabular}
\end{center}
\caption{Some major proof systems or formats. The European ones are in the first column.
  Those addressed in the project are underlined\label{systems}}
\end{framed}
\end{figure}

At the beginning of the 20th century, a solution was found to the
crisis of non-Euclidean geometries: the definition of the various
geometries in predicate logic \cite{HilbertAckermann} restored the
universality of logical truth, allowing to determine which axiom
was used in which proof and which theorem held in which geometry.
In the same way, we defend that the interoperability of
proof systems can only be achieved if we are able to express the
theories implemented in these proof systems in a common logical
framework.

In 1928, predicate logic, the first logical framework in the history,
was a huge success, since three important theories used at that time
(geometry, arithmetic and set theory) could be expressed in it. But it
also has limitations, which explains that another of the major
theories used at that time (Russell's type theory, from The Principia
Mathematica) has not been expressed in it. Since then, several other
versions of type theory, such as Church's type theory \cite{Church40},
Martin-L\"of's type Theory \cite{Martin-Lof84}, and the Calculus of
constructions \cite{CoquandHuet88}, have also been defined as
autonomous theories. These theories are those implemented in most
of the current proof systems, yet they cannot be expressed in
predicate logic.

This failure has led, in the field of proof systems, to the
abandonment of predicate logic, and even of the concept of logical
framework: the theories implemented in {\sf Coq}, {\sf Matita}, {\sf
  HOL Light}, etc. are often defined as autonomous systems, and not in
a logical framework.

However, a different line of research has attempted to understand the
limitations of predicate logic and to propose other logical frameworks
repairing them. The most prominent limitations of predicate logic are
the lack of function symbols binding variables, the lack of a syntax
for proof terms, the lack of a notion of computation, the lack of a
notion of cut for axiomatic theories, and the impossibility to express
constructive proofs. These limitations have led to the development of
logical frameworks such as $\lambda$-Prolog \cite{NadathurMiller88,
  MillerNadathur12}, Isabelle \cite{Paulson90}, the $\lambda
\Pi$-calculus (also called the ``Edinburgh logical framework'')
\cite{HarperHonsellPlotkin91}, Deduction modulo theory
\cite{DowekHardinKirchner03, DowekWerner03}, Pure Type Systems
\cite{Berardi88,Terlouw89}, and ecumenical logics
\cite{Prawitz15,Dowek15,PereiraRodriguez17}.

All these logical frameworks have been unified in the $\lambda
\Pi$-calculus modulo theory \cite{CousineauDowek07}, implemented in
the system {\sf Dedukti} \cite{Assaf16}.  Geometry, arithmetic and set
theory, but also Russell's type theory, Church's type theory,
Martin-L\"of's type theory, and the Calculus of constructions can be
expressed in this framework.

So the theories implemented in various systems can all be expressed in
{\sf Dedukti} and the proofs developed in these systems can be
translated to {\sf Deduki}.  The analysis of the axioms used in each
proof then indicates in which systems this proof can be made available
\cite{Thire18,Dowek17} (a domain traditionally called ``reverse
mathematics'' \cite{Friedman76,Simpson09}).  This allows to collect
the proofs developed in various systems into a single on-line
encyclopedia.


This encyclopedia is called {\sf Logipedia}. 

A first prototype, put on-line in January 2019, contains
a few hundred lemmas and developing this encyclopedia so that it
contains in twenty years all the formal proofs then developed to allow
interoperability of proof systems, and sustainability and
cross-verification of formal proofs is the main goal of this project.

The possibility to use a {\sf Logipedia} proof in a system depends
only on the axioms used in this particular proof and not on the axioms
used in the system in which it has been developed. This analysis is
the basis of the interoperability between proof systems.  The proofs
that are currently in {\sf Logipedia}, are expressed in six different
systems: {\sf Matita}, {\sf Coq}, {\sf Lean}, {\sf HOL Light}, {\sf
  Isabelle/HOL}, and {\sf PVS}.

All there proof systems are research infrastructures and {\sf
  Logipedia} is thus a research infrastructure that permits
comminication between these infrastructures, by allowing to share 
the data processed by these infrastructures.


This encyclopedia contains only a few hundred proofs, but convinced
that such a cloud of formal proofs could bring to the applications of
formal proof technology the same boost that the cloud has brought to
computing, and also that managing such a large encyclopedia (for
instance being able to query a proof with a search engine) required
some interdisciplinary effort, we organized, in January 2019, a
meeting to discuss the future of this project
\url{http://deducteam.gforge.inria.fr/seminars/190121.html}.  This
meeting brought together 38 researchers from Austria, the Czech
Republic, France, Italy, the Netherlands, and Poland.  Since then,
colleagues from Belgium, Germany, Serbia, Sweden, and the United
Kingdom, from academia and industry, have manifested interest in
participating in this effort.  These researchers and engineers are
ready to contribute to develop this encyclopedia, aiming at sharing
proofs, under a creative common licence making them findable,
accessible, interoperable, and reusable.

Such an infrastructure is, in many ways, new in the European Strategy
on Research Infrastructures. We can even say that the idea to
structure a networking activity around the construction and the use
of a large scale infrastructure is relatively new in computer science and
mathematics, even if other projects, such as {\em OpenDreamKit} and
{\em Software Heritage} do exist. Our goal is therefore also trigger a
little, but significative, evolution on the organization of research
in computer science and mathematics in Europe.

%%% Local Variables:
%%%   mode: latex
%%%   mode: flyspell
%%%   ispell-local-dictionary: "english"
%%% End:
