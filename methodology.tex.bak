Developing a new research infrastructure, by integrating existing
ones, through data exchange, requires a wide range of activities,
described in the workplan. This different activities will require
to use different methodologies: developing new pieces of software,
modifying existing ones, experimenting with translations, proving
theorems, etc.

\subsubsection*{Integration}

The methodology to instrumenting existing proof systems so that they
can produce Dedukti proofs varies a lot in function of the considered
proof system.

\begin{itemize}

\item 
Some of them, the ``Automath-like systems'', such as Coq, Agda, and
Matita, already have a notion of proof-term, that is an explicit
representation of proofs.  In theory it is sufficient to translate
these terms into the language of Dedukti.  As discussed in the Section
\ref{concept}, in practice, this may require to reconstruct transient
proof components, or to modify the term language of the instrumented
system so that these components are included in the proof language
itself.

\item
Some other proof systems, the ``LCF-like systems'' such as
Isabelle/HOL, HOL4, and HOL Light, have a small kernel, that is the
only piece of code that needs to be instrumented. This requires some
expertise in the instrumented system, but, with this expertise, this
instrumentation is doable. In fact some of these systems already have
been instrumented and can export proofs expressed in a format called
OpenTheory. So, we can reap the benefit of these developments and just
translate the OpenTheory proofs to Dedukti.

\item
A third category of proof systems require a different and more complex
methodology. For these systems, such as PVS or Matita, all we can
extract is a proof-trace, that is a sequence of propositions, such
that the transition from one to the next is a ``easy'' just like
hoping from stone to the next is ``easy'' when crossing a river on a
ford. We thus need to use an external automated theorem prover to fill
the gaps between the propositions of the proof-trace, and we need this
automated theorem prover itself to be instrumented, so that we can
build a proof from the trace.
\end{itemize}

In all these cases, we need to take also into account that these proof
systems may call external tools, for instance automated theorem
provers, SAT solvers, SMT solvers, etc. to build automatically some
parts of the proofs. In this case the external tool has to be
instrumented as well, to that it produces a proof that can be used by
the proof system to produce its own proof.

One methodological issue here is that all proof systems are actively
developed systems that are constantly evolving, even if the theory
they implement evolves at a slower pace. In order to avoid having
translations that always work for an obsolete version, we need to
include these translators to Dedukti to the code of the systems
themselves, so that the translations are maintained at the same pace
as the proof system itself. This is our plan to migrate the
translations from Matita, Coq, and HOL Light to the systems
themselves, and all the other translations, as soon as they reach the
relevant maturity.

\subsubsection*{Automated theorem provers, SAT/SMT solvers, model checkers,
  etc.}

Instrumenting Automated theorem provers, SAT solvers, SMT solvers and
model checkers raises different methodology issues, as these systems
must be very efficient to be useful. This yields two new issues.

First, these systems include many optimizations that make their code
very complex and hence difficult to instrument. Then, instrumentation
offten slows down the instrumented a little bit and if this is acceptable
for interactive proof systems---the user being anyway slower than the
software---this is often not acceptable for automated theorem provers.

So, these systems must be instrumented in a minimal way, so that they
execution is not perturbed by the instumentation. So in most of the
cases, we can just extract a proof trace, as in the third case above,
and fill the gaps in these proofs by using another, less powerful but better
instrumented, automated theorem prover, to fill the gaps. 

Some automated theorem provers already output proof traces, meaning
that we can see them as black-boxes and concentrate on the translation
from these traces to Dedukti. To this end, we plan to extend the
prototype tool Ekstrakto to handle more proof formats and increase its
expressivity.

For the others, a preliminary task is to instrument them.  We target
proof formats that will be already integrated into Dedukti, in
particular TSTP, for automated theorem provers, and LSFC, for SMT solvers.

When an automated theorem prover is called from Dedukti to fill the
gat bewteen two propositions, we need also to translate theorem
statements expressed in Dedukti, into the logic understood by
automatic provers. We will target a pivot language for which
translations to major existing automatic provers already exist, such
as TIP~\cite{DBLP:conf/mkm/ClaessenJRS15} or the input of
Why3~\cite{DBLP:conf/esop/FilliatreP13}. A research challenge is to
make this encoding preserve the logical meaning and deal with
statement coming from multiple tools. Our approach will be to build
the encoding out of independent ``small'' encodings that can be
activated or not, making use of the concept alignment.

To handle the large amount of theorems stored in Logipedia, we need
tools to select the knowledge relevant for particular goals. We will
look at machine learning methods to select relevant knowledge in the
Logipedia corpus as well as to select potential proof term components.

\subsubsection*{Large libraries}

The methodology for translating large libraries is much more related
to that of experimental sciences: in principle, if we can translate a
small library, we can translate a large one. In practice, things are
more complicated.

So the first thing we need to do is to experiment with the theories
and the translations we do have. Most likely, some efficiency issue
will manifest and we will have to understand where these efficiency
issues come from.

\begin{itemize}
\item It may be because, we are using a computer that is too slow or
  has not enough memory.
\item It may be because Dedukti itself is too slow and we need to modify
  Dedukti itself, as we have done in the past to increase, for instance,
  the efficiency of Dedukti's conversion test.
\item It may be because the proof term produced by the translationis
  too big, as has happened in the past with translations that did not
  take sharing into account, letting the size of the proof term explode.
\item It may be because the expression of the theory itself has to be
  revised, as has happended in the past with the translation of
  inductive types.
\end{itemize}

Depending on the result of this analysis, the computer we use, Dedukti,
the translation function, or the theory itself has to be improved, until
the full library can be handled.

\subsubsection*{Infrastructure}

Developping an infrastucture to make the proofs accessible raises a
different methodology issues. Here the technology of web services,
distribution package, ergonomic user interfaces, search engines is
mature and we need to rely on this existing technology.

Yet this raises two methodology issues. The first one is that some of
these technologies must be adapted for the case of proofs. This is,
for instance, the case of the seach engines technology, as plain text
search is not adapted to formal proofs. First because logical formulas
are not made of words, themselves made of letters, but directly of
symbols. Second because all formulas are, more or less made with the
same symbols in different orders---for instance the equal sign is in
every equation, so that it does not mean anything to seach a formula
on the equal sign.

The second aspect which is both methodological and managerial is that
these technology are mastered by other people than researchers and
engineers on formal proofs. So we need to develop interdisciplinary
teams, which we have succeded, by incorporating several partners
in the consortium who will bring this expertise.

\subsubsection*{Structuring the encyclopedia}

Structuring the encyclopedia raises the same methodological issues
than developping the infrastucture to make the proofs accessible.

The technology of ontologies, for instance, is mature. But is has been
developed in different contexts---for instance for medical
ontologies---and we need to undertsand how it can be adapted to formal
proofs. 

Then, this technology is mastered by other people than researchers and
engineers on formal proofs. Fortunately we have succeded to have in
the consortium reseachers who will bring their expertise on this
topic.

\subsubsection*{New theories, new systems}

Understanding how to express new theories in Dedukti is, from a
methodological point of view, closer to mathematics and logic.  The
main difficulty is to prove the properties (such as confluence,
termination, etc.) of the proposed theories and to prove that they are
indeed a sound and complete expression in Dedukti of the original
theories.

This work can of course benefit from the existing expression of the
more mature systems above: although every system has its
idiosyncrasies that require special techniques, there are also many
common aspects. In particular, the systems considered here are based
on similar foundations (more or less elaborate type theories, set
theories, etc.) as those considered above, and ideas that have proved
successful there can be transferred {\em mutatis mutandis}. For
instance, issues such as proof-irrelevance, induction, co-induction,
etc. are similar accross theories and systems.

Yet, an interesting methodological issue, manifests when we attempt to
express new theories in Dedukti. In some cases, we do not succeed, but
we understand that a minor modification of Dedukti would help. There
is here a subtle balance to find between modifying Dedukti each time
we want to express a new feature in it, and having a rigid Dedukti
that would never evolve. As any logical framework, Dedukti should
remain simple enough and neutral with respect the the various theories
it permits to express, but also flexible enough so that it evolves
according to the needs of its users.

\subsubsection{Proof engineering}

\item
We want to develop algorithms to analyze which symbol, axiom, rewrite
rule is used in each proof, and consequently in which system each proof
can be used. We also want to develop algorithms to eliminate some of the 
symbols, axioms, and rewrite rules used in a proof in order to be able to 
use it in more systems.

\item
Each library imported in Logipedia will come with its own
definition of natural numbers, real numbers, etc. We want to develop
``concept alignments algorithms'' to transport theorems from one
structure to another isomorphic one.

\paragraph{Aligning logical foundations}

Our first step will be to divide logical theories in clusters,
according to the key parameters mentioned above, and then to build a
web of syntactic translations between systems. We do not expect full
back-and-forth translations, as some systems are well-known to be
proof-theoretically stronger than others, but we will seek to
establish suitable equiprovability results for fragments of the
relevant languages.

We shall also deal with specific case studies to test our work.  For
example, to test our methods for translating classical proofs into
constructive ones, one could verify Michael Beeson's "wholesale
importation" (he uses the double negation translation to import all
the negative results from \cite{} to intuitionistic logic) using the
library of GeoCoq proofs.

\paragraph{Aligning theorem proving objects (case studies)}

We call big scale concept alignment the equivalence between different
axiom systems, and small scale concept alignment the equivalence (or
relationship) between different definitions.  Big scale concept
alignment is the alignment of different theories, usually expressed
using different axiom systems (eg., Tarski, Hilbert, Euclid) for
different geometries: euclidean and non euclidean. It also includes
alignment of different kind of analytic definitions of geometry i.e.,
between different analytic models (e.g., real closed fields, reals,
complex numbers), different definitions of projective
geometry. Porting GeoCoq to other proof assistant will bring some of
these big scale concept alignments.  Small scale concept alignment
requires proof of equivalence between different definitions of the
same concept in the same or different language.  To some extent this
task could be automated, but the difficulty is that some equivalences
are valid only in some contexts.

\paragraph{Automated theory alignment}

In one line of research, we set to use logic-deduction approaches that
have been developed for the automatic alignment of database schemas
and instances, such as \cite{}.  In order to align theories across
libraries, we will propagate alignment information, based on a
certified probabilistic inference engine, which will extend the work
in \cite{}. We envisage a collaboration with WP7, aimed at reusing the
ontology framework integrated in the core of Dedukti. Concretely, each
of the domain-specific meta-data, essentially the theory schemas, will
be specified in the ontology framework. Their validity with respect to
the underlying instances will be mechanically checked and the
alignment of the schemas will inform that of the underlying theories.

In the second line of research, we will employ machine learning
techniques, based on neural networks, to design heuristics for finding
new alignments.

\paragraph{Alignment-based services}

This task will be split into two lines of work corresponding to findability and reuse across prover libraries.
Firstly, an alignment-based search service will accept queries in any language $L$ encoded into Dedukti but will find results in any library imported into Dedukti, not just the one underlying $L$.
Secondly, a proof rewriting service will use alignments between two libraries to migrate proofs from one to the other.
Both applications depend alignment-induced expression translation.
All three components are described below.

\textbf{Expression Translation.}
We refine the existing definitions of alignments in a way that allows alignments creators to specify translation rules for expressions formed from the aligned operators.
Conceptually, we will provide a declarative user-friendly syntax for these.
But structurally, they will correspond to rules in the logic programming systems ELPI.
Thus ELPI can directly execute the translation function induced by a set of alignments.

\textbf{Search.}
We use the MathWebSearch system to perform unification queries in formal libraries.
It would be straightforward to index and search all libraries available in Dedukti in MathWebSearch.
However, then if concepts $a$ and $b$ from libraries $K$ and $L$ are aligned, every query about $a$ in the language of $K$ would only find results in $K$ but never from $L$.
By using the alignment-based expression translation, we can translate the user's query into other the language of $L$ (thus replacing in particular $a$ with $b$) and then run the translated query as well.
Thus, we can find results in all libraries.

Concretely, in the implementation, the UI will ask for a query written in Dedukti syntax together and provide a set of checkboxes indicating which libraries should be searched.
The system then translates the query into the selected libraries and displays all results.

\textbf{Proof Rewriting.} In the recent literature there are two main
approaches to alignment based proof rewriting. The first one, based on
logical relations, has been proposed to translate a proof about $X$
into a proof about $X'$ remaining in the same logic. A relation is
established between elements of $X$ and elements of $X'$ where, for
example, $X$ can be the type of sorted lists of numbers, $X'$ the type
of balanced search trees and the relation holds between data
structures that contain the same set of integers. Then,
oversimplifying, for each pair of corresponding functions acting
respectively on $X$, $X'$, it is shown that they map related elements
to related elements. Continuing the example, the function that inserts
a new integer into the sorted list and the one that does the same into
the balanced search tree map data structures that contain the same
elements to data structures with the same property. Such proofs can be
obtained fully automatically when the functions are obtained
compositionally, without operating directly on the data structures. In
the remaining cases a human needs to provide the proof.

The first methodology is very accurate, but it requires human
intervention and it can be applied only when the alignments can be
simply expressed as relations between types and when everything is in
just one logic. The second methodology is less accurate, but somehow
more practical: it converts a proof into a sequence of intermediate
statements that need to hold, it translates each statement from one
system to the other ignoring the justification for the statement and
it fires an automated prover to fill the gap in the target system. By
varying the level of granularity the automated provers are allowed to
find alternative proofs, for example when some low-level properties of
data structure $X'$ are not available on data structure $X$, but a
different proof can still establish a statement that does not involve
them. Even when the provers fail to fill the gap the proof sketch
obtained can still be useful to the user that can try to manually fill
the gaps instead of restarting from scratch.

The task requires implementing multiple transformations and
translations of expressions containing binders (statements, proof
terms, etc.), which is well known to be a delicate task. ELPI,
developed by a join Ubo-Inr team, is a very high level programming
language of the logic programming family that allows to concisely
manipulate expressions with binders eliminating the most frequent
sources of mistakes (name capture, for example). ELPI comes with an
interpreter implemented in Ocaml that has been designed to be easily
integrated in other Ocaml based tools, like Coq. We plan to first
integrate ELPI in Dedukti so that Dedukti/Logipedia expressions can be
directly manipulated into ELPI. We also plan to implement means to
call external provers from ELPI via Logipedia translations. Then we
will use a mix of ELPI code and Dedukti rewrite rules to implement
alignment based proof and statement rewriting. Statement rewriting
will find direct application to alignment based search and browsing as
well.


%%% Local Variables:
%%%   mode: latex
%%%   ispell-local-dictionary: "english"
%%% TeX-master: "propB"
%%% End:
