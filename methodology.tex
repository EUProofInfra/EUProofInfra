Developing a new research infrastructure that integrates existing ones
through data exchange requires a wide range of activities, described
in the work-plan. These different activities will require different
methodologies: developing new pieces of software, modifying existing
ones, proving theorems, experimenting, etc.

We review here the different methodologies used in the project, again 
objective by objective.

\cmsection{Integration}

To integrate existing proof systems into Logipedia, we need to
instrument these systems to export their proofs to
Dedukti. The methodology required to achieve this goal varies a lot
with respect to the proof system considered. With respect to the
integration methodology, we distinguish three broad categories of
systems:

\begin{compactitem}
\item
Some existing proof systems, the ``Automath-like systems'', such as Coq, Agda, and
Matita, already have a notion of proof-term, that is an explicit
representation of proofs.  In theory, it is sufficient to translate
these terms into the language of Dedukti.  As discussed in the Section
\ref{concept}, in practice, this may require to reconstruct transient
proof components, or to modify the term language of the instrumented
system so that these components are included in the proof language
itself.

\item
Some other proof systems, the ``LCF-like systems'' such as
Isabelle/HOL, HOL4, and HOL Light, have a small kernel, that is the
only piece of code that needs to be instrumented. This requires some
expertise in the instrumented system, but, with this expertise, this
instrumentation is doable. In fact some of these systems already have
been instrumented and can export proofs expressed in a format called
OpenTheory. So, we can reap the benefit of these developments and just
translate the OpenTheory proofs to Dedukti.

\item A third category of proof systems require a different and more
  complex methodology. For these systems, such as PVS or Mizar, all
  that we
  can extract is a proof-trace, that is a sequence of propositions,
  such that the transition from one to the next is ``easy''.
  To fill the gaps between the propositions of the proof-trace,
  we thus need to use an external automated theorem prover. In
  addition, we need to instrument this automated theorem prover
  itself, so that we can build a complete proof from the trace.
\end{compactitem}

In all these cases, we need to take also into account that these proof
systems may rely on external tools to build automatically some parts
of the proofs, for instance automated theorem provers, SAT solvers,
SMT solvers, etc. In this case the external tool has to be
instrumented as well, to that it produces a proof that can be used by
the proof system to produce its own proof.

A methodological issue is that all proof systems are actively
developed systems that are constantly evolving, even if the theory
they implement evolves at a slower pace. In order to avoid having
translations that always work for an obsolete version, we need to
include these translators to Dedukti to the code of the systems
themselves, so that the translations are maintained together with
the proof system itself. This is our plan to migrate the
translations from Matita, Coq, and HOL Light to the systems
themselves, and all the other translations, as soon as they reach the
relevant maturity.

\cmsection{Automatic theorem provers}

Instrumenting automatic theorem provers, SAT/SMT, predicate solvers,
etc.
raises different methodological questions, as these systems
must be very efficient to be useful. This yields two new issues.

First, these systems include many optimisations that make their code
very complex and hence difficult to instrument. Then, instrumentation
often slows down the instrumented system slightly and if this is
acceptable for interactive proof systems---the user being anyway
slower than the software---this is often not acceptable for automated
theorem provers.
So, these systems must be instrumented in a minimal way, so that their
execution is not impacted by the instrumentation.  Most of the
time, we can just extract a proof trace, as in the third case above,
and fill the gaps in these proofs using another, less powerful but better
instrumented automated theorem prover.

Some automated theorem provers already output proof traces, meaning
that we can see them as black-boxes and concentrate on the translation
from these traces to Dedukti. To this end, we plan to extend the
prototype tool Ekstrakto to handle more proof formats and increase its
expressivity.
For the others, a preliminary task is to instrument them.  We target
proof formats that will be already integrated into Dedukti, in
particular TSTP, for automated theorem provers, and LSFC, for SMT solvers.

When an automated theorem prover is called from Dedukti to fill the
gap between two propositions, we need also to translate theorem
statements expressed in Dedukti, into the logic understood by
automatic provers. We will target a pivot language for which
translations to major existing automatic provers already exist, such
as TIP~\cite{DBLP:conf/mkm/ClaessenJRS15} or the input of
Why3~\cite{DBLP:conf/esop/FilliatreP13}. A research challenge is to
make this encoding preserve the logical meaning and deal with
statements coming from multiple tools. Our approach will be to build
the encoding by composing independent ``small'' encodings that can be
activated individually, according to which ones are needed to align
the concepts in the various tools with those of Logipedia.

In order to make use of the large amount of theorems stored in
Logipedia from automated theorem provers, we need tools to select the
knowledge relevant for particular goals. We will look at machine
learning methods to select relevant knowledge in the Logipedia corpus
as well as to select potential proof term components.

\cmsection{Large libraries}

Translating a large library from a given system to Dedukti is almost
an experimental science: in principle, if we can translate a small
library, we can translate a large one. In practice, things are more
complicated.  Hence, the first thing we need to do is to experiment
with the theories and the translations we do have. Most likely, some
efficiency issues will manifest, and we will have to understand where
these efficiency issues come from:

\begin{compactitem}
\item It may be because we are using a computer that is too slow or
  does not have enough memory.
\item It may be because Dedukti itself is too slow and we need to modify
  Dedukti itself, as we have done in the past to increase, for instance,
  the efficiency of Dedukti's conversion test.
\item It may be because the proof term produced by the translation is
  too big, as has happened in the past with translations that did not
  take sharing into account.
\item It may be because the expression of the theory itself has to be
  revised, as has happened in the past with the translation of
  inductive types.
\end{compactitem}

Depending on the result of this analysis, the computer we use, Dedukti,
the translation function, or the theory itself has to be improved, until
the full library can be handled.

\cmsection{Infrastructure}

Developing an infrastructure to make the proofs more easily accessible
raises different methodological questions. Here the technology of web
services, distribution package, ergonomic user interfaces, search
engines is mature and we need to rely on this existing technology.

Yet this raises two methodological issues. The first is that some of
these technologies must be adapted for the case of proofs. This is,
for instance, the case of the search engines technology, as plain text
search is not adapted to formal proofs. First because logical formulas
are not made of words, themselves made of letters, but directly of
symbols. Second because all formulas are, more or less made with the
same symbols in different orders---for instance the equal sign is in
every equation, so that it does not mean anything to search a formula
on the equal sign.

The second aspect which is both methodological and managerial is that
these technologies are mastered by other people than researchers and
engineers on formal proofs. So we need to develop interdisciplinary
teams, which we do by incorporating several partners in the consortium
who will bring this expertise.

\cmsection{Structuring the encyclopedia}

Structuring the encyclopedia raises the same methodological issues
as developing the infrastructure to make the proofs accessible.

The technology of ontologies, for instance, is mature. But is has been
developed in different contexts---for instance for medical
ontologies---and we need to understand how it can be adapted to formal
proofs.

Again, this technology is mastered by other people who are not
researchers and engineers on formal proofs. Fortunately, we have
succeeded to have in the consortium researchers who will bring their
expertise on this topic.

\cmsection{New theories, new systems}

Understanding how to express new theories in Dedukti is, from a
methodological point of view, closer to mathematics and logic.  The
main difficulty is to prove the properties (such as confluence,
termination, etc.)~of the proposed theories and to prove that they are
indeed a sound and complete expression in Dedukti of the original
theories.

This work can of course benefit from the existing expression of the
more mature systems described before: although every system has its
idiosyncrasies that require special techniques, there are also many
common aspects. In particular, the systems considered here are based
on similar foundations (more or less elaborate type theories, set
theories, etc.) as those considered above, and ideas that have proved
successful there can be transferred {\em mutatis mutandis}. For
instance, issues such as proof-irrelevance, induction, co-induction,
etc. are similar across theories and systems.

Yet, an interesting methodological issue, manifests when we attempt to
express new theories in Dedukti. In some cases, we do not succeed, but
we understand that a minor modification of Dedukti would help. There
is here a subtle balance to find between modifying Dedukti each time
we want to express a new feature in it, and having a rigid Dedukti
that would never evolve. As any logical framework, Dedukti should
remain simple enough and neutral with respect the various theories
it permits to express, but also flexible enough so that it evolves
according to the needs of its users.

\cmsection{Proof engineering}

Like the previous themes that also belong to joint research
activities, proof engineering first requires some mathematical
developments, for instance to establish that proofs in one theory can
or cannot always be translated to another theory. But there is also a
more empirical aspect in this work: for instance, although in
principle not all Matita proofs can be translated to Isabelle/HOL, we
have established that the full arithmetic library of Matita could. So
it is important to consider specific case studies, such as
constructivisation, elimination of universes, elimination of dependent
types, elimination of the axiom of choice, etc.

It is important to notice that Logipedia allows bridges to be built
between theoretical work and experimental work. For instance, there
are many mathematical algorithms, such as constructivisation
algorithms or algorithms to eliminate the axiom of choice that have
been developed in theoretical papers. {\bf Logipedia gives us a unique
opportunity to experiment with these algorithms for translating proofs
from a classical to a constructive theory on real size proofs that
have been developed for other purposes.}

Another methodological innovation we propose is that of small scale
concept alignment. For instance, a small step concept alignment might
be to identify that an algebraic structure, called $N$ in one theory,
and another, called $nat$ in a different theory, refer to the same
object, which means that these structures are isomorphic. Before
proving this isomorphism, either by hand or automatically, we need to
discover such potential isomorphisms. For this task, heuristics and
machine learning seem to be the right methods. So this work on
Logipedia is also part of a current to apply machine learning in
automated theorem proving and in formal proof technology. In some
sense, building a large encyclopedia paves the way to consider proofs
as big data and apply statistical methods to proofs, including machine
learning.

Finally, to conduct the project as a whole, the critical feed back
from the users is essential to have ``falsify'' wrong directions. This
is why the four club of users are a key element to our methodology.

%%% TeX-master: "propB"
%%% End:
%%% Local Variables:
%%%   mode: latex
%%%   mode: flyspell
%%%   ispell-local-dictionary: "british"
%%% End:
