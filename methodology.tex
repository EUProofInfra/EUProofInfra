The methodology is the same for integrating any library to Logipedia,
but due to a difference of readiness of the various systems we focus on,
our priorities are different.

\begin{enumerate}
\item We already know how to express most of the theories of Atelier B,
Coq, FoCaLiZe, HOL Light, HOL4, 
Isabelle, Matita, and Rodin in  Dedukti. We propose
to instrument these systems so that they can produce Dedukti
proofs that we can include in Logipedia.

\item For other theories, such as those of Abella, Agda, Lean, Minlog,
  Mizar, PVS, TLA+, the work is in progress, or not yet started.  So
  we must first understand how they can be expressed in Dedukti.

\item Besides the standard libraries of these systems, large libraries
  have been developed: the Isabelle Archive of formal Proofs \cite{AFP},
   Flyspeck \cite{Flyspeck}, MathComp\cite{Mathcomp}, 
  CompCert \cite{Compcert},  CakeML \cite{CakeML}, ...  We aim to include
  some of these libraries in Logipedia.
  
\item
Besides proof systems, we also want to include proofs coming from
automated theorem provers, SAT solvers, SMT solvers, and model
checkers.  So we must instrument some of these systems so that they
can produce Dedukti proofs that we can include in Logipedia.

\item
We want to develop algorithms to analyze which symbol, axiom, rewrite
rule is used in each proof, and consequently in which system each proof
can be used. We also want to develop algorithms to eliminate some of the 
symbols, axioms, and rewrite rules used in a proof in order to be able to 
use it in more systems.


\item
Each library imported in Logipedia will come with its own
definition of natural numbers, real numbers, etc. We want to develop
``concept alignments algorithms'' to transport theorems from one
structure to another isomorphic one.

\item 
Besides data, we propose to include in Logipedia, metadata and
an inner structure.
\end{enumerate}

\subsubsection{Methodology of work package 1: instrumentation}

We know how to express in Dedukti the theories implemented in Matita,
HOL4, Coq, Agda, Isabelle and Atelier B, and some of these systems
have been partially instrumented to export proofs that can be checked
in Dedukti. Our first work package is to complete this instrumentation
to be able to export most of the proofs of these systems. As a
consequence, the work includes a strong practical component.

Three methods have to be used here: some of the systems (Automath
style), such as Coq and Agda already have proof-terms that can be
output, thus the main task is to translate these proofs into the
Dedukti format. Others (LCF style), such as Isabelle and HOL4, have an
inference kernel that can be instrumented, the main task here is to
transform the internal proof-object into an external
proof-term. Others, such as Atelier B are slightly more difficult to
address. For those, we need to use the water ford method: extract an
incomplete trace (a sequence of lemmas) and fill the gap using
automated theorem proving, as experimented with Atelier B and Zenon.

\paragraph{Matita.}
The Krajano fork of Matita was the first system that could export
whole libraries to Dedukti, so the support for Matita is already at a
relatively high level. We plan to make this translation a part of the
code of Matita itself so that it is maintained with the rest of the
system.

Dedukti supports a core language that has less features than the one
of Matita. During the exportation the features of Matita, like
inductive types, are translated to this core language. In order to be
able to re-import from Dedukti using the higher order features,
Krajono needs to be extended in order to record metadata that allows
to invert the translation.

\paragraph{Coq.}
Previous attempts at extracting data from Coq without a direct
interaction with the developers of Coq resulted in prototypes like
CoqinE that quickly became outdated.  To overcome the problems
associated with external tools such as CoqInE, we plan to have the
required instrumentation merged in Coq proper and reuse it for other
projects that could benefit from it so to amortize its development
cost. More precisely E. Tassi is a core Coq developer acquainted with
its development process and he will take care of the integration of
the infrastructure in Coq and foster its reuse in third party projects
with similar needs such as SerAPI, CoqHammer and Coq-Elpi.

To implement the translation of Coq terms to Dedukti terms, there are
two additional problems that need to be solved. The first one is that
the encoding in Dedukti requires some information, typically types of
sub-expressions, that are not stored in Coq, it is transient. So the
instrumentation for steps (a) and (c) needs to be complemented by
providing not only access to existing data but also to log transient
data or re-synthesize it on demand. Both approaches may be used,
depending on the the tradeoff between computation time and space for
storage. The second problem, which is more critical, is that type
theory of Coq is extremely large, with features that have no
corresponding representation in Dedukti, the type theory of Coq needs
to be translated to a core one representable in Dedukti.  This
translation to a core calculus is not implemented in Coq and the
amount and complexity of code necessary for it is very significant and
indeed the CoqinE prototype only covers a small subset of Coq.

To make extra-logical data available in a structured and extensible
format, we plan to take advantage of the work done by Sacerdoti Coen
(UBo) in 2019 in exporting non trivial logical and extra-logical data
from Coq to an XML format. Data in that format was then translated by
Kohlhase et al (UBo + FAU) to the MMT system, another logical
framework to encode different logics and their libraries. We plan to
extend that format to include even more extra-logical data as well as
the data gathered in step (a) and (c). We shall evaluate if all the
data needed for step (b) can be saved in this format, and give us the
freedom to implement step (b) in a standalone tool making no requests
to Coq in order to re-synthesize missing data.



\paragraph{Agda.}
The main two issues with the instrumentation of Agda that were
discovered during the development of the prototype Agda2Dedukti were
(1) the reliance on type-directed conversion rules such as
$\eta$-equality and irrelevance and (2) the complex equational theory
of universe levels used for universe polymorphism. We plan to
investigate better approaches to translate type-directed conversion
rules, either by improving the encoding or by extending Dedukti
itself. To deal with universe level polymorphism, we plan to define a
sound and complete embedding of Agda's type of universe levels in
Dedukti, based on the existing work on encoding AC
(associative-commutative) theories.

\paragraph{Isabelle.}
We will revisit important aspects of the Isabelle/HOL logic
implementation on top of the Isabelle/Pure framework, such as
normalization of proofs, efficient type-class reasoning, special
representation of derived rules and definition principles (datatypes,
recursion, induction). The volume of proof term output may be reduced
further, by taking more structure of the target language (Dedukti)
into account and omitting certain low-level reasoning of HOL (e.g.\
for inductive types).
 %
The underlying Isabelle/ML implementation platform (on top of Poly/ML)
will be revisited as well, to improve monitoring of memory usage, and
to double the standard heap size from 16\,GB to 32\,GB (without
suffering from the full overhead of 64\,bit addressing).

\paragraph{HOL4.}
This part of the project will be about re-thinking and re-designing
the tools HOL4-to-OpenTheory and OpenTheory-to-Dedukti tools such that
they scale to the point where real examples of interest, such as those
mentioned in the concept, can be exported.


\paragraph{Atelier-B and Rodin.} \ldots


% A technological hurdle in the instrumentation of the systems under
% consideration is that all of them are actively developed systems
% that are constantly evolving.

\subsubsection{Methodology of the work package 4}

{\bf \large From automatic theorem provers to Dedukti}

We target to be able to obtain Dedukti proofs from as many established
automatic theorem provers as possible.

Some of them already output proof traces, meaning that we can see them
as black-boxes and concentrate on the translation from their proof format
to Dedukti. To this end, we plan to extend the prototype tool Ekstrakto
to handle more proof formats and increase its expressivity.

TODO: move here some parts from ``concept''

For those which do not output traces, or which output partial traces, a
preliminary task is to instrument them. We target in particular the
three SMT solvers Alt-Ergo, CVC4 and veriT, the two first-order provers
E-prover and Zipperposition, and the model-checker Cubicle. The
following table summarizes their degree of proof production, on a scale
from 0 (no proof production) to 5 (complete proof production, for all
features supported by the solver, and that can be independently checked
without proof reconstruction):
%
\begin{center}
\begin{tabular}{|r|c|c|c|c|c|c|}
\hline
 & Alt-Ergo & CVC4 & veriT & E-prover & Zipperposition & Cubicle\\
\hline
Level of proof production & 0 & 2 & 3 & 4 & 4 & 0\\
\hline
Proof format & - & LFSC & veriT & TSPT & TSTP & -\\
\hline
\end{tabular}
\end{center}
%
For solvers which are currently not proof producing, we target proof
formats that will be already integrated into Dedukti, in particular TSTP
and LSFC.

TODO: explain the methodology to instrument a prover


{\bf \large From Dedukti to automatic theorem provers}

The first step is to encode theorem statements expressed in Dedukti,
into the logic understood by automatic provers. We will target a pivot
language for which translations to major existing automatic provers
already exist, such as TIP~\cite{DBLP:conf/mkm/ClaessenJRS15} or the
input of Why3~\cite{DBLP:conf/esop/FilliatreP13}. A research challenge
is to make this encoding preserve the logical meaning and deal with
statement coming from multiple tools. Our approach will be to build the
encoding out of independent ``small'' encodings that can be activated or
not, making use of the concept alignment provided with the workpackage
6.

To handle the large amount of theorems stored in Logipedia, we need
tools to select the knowledge relevant for particular goals. We will
look at machine learning methods to select relevant knowledge in the
Logipedia corpus as well as to select potential proof term components.

We shall produce sets of benchmarks out of Logipedia, some of them
requiring to combine statements coming from different systems. These
benchmarks will be used to evaluate the tools developed here, and will
also make publicly available for automatic prover designers.

{\bf \large Large scale application: formal verification of C code}

The aforementioned methodology will be put at scale in the formal
verification of C code, through the instrumentation of the Frama-C and
Why3 platforms. To obtain end-to-end guaranties, we will output proofs
for all the steps used to validate C code with respect to specifications
(weakest precondition computation, logical transformations, user-defined
simplification rules, calls to automatic solvers) and combine them in
Dedukti. We will also supply the users and the solvers with the
Logipedia libraries.


{\bf \large Automatic theorem provers to increase Logipedia readiness}

We will develop internal automation for Dedukti, use it for proof
reconstruction, and look at machine learning methods to guide the
internal proof. In particular, we will experiment with direct search for
type inhabitants applied to the $\lambda\Pi$-calculus combined with
heuristic rewriting. We will also develop computational proof
reconstruction based on direct inspection of ATP proofs and translation
back to Dedukti terms.

This internal automation will be used to increase Logipedia readiness by
filling the remaining missing parts inside proofs and between proofs,
and discharging proofs of concept alignments discovered by the
workpackage 6. For this application, we need to qualify the class of
problems coming from making theorems coming from various sources, and
use or develop the more appropriate automation. Internal automation
combined with premise selection will also offer Dedukti users, inside
and outside the project, to more efficiently write proofs.


\subsubsection{Methodology of the work package 3}

\subsubsection{Methodology of the work package 9}

\subsubsection{Methodology of the work package 2}

The work to be carried out in this work package will benefit from the existing
encodings of the more mature systems considered in
\WPref{instrumentation}:\ednote{Have the WPs been explicitly introduced at this point?}
although every system has its idiosyncrasies that require special techniques,
there are also many common aspects. In particular, the systems considered here
are based on similar foundations (more or less elaborate type theories, set
theories, first-order or second-order predicate logic) as those considered in
\WPref{instrumentation}, and ideas that have proved successful there can be
transferred immediately. Also, improvements made to Dedukti in
\WPref{instrumentation} will benefit encodings of and proof checking for the
systems considered here, and conversely, foundational issues discovered in
encoding these systems can feed back into \WPref{instrumentation}. For example,
support for corecursion and coinduction, preferably based on a realizability
interpretation, are important for Coq, Agda, and Matita (in
\WPref{instrumentation}) as well as for Abella and Minlog.

There is also a strong relationship with \WPref{atpetc} because many of the
systems considered in this work package call on automatic back-end provers and
require instrumentation similar to the proof engines considered in
\WPref{atpetc} so that proofs can be exported to Dedukti. Even in a
type-theoretic system such as Abella, the Bedwyr and unification engines do not
produce proof objects in a form that is readily interpretable by Dedukti.
Additionally, some of the systems developed here may also make use of the native
Dedukti provers developed in \WPref{atpetc}.


\subsubsection{Methodology of the work package 5 + 6}

\paragraph{Aligning logical foundations}

Our first step will be to divide logical theories in clusters,
according to the key parameters mentioned above, and then to build a
web of syntactic translations between systems. We do not expect full
back-and-forth translations, as some systems are well-known to be
proof-theoretically stronger than others, but we will seek to
establish suitable equiprovability results for fragments of the
relevant languages.

We shall also deal with specific case studies to test our work.  For
example, to test our methods for translating classical proofs into
constructive ones, one could verify Michael Beeson's "wholesale
importation" (he uses the double negation translation to import all
the negative results from \cite{} to intuitionistic logic) using the
library of GeoCoq proofs.

\paragraph{Aligning theorem proving objects (case studies)}

We call big scale concept alignment the equivalence between different
axiom systems, and small scale concept alignment the equivalence (or
relationship) between different definitions.  Big scale concept
alignment is the alignment of different theories, usually expressed
using different axiom systems (eg., Tarski, Hilbert, Euclid) for
different geometries: euclidean and non euclidean. It also includes
alignment of different kind of analytic definitions of geometry i.e.,
between different analytic models (e.g., real closed fields, reals,
complex numbers), different definitions of projective
geometry. Porting GeoCoq to other proof assistant will bring some of
these big scale concept alignments.  Small scale concept alignment
requires proof of equivalence between different definitions of the
same concept in the same or different language.  To some extent this
task could be automated, but the difficulty is that some equivalences
are valid only in some contexts.

\paragraph{Automated theory alignment}

In one line of research, we set to use logic-deduction approaches that
have been developed for the automatic alignment of database schemas
and instances, such as \cite{}.  In order to align theories across
libraries, we will propagate alignment information, based on a
certified probabilistic inference engine, which will extend the work
in \cite{}. We envisage a collaboration with WP7, aimed at reusing the
ontology framework integrated in the core of Dedukti. Concretely, each
of the domain-specific meta-data, essentially the theory schemas, will
be specified in the ontology framework. Their validity with respect to
the underlying instances will be mechanically checked and the
alignment of the schemas will inform that of the underlying theories.

In the second line of research, we will employ machine learning
techniques, based on neural networks, to design heuristics for finding
new alignments.

\paragraph{Alignment-based services}

This task will be split into two lines of work corresponding to findability and reuse across prover libraries.
Firstly, an alignment-based search service will accept queries in any language $L$ encoded into Dedukti but will find results in any library imported into Dedukti, not just the one underlying $L$.
Secondly, a proof rewriting service will use alignments between two libraries to migrate proofs from one to the other.
Both applications depend alignment-induced expression translation.
All three components are described below.

\textbf{Expression Translation.}
We refine the existing definitions of alignments in a way that allows alignments creators to specify translation rules for expressions formed from the aligned operators.
Conceptually, we will provide a declarative user-friendly syntax for these.
But structurally, they will correspond to rules in the logic programming systems ELPI.
Thus ELPI can directly execute the translation function induced by a set of alignments.

\textbf{Search.}
We use the MathWebSearch system to perform unification queries in formal libraries.
It would be straightforward to index and search all libraries available in Dedukti in MathWebSearch.
However, then if concepts $a$ and $b$ from libraries $K$ and $L$ are aligned, every query about $a$ in the language of $K$ would only find results in $K$ but never from $L$.
By using the alignment-based expression translation, we can translate the user's query into other the language of $L$ (thus replacing in particular $a$ with $b$) and then run the translated query as well.
Thus, we can find results in all libraries.

Concretely, in the implementation, the UI will ask for a query written in Dedukti syntax together and provide a set of checkboxes indicating which libraries should be searched.
The system then translates the query into the selected libraries and displays all results.

\textbf{Proof Rewriting.} In the recent literature there are two main
approaches to alignment based proof rewriting. The first one, based on
logical relations, has been proposed to translate a proof about $X$
into a proof about $X'$ remaining in the same logic. A relation is
established between elements of $X$ and elements of $X'$ where, for
example, $X$ can be the type of sorted lists of numbers, $X'$ the type
of balanced search trees and the relation holds between data
structures that contain the same set of integers. Then,
oversimplifying, for each pair of corresponding functions acting
respectively on $X$, $X'$, it is shown that they map related elements
to related elements. Continuing the example, the function that inserts
a new integer into the sorted list and the one that does the same into
the balanced search tree map data structures that contain the same
elements to data structures with the same property. Such proofs can be
obtained fully automatically when the functions are obtained
compositionally, without operating directly on the data structures. In
the remaining cases a human needs to provide the proof.

The first methodology is very accurate, but it requires human
intervention and it can be applied only when the alignments can be
simply expressed as relations between types and when everything is in
just one logic. The second methodology is less accurate, but somehow
more practical: it converts a proof into a sequence of intermediate
statements that need to hold, it translates each statement from one
system to the other ignoring the justification for the statement and
it fires an automated prover to fill the gap in the target system. By
varying the level of granularity the automated provers are allowed to
find alternative proofs, for example when some low-level properties of
data structure $X'$ are not available on data structure $X$, but a
different proof can still establish a statement that does not involve
them. Even when the provers fail to fill the gap the proof sketch
obtained can still be useful to the user that can try to manually fill
the gaps instead of restarting from scratch.

The task requires implementing multiple transformations and
translations of expressions containing binders (statements, proof
terms, etc.), which is well known to be a delicate task. ELPI,
developed by a join Ubo-Inr team, is a very high level programming
language of the logic programming family that allows to concisely
manipulate expressions with binders eliminating the most frequent
sources of mistakes (name capture, for example). ELPI comes with an
interpreter implemented in Ocaml that has been designed to be easily
integrated in other Ocaml based tools, like Coq. We plan to first
integrate ELPI in Dedukti so that Dedukti/Logipedia expressions can be
directly manipulated into ELPI. We also plan to implement means to
call external provers from ELPI via Logipedia translations. Then we
will use a mix of ELPI code and Dedukti rewrite rules to implement
alignment based proof and statement rewriting. Statement rewriting
will find direct application to alignment based search and browsing as
well.

%%%%% methodology of work package 7
\subsubsection{WPtref{structuring}}

This work package will be carried out in three overlapping phases, each consisting of multiple tasks as described below.

\ednote{add milestone references when the milestones are finalized}

\paragraph{Framework Extensions}
The first phase designs and implements the necessary language extensions.
A first version of this implementation will become available by during the first 18 months.

\ednote{to be expanded}


\paragraph{Reference Ontology}
The second phase develops a reference ontology for formal libraries.
Its primitive concepts and properties will include both the formal libraries themselves and the content of informal research articles that they formalize.
Topically it will cover both software engineering and mathematics.

The ontology will be developed throughout the project.
Annual releases will be made available to the project partners to collect early feedback.
\ednote{to be expanded}

\paragraph{Library Imports}
The work package climaxes in the third phase that extends the existing imports of libraries in a way that makes use of that above.
Due to the enormous difficult of these imports, we will focus our attention drastically:
\begin{itemize}
  \item among formal libraries, we consider only the biggest and most widely used ones: the ones of Isabelle and Coq,
  \item among natural language research articles, we consider \ednote{@Senellart: write something here}   
\end{itemize}

\ednote{to be expanded}


\subsection{Readiness of the project}

This idea of building such a standard for proofs has already been
investigated in the past, such as in the Qed manifesto \cite{Qed94}, but
has produced limited results.

Our thesis is that, since the
Qed project, the situation has radically changed. After
thirty years of research, we have an empirical evidence that most of
the formal proofs developed in one of these systems can also be
developed in another. We understand the relationship between the
theories implemented in these systems much better. We have developed
several logical frameworks, extending predicate logic, in which these
theories can be expressed. And we have developed reverse mathematics
algorithms to analyze which axioms and rules are used in each proofs
and algorithms, such as constructivization algorithms, to translate
proofs from one theory to another.


%%% Local Variables:
%%%   mode: latex
%%%   ispell-local-dictionary: "english"
%%% TeX-master: "propB"
%%% End:
