Developing a new research infrastructure, by integrating existing
ones, through data exchange, requires a wide range of activities,
described in the work-plan. This different activities will require to
use different methodologies: developing new pieces of software,
modifying existing ones, experimenting, proving theorems, etc.

\cmsection{Integration}

The methodology to instrumenting existing proof systems so that they
can produce Dedukti proofs varies a lot in function of the considered
proof system.

\begin{itemize}

\item 
Some of them, the ``Automath-like systems'', such as Coq, Agda, and
Matita, already have a notion of proof-term, that is an explicit
representation of proofs.  In theory it is sufficient to translate
these terms into the language of Dedukti.  As discussed in the Section
\ref{concept}, in practice, this may require to reconstruct transient
proof components, or to modify the term language of the instrumented
system so that these components are included in the proof language
itself.

\item
Some other proof systems, the ``LCF-like systems'' such as
Isabelle/HOL, HOL4, and HOL Light, have a small kernel, that is the
only piece of code that needs to be instrumented. This requires some
expertise in the instrumented system, but, with this expertise, this
instrumentation is doable. In fact some of these systems already have
been instrumented and can export proofs expressed in a format called
OpenTheory. So, we can reap the benefit of these developments and just
translate the OpenTheory proofs to Dedukti.

\item
A third category of proof systems require a different and more complex
methodology. For these systems, such as PVS or Matita, all we can
extract is a proof-trace, that is a sequence of propositions, such
that the transition from one to the next is a ``easy'' just like
hoping from stone to the next is ``easy'' when crossing a river on a
ford. We thus need to use an external automated theorem prover to fill
the gaps between the propositions of the proof-trace, and we need this
automated theorem prover itself to be instrumented, so that we can
build a proof from the trace.
\end{itemize}

In all these cases, we need to take also into account that these proof
systems may call external tools, for instance automated theorem
provers, SAT solvers, SMT solvers, etc. to build automatically some
parts of the proofs. In this case the external tool has to be
instrumented as well, to that it produces a proof that can be used by
the proof system to produce its own proof.

A methodological issue is that all proof systems are actively
developed systems that are constantly evolving, even if the theory
they implement evolves at a slower pace. In order to avoid having
translations that always work for an obsolete version, we need to
include these translators to Dedukti to the code of the systems
themselves, so that the translations are maintained at the same pace
as the proof system itself. This is our plan to migrate the
translations from Matita, Coq, and HOL Light to the systems
themselves, and all the other translations, as soon as they reach the
relevant maturity.

\cmsection{Automated theorem provers, SAT/SMT solvers, model checkers,
  etc.}

Instrumenting Automated theorem provers, SAT solvers, SMT solvers and
model checkers raises different methodology questions, as these systems
must be very efficient to be useful. This yields two new issues.

First, these systems include many optimizations that make their code
very complex and hence difficult to instrument. Then, instrumentation
often slows down the instrumented system a little bit and if this is
acceptable for interactive proof systems---the user being anyway
slower than the software---this is often not acceptable for automated
theorem provers.

So, these systems must be instrumented in a minimal way, so that they
execution is not perturbed by the instrumentation. So in most of the
cases, we can just extract a proof trace, as in the third case above,
and fill the gaps in these proofs by using another, less powerful but better
instrumented, automated theorem prover, to fill the gaps. 

Some automated theorem provers already output proof traces, meaning
that we can see them as black-boxes and concentrate on the translation
from these traces to Dedukti. To this end, we plan to extend the
prototype tool Ekstrakto to handle more proof formats and increase its
expressivity.

For the others, a preliminary task is to instrument them.  We target
proof formats that will be already integrated into Dedukti, in
particular TSTP, for automated theorem provers, and LSFC, for SMT solvers.

When an automated theorem prover is called from Dedukti to fill the
gap between two propositions, we need also to translate theorem
statements expressed in Dedukti, into the logic understood by
automatic provers. We will target a pivot language for which
translations to major existing automatic provers already exist, such
as TIP~\cite{DBLP:conf/mkm/ClaessenJRS15} or the input of
Why3~\cite{DBLP:conf/esop/FilliatreP13}. A research challenge is to
make this encoding preserve the logical meaning and deal with
statement coming from multiple tools. Our approach will be to build
the encoding out of independent ``small'' encodings that can be
activated or not, making use of the concept alignment.

To handle the large amount of theorems stored in Logipedia, we need
tools to select the knowledge relevant for particular goals. We will
look at machine learning methods to select relevant knowledge in the
Logipedia corpus as well as to select potential proof term components.

\cmsection{Large libraries}

The methodology for translating large libraries is much more related
to that of experimental sciences: in principle, if we can translate a
small library, we can translate a large one. In practice, things are
more complicated.

So the first thing we need to do is to experiment with the theories
and the translations we do have. Most likely, some efficiency issue
will manifest and we will have to understand where these efficiency
issues come from.

\begin{itemize}
\item It may be because, we are using a computer that is too slow or
  does not have enough memory.
\item It may be because Dedukti itself is too slow and we need to modify
  Dedukti itself, as we have done in the past to increase, for instance,
  the efficiency of Dedukti's conversion test.
\item It may be because the proof term produced by the translation is
  too big, as has happened in the past with translations that did not
  take sharing into account, letting the size of the proof term 
  was too big.
\item It may be because the expression of the theory itself has to be
  revised, as has happened in the past with the translation of
  inductive types.
\end{itemize}

Depending on the result of this analysis, the computer we use, Dedukti,
the translation function, or the theory itself has to be improved, until
the full library can be handled.

\cmsection{Infrastructure}

Developing an infrastructure to make the proofs accessible raises a
different methodology questions. Here the technology of web services,
distribution package, ergonomic user interfaces, search engines is
mature and we need to rely on this existing technology.

Yet this raises two methodology issues. The first one is that some of
these technologies must be adapted for the case of proofs. This is,
for instance, the case of the search engines technology, as plain text
search is not adapted to formal proofs. First because logical formulas
are not made of words, themselves made of letters, but directly of
symbols. Second because all formulas are, more or less made with the
same symbols in different orders---for instance the equal sign is in
every equation, so that it does not mean anything to search a formula
on the equal sign.

The second aspect which is both methodological and managerial is that
these technology are mastered by other people than researchers and
engineers on formal proofs. So we need to develop interdisciplinary
teams, which we have succeeded, by incorporating several partners
in the consortium who will bring this expertise.

\cmsection{Structuring the encyclopedia}

Structuring the encyclopedia raises the same methodological issues
than developing the infrastucture to make the proofs accessible.

The technology of ontologies, for instance, is mature. But is has been
developed in different contexts---for instance for medical
ontologies---and we need to understand how it can be adapted to formal
proofs. 

Then, this technology is mastered by other people than researchers and
engineers on formal proofs. Fortunately we have succeeded to have in
the consortium researchers who will bring their expertise on this
topic.

\cmsection{New theories, new systems}

Understanding how to express new theories in Dedukti is, from a
methodological point of view, closer to mathematics and logic.  The
main difficulty is to prove the properties (such as confluence,
termination, etc.) of the proposed theories and to prove that they are
indeed a sound and complete expression in Dedukti of the original
theories.

This work can of course benefit from the existing expression of the
more mature systems above: although every system has its
idiosyncrasies that require special techniques, there are also many
common aspects. In particular, the systems considered here are based
on similar foundations (more or less elaborate type theories, set
theories, etc.) as those considered above, and ideas that have proved
successful there can be transferred {\em mutatis mutandis}. For
instance, issues such as proof-irrelevance, induction, co-induction,
etc. are similar across theories and systems.

Yet, an interesting methodological issue, manifests when we attempt to
express new theories in Dedukti. In some cases, we do not succeed, but
we understand that a minor modification of Dedukti would help. There
is here a subtle balance to find between modifying Dedukti each time
we want to express a new feature in it, and having a rigid Dedukti
that would never evolve. As any logical framework, Dedukti should
remain simple enough and neutral with respect the the various theories
it permits to express, but also flexible enough so that it evolves
according to the needs of its users.

\cmsection{Proof engineering}

Like the previous theme that also belong to joint research activities,
proof engineering first requires some mathematical developments, for
instance to establish that proofs in one theories can always, or not,
be translated to another theory.

But there is also a more empirical aspect in this work: for instance,
although, in principle not all Matita proofs can be translated to
Isabelle/HOL, we have established that the full arithmetic library of
Matita could. So it is important to consider specific case studies,
such as construvtivisation, elimination of universes, elimination of
dependent types, elimination of the axiom of choice, etc.

It is important to notice that Logipedia permits to build bridges
between theoretical work and experimental work here. For instance,
there are many mathematical algorithms, such as 
constructivisation algorithms or algorithms to
eliminate the axiom of choice that have been developed in theoretical
papers. Logipedia, allowing us to translate proofs from a classical to
a constructive theory, gives us a unique opportunity to experiment
with these algorithms on real size proofs that have been developed for
other purposes.

Another methodological innovation is that small scale concept
alignment, for instance noticing that an algebraic structure called
$N$ in one theory and another called $nat$ in another theory refer to
the same object, that is are isomorphic. Before proving this
isomorphism, either by hand or automatically, we need to discover such
potential isomorphisms. For this, machine learning, seems to be the
right method. So this work on Logipedia is also part of a current to
apply machine learning in automated theorem proving and in formal
proof technology. In some sense, building a large encyclopedia paves
the way to consider proofs as big data and apply statistical methods to 
proofs, including machine learning.

%%% Local Variables:
%%%   mode: latex
%%%   ispell-local-dictionary: "english"
%%% TeX-master: "propB"
%%% End:
