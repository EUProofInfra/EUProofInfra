The methodology is the same for integrating any library to Logipedia,
but due to a difference of readiness of the various systems we focus on,
our priorities are different.

\begin{enumerate}
\item We already know how to express most of the theories of Atelier B,
Coq, FoCaLiZe, HOL Light, HOL4, 
Isabelle, Matita, and Rodin in  Dedukti. We propose
to instrument these systems so that they can produce Dedukti
proofs that we can include in Logipedia.

\item For other theories, such as those of Abella, Agda, Lean, Minlog,
  Mizar, PVS, TLA+, the work is in progress, or not yet started.  So
  we must first understand how they can be expressed in Dedukti.

\item Besides the standard libraries of these systems, large libraries
  have been developed: the Isabelle Archive of formal Proofs \cite{AFP},
   Flyspeck \cite{Flyspeck}, MathComp\cite{Mathcomp}, 
  CompCert \cite{Compcert},  CakeML \cite{CakeML}, ...  We aim to include
  some of these libraries in Logipedia.
  
\item
Besides proof systems, we also want to include proofs coming from
automated theorem provers, SAT solvers, SMT solvers, and model
checkers.  So we must instrument some of these systems so that they
can produce Dedukti proofs that we can include in Logipedia.

\item
We want to develop algorithms to analyze which symbol, axiom, rewrite
rule is used in each proof, and consequently in which system each proof
can be used. We also want to develop algorithms to eliminate some of the 
symbols, axioms, and rewrite rules used in a proof in order to be able to 
use it in more systems.


\item
Each library imported in Logipedia will come with its own
definition of natural numbers, real numbers, etc. We want to develop
``concept alignments algorithms'' to transport theorems from one
structure to another isomorphic one.

\item 
Besides data, we propose to include in Logipedia, metadata and
an inner structure.
\end{enumerate}

\subsubsection{Methodology of work package 1: instrumentation}

We know how to express in Dedukti the theories implemented in Matita,
HOL4, Coq, Agda, Isabelle and Atelier B, and some of these systems
have been partially instrumented to export proofs that can be checked
in Dedukti. Our first work package is to complete this instrumentation
to be able to export most of the proofs of these systems. As a
consequence, the work includes a strong practical component.

Three methods have to be used here: some of the systems (Automath
style), such as Coq and Agda already have proof-terms that can be
output, thus the main task is to translate these proofs into the
Dedukti format. Others (LCF style), such as Isabelle and HOL4, have an
inference kernel that can be instrumented, the main task here is to
transform the internal proof-object into an external
proof-term. Others, such as Atelier B are slightly more difficult to
address. For those, we need to use the water ford method: extract an
incomplete trace (a sequence of lemmas) and fill the gap using
automated theorem proving, as experimented with Atelier B and Zenon.

% A technological hurdle in the instrumentation of the systems under
% consideration is that all of them are actively developed systems
% that are constantly evolving.

\subsubsection{Methodology of the work package 4}

{\bf \large From automatic theorem provers to Dedukti}

We target to be able to obtain Dedukti proofs from as many established
automatic theorem provers as possible.

Some of them already output proof traces, meaning that we can see them
as black-boxes and concentrate on the translation from their proof format
to Dedukti. To this end, we plan to extend the prototype tool Ekstrakto
to handle more proof formats and increase its expressivity.

TODO: move here some parts from ``concept''

For those which do not output traces, or which output partial traces, a
preliminary task is to instrument them. We target in particular the
three SMT solvers Alt-Ergo, CVC4 and veriT, the two first-order provers
E-prover and Zipperposition, and the model-checker Cubicle. The
following table summarizes their degree of proof production, on a scale
from 0 (no proof production) to 5 (complete proof production, for all
features supported by the solver, and that can be independently checked
without proof reconstruction):
%
\begin{center}
\begin{tabular}{|r|c|c|c|c|c|c|}
\hline
 & Alt-Ergo & CVC4 & veriT & E-prover & Zipperposition & Cubicle\\
\hline
Level of proof production & 0 & 2 & 3 & 4 & 4 & 0\\
\hline
Proof format & - & LFSC & veriT & TSPT & TSTP & -\\
\hline
\end{tabular}
\end{center}
%
For solvers which are currently not proof producing, we target proof
formats that will be already integrated into Dedukti, in particular TSTP
and LSFC.

TODO: explain the methodology to instrument a prover


{\bf \large From Dedukti to automatic theorem provers}

The first step is to encode theorem statements expressed in Dedukti,
into the logic understood by automatic provers. We will target a pivot
language for which translations to major existing automatic provers
already exist, such as TIP~\cite{DBLP:conf/mkm/ClaessenJRS15} or the
input of Why3~\cite{DBLP:conf/esop/FilliatreP13}. A research challenge
is to make this encoding preserve the logical meaning and deal with
statement coming from multiple tools. Our approach will be to build the
encoding out of independent ``small'' encodings that can be activated or
not, making use of the concept alignment provided with the workpackage
6.

To handle the large amount of theorems stored in Logipedia, we need
tools to select the knowledge relevant for particular goals. We will
look at machine learning methods to select relevant knowledge in the
Logipedia corpus as well as to select potential proof term components.

We shall produce sets of benchmarks out of Logipedia, some of them
requiring to combine statements coming from different systems. These
benchmarks will be used to evaluate the tools developed here, and will
also make publicly available for automatic prover designers.

{\bf \large Large scale application: formal verification of C code}

The aforementioned methodology will be put at scale in the formal
verification of C code, through the instrumentation of the Frama-C and
Why3 platforms. To obtain end-to-end guaranties, we will output proofs
for all the steps used to validate C code with respect to specifications
(weakest precondition computation, logical transformations, user-defined
simplification rules, calls to automatic solvers) and combine them in
Dedukti. We will also supply the users and the solvers with the
Logipedia libraries.


{\bf \large Automatic theorem provers to increase Logipedia readiness}

We will develop internal automation for Dedukti, use it for proof
reconstruction, and look at machine learning methods to guide the
internal proof. In particular, we will experiment with direct search for
type inhabitants applied to the $\lambda\Pi$-calculus combined with
heuristic rewriting. We will also develop computational proof
reconstruction based on direct inspection of ATP proofs and translation
back to Dedukti terms.

This internal automation will be used to increase Logipedia readiness by
filling the remaining missing parts inside proofs and between proofs,
and discharging proofs of concept alignments discovered by the
workpackage 6. For this application, we need to qualify the class of
problems coming from making theorems coming from various sources, and
use or develop the more appropriate automation. Internal automation
combined with premise selection will also offer Dedukti users, inside
and outside the project, to more efficiently write proofs.


\subsubsection{Methodology of the work package 3}

\subsubsection{Methodology of the work package 7}

\subsubsection{Methodology of the work package 9}

\subsubsection{Methodology of the work package 2}

\subsubsection{Methodology of the work package 5 + 6}

\paragraph{Aligning logical foundations}

Our first step will be to divide logical theories in clusters,
according to the key parameters mentioned above, and then to build a
web of syntactic translations between systems. We do not expect full
back-and-forth translations, as some systems are well-known to be
proof-theoretically stronger than others, but we will seek to
establish suitable equiprovability results for fragments of the
relevant languages.

We shall also deal with specific case studies to test our work.  For
example, to test our methods for translating classical proofs into
constructive ones, one could verify Michael Beeson's "wholesale
importation" (he uses the double negation translation to import all
the negative results from \cite{} to intuitionistic logic) using the
library of GeoCoq proofs.

\paragraph{Aligning theorem proving objects (case studies)}

We call big scale concept alignment the equivalence between different
axiom systems, and small scale concept alignment the equivalence (or
relationship) between different definitions.  Big scale concept
alignment is the alignment of different theories, usually expressed
using different axiom systems (eg., Tarski, Hilbert, Euclid) for
different geometries: euclidean and non euclidean. It also includes
alignment of different kind of analytic definitions of geometry i.e.,
between different analytic models (e.g., real closed fields, reals,
complex numbers), different definitions of projective
geometry. Porting GeoCoq to other proof assistant will bring some of
these big scale concept alignments.  Small scale concept alignment
requires proof of equivalence between different definitions of the
same concept in the same or different language.  To some extent this
task could be automated, but the difficulty is that some equivalences
are valid only in some contexts.

\paragraph{Automated theory alignment}

In one line of research, we set to use logic-deduction approaches that
have been developed for the automatic alignment of database schemas
and instances, such as \cite{}.  In order to align theories across
libraries, we will propagate alignment information, based on a
certified probabilistic inference engine, which will extend the work
in \cite{}. We envisage a collaboration with WP7, aimed at reusing the
ontology framework integrated in the core of Dedukti. Concretely, each
of the domain-specific meta-data, essentially the theory schemas, will
be specified in the ontology framework. Their validity with respect to
the underlying instances will be mechanically checked and the
alignment of the schemas will inform that of the underlying theories.

In the second line of research, we will employ machine learning
techniques, based on neural networks, to design heuristics for finding
new alignments.

\paragraph{Alignment based services}

\textbf{Expression Translation.}

\textbf{Search Service.}

\textbf{Proof Rewriting.} In the recent literature there are two main
approaches to alignment based proof rewriting. The first one, based on
logical relations, has been proposed to translate a proof about $X$
into a proof about $X'$ remaining in the same logic. A relation is
established between elements of $X$ and elements of $X'$ where, for
example, $X$ can be the type of sorted lists of numbers, $X'$ the type
of balanced search trees and the relation holds between data
structures that contain the same set of integers. Then,
oversimplifying, for each pair of corresponding functions acting
respectively on $X$, $X'$, it is shown that they map related elements
to related elements. Continuing the example, the function that inserts
a new integer into the sorted list and the one that does the same into
the balanced search tree map data structures that contain the same
elements to data structures with the same property. Such proofs can be
obtained fully automatically when the functions are obtained
compositionally, without operating directly on the data structures. In
the remaining cases a human needs to provide the proof.

The first methodology is very accurate, but it requires human
intervention and it can be applied only when the alignments can be
simply expressed as relations between types and when everything is in
just one logic. The second methodology is less accurate, but somehow
more practical: it converts a proof into a sequence of intermediate
statements that need to hold, it translates each statement from one
system to the other ignoring the justification for the statement and
it fires an automated prover to fill the gap in the target system. By
varying the level of granularity the automated provers are allowed to
find alternative proofs, for example when some low-level properties of
data structure $X'$ are not available on data structure $X$, but a
different proof can still establish a statement that does not involve
them. Even when the provers fail to fill the gap the proof sketch
obtained can still be useful to the user that can try to manually fill
the gaps instead of restarting from scratch.

The task requires implementing multiple transformations and
translations of expressions containing binders (statements, proof
terms, etc.), which is well known to be a delicate task. ELPI,
developed by a join Ubo-Inr team, is a very high level programming
language of the logic programming family that allows to concisely
manipulate expressions with binders eliminating the most frequent
sources of mistakes (name capture, for example). ELPI comes with an
interpreter implemented in Ocaml that has been designed to be easily
integrated in other Ocaml based tools, like Coq. We plan to first
integrate ELPI in Dedukti so that Dedukti/Logipedia expressions can be
directly manipulated into ELPI. We also plan to implement means to
call external provers from ELPI via Logipedia translations. Then we
will use a mix of ELPI code and Dedukti rewrite rules to implement
alignment based proof and statement rewriting. Statement rewriting
will find direct application to alignment based search and browsing as
well.


\subsection{Readiness of the project}

This idea of building such a standard for proofs has already been
investigated in the past, such as in the Qed manifesto \cite{Qed94}, but
has produced limited results.

Our thesis is that, since the
Qed project, the situation has radically changed. After
thirty years of research, we have an empirical evidence that most of
the formal proofs developed in one of these systems can also be
developed in another. We understand the relationship between the
theories implemented in these systems much better. We have developed
several logical frameworks, extending predicate logic, in which these
theories can be expressed. And we have developed reverse mathematics
algorithms to analyze which axioms and rules are used in each proofs
and algorithms, such as constructivization algorithms, to translate
proofs from one theory to another.


%%% Local Variables:
%%%   mode: latex
%%%   ispell-local-dictionary: "english"
%%% TeX-master: "propB"
%%% End:
