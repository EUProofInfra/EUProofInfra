\bigskip

\noindent
{\bf \Large \underline{Logical Frameworks}}

\medskip

We have said that the multiplication of systems and theories in which
formal proofs are developed jeopardizes the universality of logical
truth. But it is not the first time in history that the universality
of logical truth is challenged.  In the 
19$^{\mbox{\footnotesize th}}$ century, for instance,
the universality of logical truth had been challenged in a similar
way, by the non-Euclidean geometries, as some statements could be true
in one geometry, but false in others.  At the beginning of the 20$^{\mbox{\footnotesize th}}$ 
century, a solution to this problem was found.  The definition of the
various geometries in predicate logic \cite{HilbertAckermann}
permitted to understand which axiom was used in which proof, restoring
the universality of the truth of judgements of the form ``the
proposition $B$ is true under the axioms $A_1, ..., A_n$''.

Predicate logic is not {\em per se} a theory, but it is a framework in
which one can express theories, as sets of axioms: {\em a logical
  framework}. As we shall see, expressing the various theories
implemented in Coq, Matita, Isabelle/HOL, PVS, etc.  in a common
logical framework will permit, in a similar way, to analyze which
``axiom'' is used in which proof, and hence, in which theories this
proof is correct, and in which systems it can be used.

In 1928, predicate logic was a huge success, since three important
theories used at that time (geometry, arithmetic, and set theory)
could be expressed in it. But it also has limitations, which explains
that another of the major theories used at that time (Russell's type
theory, from The Principia Mathematica) has not been expressed in
it. Since then, several other theories, such as Church's type theory
\cite{Church40}, Martin-L\"of's type Theory \cite{Martin-Lof84}, and
the Calculus of constructions \cite{CoquandHuet88}, have also been
defined as autonomous theories, and not in predicate logic.  These
theories are those implemented in most of the current proof systems.

This failure has led, in the field of proof systems, to the
abandonment of predicate logic, and even of the concept of logical
framework: the theories implemented in Coq, Matita, etc., are often
defined as autonomous systems, and not in a logical framework.

However, a different line of research has attempted to understand the
limitations of predicate logic and to propose other logical frameworks
repairing them. The most prominent limitations of predicate logic are
the lack of function symbols binding variables, the lack of a syntax
for proof terms, the lack of a notion of computation, the lack of a
notion of proof reduction for axiomatic theories, and the
impossibility to express constructive proofs. These limitations have
led to the development of other logical frameworks such as
$\lambda$-Prolog \cite{NadathurMiller88, MillerNadathur12}, Isabelle
\cite{Paulson90}, the $\lambda \Pi$-calculus (also called the
``Edinburgh logical framework'') \cite{HarperHonsellPlotkin91},
Deduction modulo theory \cite{DowekHardinKirchner03, DowekWerner03},
Pure Type Systems \cite{Berardi88,Terlouw89}, and ecumenical logics
\cite{Prawitz15,Dowek15,PereiraRodriguez17}.

All these logical frameworks have been unified in the $\lambda
\Pi$-calculus modulo theory \cite{CousineauDowek07}, implemented in
the system Dedukti \cite{Assaf16} on which Logipedia is
based. Geometry, arithmetic and set theory, but also Russell's type
theory, Church's type theory, Martin-L\"of's type theory, and the
Calculus of constructions can be expressed in this framework.

For instance, although the details are not important, Church's type
theory can be expressed in Dedukti, with 11 ``axioms'': 8 declarations
and 3 rewrite rules.

\begin{framed}
\begin{center}
{\bf \Large Church's type theory expressed in Dedukti}
\end{center}

$\begin{array}{rcl}
type&:&Type\\
\eta&:&type \rightarrow Type\\
o&:&type\\
\mbox{\it nat}&:&type\\
\mbox{\it arrow}&:&type \rightarrow type \rightarrow type\\
\varepsilon&:&(\eta~o) \rightarrow Type\\
\Rightarrow&:&(\eta~o) \rightarrow (\eta~o) \rightarrow (\eta~o)\\
\forall&:&\Pi a:type~(((\eta~a) \rightarrow (\eta~o)) \rightarrow (\eta~o))\\
\\
(\eta~(\mbox{\it arrow}~x~y)) &\longrightarrow& (\eta~x) \rightarrow (\eta~y)\\
(\varepsilon~(\Rightarrow~x~y)) &\longrightarrow& (\varepsilon~x) \rightarrow (\
\varepsilon~y)\\
(\varepsilon~(\forall~x~y)) &\longrightarrow& \Pi z:(\eta~x)~(\varepsilon~(y~z))
\end{array}$
\end{framed}

So the theories implemented in various systems can all be expressed in
Dedukti and the proofs developed in these systems can be translated to
Dedukti. Just like in the case of non Euclidean geometry, this allows
to analyse the symbols and rewrite rules used in each proof
\cite{Thire18,Dowek17} (a domain traditionally called ``reverse
mathematics'' \cite{Friedman76,Simpson09}) and to deduce in which
systems each proof can be used.  This analysis is the basis of the
interoperability between proof systems.

\bigskip

\noindent
{\bf \Large \underline{The Logipedia integration levels}}

\medskip

Thus, to make a formal proof, developed in some system $X$, accessible
to the users of other systems, the first step is to express the theory
$D[X]$, implemented in the system $X$, in the logical framework
Dedukti.  Then, we must instrument the system $X$ so that the proof
can be exported from it, as a piece of data, expressed as a proof in
$D[X]$ and included in Logipedia. Next, we need to analyze this proof
in order to determine which symbols, axioms and rewrite rules of
$D[X]$ it actually uses and, thus, in which alternative theories it
can be expressed.  Finally, we must align its concepts with the
definitions already present in Logipedia and decide where it fits in
the general structure of the encyclopedia.

To measure the level of integration of an existing proof system and
associated proof library in Logipedia, we introduce a metric: {\em the
  Logipedia integration levels} that counts six levels.

\begin{framed}
\begin{center}
{\bf The Logipedia integration levels (LIL)\label{lil}}
\end{center}

\begin{itemize}
\item[LIL 1:] The theory implemented in the system has been defined in
  the $\lambda\Pi$-calculus modulo theory and in Dedukti.

\item[LIL 2:] The system has been instrumented so some of its proofs
  can be exported and checked in Dedukti.

\item[LIL 3:] A significant part of the library of the system has been
  exported and checked in Dedukti.

\item[LIL 4:] A significant part of the library of the system have
  been made available in Logipedia.

\item[LIL 5:] A tool has been defined to analyze the Dedukti proofs
  for the system, detect those that can be expressed in a theory
  weaker than that of the system, and translate those proofs into a
  weaker logic.

\item[LIL 6:] All proofs of the system have been exported, translated,
  and made available in Logipedia.
\end{itemize}
\end{framed}

The eighteen systems addressed in this project currently have different
integration levels, and we have different targeted levels for them.

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
System & current level & targeted level\\
\hline
Matita & 5 & 6\\
\hline
HOL Light & 3 & 5\\
\hline
FoCaLiZe & 3 & 5\\
\hline
Coq & 3 & 5\\
\hline
Agda & 2 & 4\\
\hline
Atelier B & 1 & 5\\
\hline
ProB & 1 & 5\\
\hline
Isabelle & 2 & 5\\
\hline
HOL4 & 1 & 5\\
\hline
TSTP & 1 & 4\\
\hline
Smart & 1 & 3\\
\hline
Rodin & 1 & 3\\
\hline
Minlog & 0 & 4\\
\hline
PVS & 0 & 2\\
\hline
Mizar & 0 & 4\\
\hline
TLA$^+$ & 0 & 2\\
\hline
Why3 & 0 & 4\\
\hline
LFSC & 0 & 4\\
\hline
\end{tabular}
\end{center}

These systems can be roughly divided into two groups. Those in the
first half of the table (Matita, HOL Light, FoCaLize, Coq, Agda,
Atelier B, ProB, Isabelle/HOL, HOL4, TSTP, Smart, and Rodin) for which
we have preliminary results and that we plan to bring to a very high
level of integration. Those in the second half (Minlog, PVS, 
Mizar, TLA$^+$, Why3, and LFSC) with which we are
starting and for which our goal are less ambitious.

These two groups of systems will be addressed in different work
packages, but both are key to the project. The first ones will
constitute Logipedia in 2024 and the second ones prepare the long term
future of the infrastructure.

\bigskip

\noindent
{\bf \Large \underline{Integration}}

\medskip
  
Consider a system $X$ at LIL 1, so the basic theory $D[X]$ has already
been expressed in Dedukti. The next step is to implement a method to
automatically translate proofs from system $X$ to its expression in
Dedukti, and make these proofs available in Logipedia so they can be
exported to other systems. This step is called the
\emph{instrumentation} of system $X$.

The systems we consider fall into three broad classes: the systems
based on dependent type theory, the systems based on simple type
theory, and those based on set theory and predicate logic.

\paragraph*{Systems based on dependent type theory (Agda, Coq, and Matita).}

Coq is an interactive theorem prover developed at Inria since 1984.
It is based on Type Theory and was used to formally verify the
correctness of both industrially relevant software such as the
CompCert C compiler and complex mathematical proofs such as the one of
the Four Color theorem and the one of the Odd Order theorem. In 2013
Coq received the ACM system award.

Matita is an interactive theorem prover developed at the University of
Bologna and used for teaching logic courses and to verify software and
mathematical proofs, with special attention to predicative
foundations. The first generation of the system (up to version 0.5.9)
was born as a by-product of the MoWGLI FET-Open Project, it was
compatible with the logic of Coq and it could re-use its libraries. It
was an important test-bench for the integration of Mathematical
Knowledge Management techniques with Interactive Theorem Proving,
featuring for example a library of theorems distributed over multiple
servers, innovative indexing and search techniques and automatic
translation of proofs between declarative and procedural styles. The
second generation of the system (up to the current version 0.99.3) was
a re-implementation from scratch that departed from the logic of Coq
and that experimented with the most concise ways to implement an
efficient theorem prover. Several ideas later migrated into Coq. The
currently available largest library is the formal certification of a
complexity-preserving and cost-model-inducing compiler from C to
MCS-51 machine code, developed in the FET project CerCo (Certified
Complexity).

Agda is a dependently typed programming language and interactive proof
assistant developed at Chalmers University of Technology as well as
other places in Europe. The theory of Agda is similar to Coq and
Matita, but is more focused on interactive development and direct
manipulation of proof terms (in contrast to using a tactic language to
generate the proof terms). To support the construction of proof terms,
Agda provides powerful features such as dependent pattern and
copattern matching, eta equality for functions and record types,
first-class universe polymorphism, and definitional proof
irrelevance. In addition, Agda provides an experimental option for
extending the language with user-defined rewrite rules, which are very
similar to the rewrite rules provided by Dedukti.

\paragraph*{Systems based on simple type theory (HOL4, HOL Light, and Isabelle/HOL).}

HOL4 is home to a few medium to large scale specifications and
associated proof developments that have value outside of HOL4. These
specifications include the formal semantics of the CakeML language
(and its verified compiler) and an extensive specification of the ARM
instruction set architecture (ISA) as formalised by Anthony Fox at the
University of Cambridge.

HOL Light is an implementation of Simple type theory.

Isabelle/HOL is an implementation of Simple type theory in the
Isabelle logical framework \cite{paulson700} is an intermediate
between Type-Theory proof systems (like Coq or Agda) and classic
LCF-style systems (like HOL Light or HOL4).

  \paragraph*{Systems based on set theory and predicate logic (Atelier B,  Rodin, ProB, Smart).}

Atelier B, Rodin and ProB are platforms or tools to develop models written
using the B method, a version of set theory expressed in predicate
logic.  A model in these systems expresses a state machine constrained
by invariant properties. Verification of the model correctness implies
to verify some proof obligations produced by a weakest precondition
calculus. For example, spanning trees algorithms, distributed
algorithms, access control policies have been formalized
respectively in EventB and B method.  The development process for
the B method is based on formal proof: proof obligations are
automatically generated and must be proven by automatic or interactive
provers. This can include external solvers such as SMT solvers, and
(in the case of ProB) constraint solvers such as Sicstus
Prolog. Finally, ProB is an animator and model checker: it helps users
to gain confidence in their specifications, and it can be used as a
dis-prover to discover counter-examples to proof obligations.
  
Smart is a proof language developed by the company ProveNRun. It is
based on a classical predicate logic, extended with rank-1
polymorphism, algebraic datatypes, recursive definitions and inductive
predicates. ProvenTools will turn models implemented in Smart to proof
obligations which can be proved in the system using a mix of
interactive and automated proving. The resulting proofs are reviewable
objects, akin to proof traces built of the various atomic proof rules
supported by ProvenTools' kernel, such as definition unfolding, case
analysis or equality propagation.

\bigskip
  
Despite the differences between these three classes of proof systems,
there are several common points between them where solutions that work
for one system can also be applied to other systems. Working together
on implementing these tools will allow the researchers from these
different communities to come together and learn more about the
similarities and differences between these systems.

Instrumenting some of these systems is straightforward, but this also
raises new research challenges:


\paragraph*{Improving encodings.}
Even when we have understood how to express the basic theory of these
systems, implementation often reveals some advanced features of these
systems that require further work.  Moreover even when we know how to
express all features of some system \emph{in theory}, depending on the
choice of expression, the instrumentation may be simpler or harder in
practice. For example:
\begin{itemize}

\item Many Agda libraries (as well as some Coq libraries) rely heavily
  on type-directed conversion rules such as eta-equality for functions
  and record types, or definitional proof irrelevance. These rules
  have to be expressed by adding additional type information to the
  proof terms. This can hence lead to a large blow-up in the size of
  those proof terms, and thus greatly increase the cost of
  typechecking, thus this blow-up has to be tamed by the choice of a
  clever expression of the theory.

\item Coq, Agda, and Matita all provide support for coinductive
  (infinite) structures, which can be expressed by rewrite rules in
  Dedukti, but this expression must be carefully chosen so that the
  proofchecker does not go into an infinite loop.

\item A few of the systems rely on AC rewriting (i.e.~rewriting modulo
  associativity and commutativity of certain operations), for example
  to express universe polymorphism in dependent type theories. While
  Dedukti has support for AC rewriting, the current implementation is
  very slow, which makes typechecking the proofs in Dedukti
  unfeasible. Thus, expressing these theories requires to improve
  Dedukti itself.
\end{itemize}

Finding better ways to express such features could have a great impact
on the quality and speed of the translation process.  We plan to
investigate two possible approaches to improve the expression of
common features of proof assistants in Dedukti. First, we will
investigate whether some information in the current expressions is
redundant and can thus be omitted. Second, if this is not possible we
will investigate what minimal changes need to be made to Dedukti
itself in order to overcome these limitations.

\paragraph*{Reconstructing transient proof components.}
In all proof systems, there is some information that is created during
the process of checking a proof that is not stored in the final proof
term. In systems such as Coq, Agda, and Matita, this concerns for
example the types of each subexpression. In other systems such as
HOL4, Isabelle, and Atelier B, even more information about the proof
is discarded and only the high-level proof steps remain.  However, the
translation from the system to Dedukti might rely on this transient
information. Developing a method to reconstruct this transient
information is thus crucial.

In order to instrument systems with transient proof information, the
proof terms need to be complemented with this transient information by
either logging or re-synthesizing it on demand. Both approaches may be
used, depending on the the trade-off between computation time and space
for storage.

However, this approach is insufficient for systems that rely on
external provers such as Isabelle or Event B. In order to handle these
cases, we will instrument these external provers so they can produce
the missing parts of the proof terms.

\paragraph*{Producing more compact proofs.}
When translating a proof from some external system to Dedukti, we want
to produce proof terms that are as small as possible, so it is easy to
store them, recheck them, or export them to a different
system. However, there are at least two reasons why proof terms might
become large. First, a typical proof in external system may be large
(because big parts of it are constructed automatically). Second,
translating a proof to Dedukti may increase the size of the proof, for
example by normalizing it or by adding type annotations.  This means
translating the proofs can take a very long time, and doing anything
with the translated proof will be difficult. This also has an obvious
impact on the exporting of large libraries to Dedukti, discussed
below.

To reduce the size of proof terms produced by the translation to
Dedukti, we plan to investigate how to avoid unnecessary
normalization or duplication of (parts of) proofs. We will also
investigate what parts of each proof can be safely omitted because
they can be inferred from the rest of the proof.

\bigskip  

For each of the systems considered in this project, we can identify
some preliminary work, of various maturities, on translating proofs to
Dedukti.  We plan to build upon the previous work done on the
following tools:
\begin{itemize}
\item The standard and arithmetic libraries of Matita has been the
  first libraries to be exported to Logipedia using Krajono, a fork of
  Matita. We plan to make this translation a part of the code of
  Matita itself so that it is maintained with the rest of the system.

\item The standard library of HOL Light and FoCaLiZe already have been
  translated to Dedukti.

\item CoqInE is a prototype tool that can translate Coq proofs to
  Dedukti. Recent work to include advanced features of Coq, such as
  universe polymorphism, has dramatically increased the coverage of
  this translation. We plan to make this translation a part of the
  code of Coq itself so that it is maintained with the rest of the
  system.

\item In the summer of 2019, Guillaume Genestier worked together with
  Jesper Cockx on the implementation of an experimental translator
  from Agda to Dedukti during a research visit at Chalmers University
  in Sweden. This translator is still work in progress, but it is
  already able to translate 142 modules of the Agda standard library
  (about 25\%) to a form that can be checked in Dedukti. This
  exploratory work uncovered several challenges and opportunities for
  further work (see research challenges above).

\item The inference kernel of Isabelle has already been instrumented
  to output proofs as $\lambda$-terms that can be understood by
  Dedukti. However, this has so far been only used for small examples
  \cite{Berghofer-Nipkow:2000:TPHOL}. The challenge is to make
  Isabelle proof terms work robustly for the basic libraries and
  reasonably big applications.  Preliminary work by Wenzel (2019) has
  demonstrated the feasibility for relatively small parts of
  Isabelle/HOL, but this requires scaling up.

\item HOL4 has support for exporting proofs to the OpenTheory proof
  exchange format, and there has been some work on importing
  OpenTheory proofs into Dedukti.

\item In the context of the BWare project, an expression of the set
  theory of the B method has been provided as a theory modulo, i.e. a
  rewrite system rather than a set of axioms. This expression is used
  by the automatic prover Zenon modulo which features a back-end to
  Dedukti. Thus, as a first step through instrumentation of Atelier B
  proof obligations coming from Atelier B can be proved by Zenon
  modulo producing Dedukti proofs, hence providing a better confidence
  in the proofs produced by the native proof tools of Atelier B
  \cite{Bware}.
\end{itemize}

\subsubsection*{Success}

The success of this integration project is measured by the LIL of the various
systems discussed at the end of the project.

\bigskip

\noindent
{\bf \Large \underline{Automatic theorem provers, SAT/SMT solvers, model checkers, etc.}}

\medskip

We have previously discussed the instrumentation of proof systems,
such as Coq, Isabelle/HOL, Agda, etc.  that is interactive systems,
where the users build formal proofs with the help of the
machine. Automatic Theorem Proving are another class of systems, where
the machine builds proofs without any human intervention. The proofs
built by these systems are often expressed in simpler theories than
those developed using proof systems, but they are not of a different
nature.  They cover various domains and various kinds of application,
{\em e.g.}  combinatorial
mathematics~\cite{DBLP:journals/ai/KonevL15,DBLP:conf/sat/HeuleKM16},
where they are expected to solve one large propositional problem, or
proof of
programmes~\cite{DBLP:conf/esop/FilliatreP13,DBLP:journals/pacmpl/ProtzenkoZRRWBD17},
where they are given thousands of small problems in a combination of
quantified theories.

Including, in Logipedia, proofs built by such automatic systems,
whether they are called automatic theorem provers, SAT solvers, SMT
solvers, etc. is both a goal {\em per se}, and a tool to help to
integrate proofs developed in proof systems.  For instance we will use
automatic theorem provers to automatically make a coherent whole out
of Logipedia. A fruitful interaction between formal proofs requires
low-level glue which falls in the scope of automatic theorem provers.
For instance, they will be employed to fill the holes that appear when
considering provers with various granularity, to reduce the gaps
between proof systems, and to discharge proofs of concept alignment.

Finally, automatic theorem provers will also benefit from
Logipedia. Obviously, Logipedia will be an extensive library of formal
statements that can be used and combined by automatic theorem provers
in their proof search. This is not trivial though: lemma selection is
crucial to avoid an overhead. It can also be a source of benchmarks to
evaluate their expressivity and automation.  Lastly, Logipedia and
Dedukti will form a framework to make automatic theorem provers
cooperate with each other and with other tools, in a safe way.


\subsubsection*{From automatic theorem provers to Dedukti}

Similarly to Interactive Theorem Provers, connecting automatic theorem
provers to the Logipedia infrastructure strongly relies on the ability
for automatic theorem provers to import statements from Dedukti and
export proofs in some theory in Dedukti.


\paragraph*{Instrumenting automatic theorem provers to produce proof traces.}
The first step for connecting automatic theorem provers to the
Logipedia infrastructure, and the library of proofs, is that automatic
theorem provers should actually output some kind of proof, without
enforcing strong requirement on the format or even the level of
granularity of those proofs.  SAT (Satisfiability) solvers for
propositional logic do have a perfectly well specified format for
proof traces~\cite{TODO} and most SAT solvers are actually able to
produce proof traces in that format.  So, considering SAT solvers, the
current status is actually meeting the Logipedia needs on this aspect.

Considering other automated reasoners however, it is mandatory to
significantly improve the picture.  Some SMT (Satisifiability Modulo
Theories) solvers do produce some kind of proof trace, but the format
is specific to the solver, and the proofs are sometimes difficult to
replay due to their granularity.  Most mainstream theorem provers for
predicate logic output proofs in a standardized language~\cite{TODO},
but this language does not clearly specify the semantics of the proof
steps.  Model-checkers do not provide proof traces per se, although
such traces would be useful for various usages and notably
certification.

Our goals are to improve reasoning tools to demonstrate the
feasibility to produce sufficiently detailed proofs for connecting to
Logipedia, and to design a set of theoretical methods and practical
tools that can be used to further connect Logipedia to the other
existing automated reasoners of the same kind.  We will work on three
SMT solvers (alt-ergo, CVC4, veriT), two provers for predicate logic
(the E-prover, Zipperposition), and one model-checker (cubicle).  We
will also consider more specific reasoning tools, with the aim to
demonstrate that this approach also applies to more specific
reasoners.  We envision to experiment the approach on a Coherent Logic
reasoner.  These reasoners are particularly useful for geometry
reasoning, and as such, they are quite complementary to the other
considered automatic reasoners.  They are also very suitable as as a
first experiment, since their proofs are fairly straightforward to
interpret.  They thus constitute a very interesting low hanging fruit.
  
It is not possible, and not even desirable, to require all tools to
directly talk in the language of Logipedia.  Indeed, proof trace
languages that are specific to one kind of reasoning tool are more
appropriate than Dedukti for instrumenting already large pieces of
software, enabling quick output, and allowing proof post-processing at
the right level of abstraction on the produced proof traces.
Furthermore, provided that those proofs are detailed enough,
translation of traces to Dedukti will not be a difficult task, and a
the work necessary to translate proof traces for a myriad of very
different reasoners will be implemented in a unique tool (Ekstrakto,
see below) to take advantage of the fact that reasoning techniques,
and thus proof methods, are themselves pretty shared among the
solvers.

\paragraph*{Translate automatic theorem provers traces into Dedukti.}
As pointed out above, it is easier to instrument provers to make them
output traces instead of directly provide Dedukti proofs. The second
step to connect automatic theorem provers to the Logipedia
infrastructure is to reconstruct the proof traces in order to build
Dedukti proofs from them. The proposed process is the following: each
step of the trace is transformed into an independent sub-problem; each
of these sub-problems is given to a prover that can output Dedukti
proofs; proofs of the sub-problems are then combined to produce a
global proof of the original problem.  Since sub-problems correspond to
atomic steps of the proof trace, they are relatively simple, so that
we are confident that the prover producing Dedukti proofs will not
struggle to find a proof. This process is quite similar to what is
done by the hammer tools of interactive theorem provers (Sledgehammer
in Isabelle/HOL, HOLyHammer for HOL4, etc.) which reconstruct proofs
from traces produced by automated theorem provers.

This scheme has already been prototyped in a tool called
Ekstrakto. Ekstrakto takes a TSTP file, as can be produced by
e.g.\ the provers E and Zipperposition, and it uses Zenon Modulo and
ArchSAT to prove the sub-problems. Ekstrakto was designed to be
agnostic w.r.t.\ the prover producing the trace; in particular it does
not depend on the specific set of inference rules of the prover. It
was also designed to be agnostic w.r.t.\ the prover used to prove the
sub-problems; it is only required that the prover can output a Dedukti
proof in the correct expression of predicate logic.

Although Ekstrakto has already shown that it is a valuable approach,
it is a work in progress. In particular, the following issues will be
addressed in the project:

\begin{enumerate}
  % extension to other proof trace formats
\item Up to now, Ekstrakto can only understand traces in TSTP format as
  input. We plan to make it accept traces in other formats, notably traces
  from SMT solvers, as well as all formats that will appear when instrumenting
  other automatic theorem provers.

  % unprovable steps
\item Some steps in the proof traces are not provable: their
  conclusion is not a logical consequence of their premises. However,
  they preserve provability: the original problem has a proof if and
  only if the problem with the conclusion of the step also has a
  proof. This is the case for instance of the Skolemization step an
  automated theorem prover for predicate logic, of the introduction of
  new definitions, as well as the RAT property in traces produced by
  SAT solvers. The approach of Ekstrakto cannot be used here, because
  the sub-problem corresponding to the step cannot be proved. However,
  since provability is preserved, it should be possible to transform a
  proof using the conclusion of the step into a proof using its
  premises. Such a transformation depends on the nature of the step
  that has been used. We plan to include in Ekstrakto a way to handle
  Skolemization and definition introduction, which are the two step
  families that are missing to be able to manage all traces from the
  major theorem provers for predicate logic.

  % specialization for theories

\item Dedukti-producing provers used by Ekstrakto, namely Zenon Modulo
  and ArchSAT, are meant for pure predicate logic. However, we would
  like to deal with proof traces that use some specialized theory,
  e.g.\ arithmetic or bit-vectors, as could be output by SMT
  solvers. Although such theories could be presented as a set of
  axioms in predicate logic, it is almost certain that neither Zenon
  Modulo nor ArchSAT could be able to find non-trivial proofs using
  these axioms. Here, the idea would be to develop small provers
  dedicated to a particular theory, and outputting Dedukti
  proofs. Such provers would be called when a step in the trace relies
  on said theory. These provers need not be very optimized, since
  trace steps are relatively small; this should help producing Dedukti
  traces. A way to achieve this could be to extend Zenon Modulo:
  indeed, Zenon modulo can find proofs modulo arithmetic, but it is
  not able to produce a Dedukti proof yet.
\end{enumerate}
  
\subsubsection*{From Dedukti to automatic theorem provers}

%\label{concept:wp4:deduktitoatp}
In the other direction, Logipedia will constitute a source of
knowledge for automatic theorem provers. For this to be affordable,
this project will study the following challenges.

\paragraph*{Translate Dedukti statements into automatic theorem provers
  inputs.}
Automatic theorem provers are mostly based on (parts of) predicate
logic.  Logipedia theorems, which mostly come from interactive
provers, will be expressed in the Dedukti encodings of much more
expressive logics, such as dependent type theory or simple type
theory.  Theorem statements thus need to be expressed again to be
manipulable by automatic theorem provers.

Encodings from expressive logics to predicate logic already exist,
and are used for instance in {\em hammers} for using automatic theorem
provers into interactive theorem
provers~\cite{DBLP:conf/lpar/PaulsonB10,DBLP:journals/jar/CzajkaK18}.
In these works, the encodings are specific to one system to be
affordable. In Logipedia, we have to combine and take benefit from
statements coming from the encodings of different systems based on
different logics. The key challenge here are thus:
\begin{itemize}
\item to avoid the loss of meaning coming from a succession of
  encodings; and
  \item to encompass statements coming from different systems.
\end{itemize}

We plan to investigate a new approach where, instead of hammers, the
encoding is a succession of fine-grained encodings dedicated to one
aspect. These ``small'' encodings will offer the possibility to be
activated independently depending on the origin of the statement. They
give the other advantages of being modular, easily extensible, and
more reliable: each encoding is simple, and may output proofs, {\em
  e.g.} using Ekstrakto. They will be performed modulo the concept
alignment.

It is common in the automatic theorem proving community to evaluate
the performance on sets of benchmarks. As a side effect of this task,
a new set of benchmarks will be extracted from Logipedia to measure
the performance of automatic theorem provers on problems coming from
different logics, and from a combination of these logics. In our case,
it will not only allow to compare automatic theorem provers but also
to measure the success of this task by evaluated the quality of
encodings, and comparing our ``small'' encodings by activating them or
not.

\paragraph*{Logipedia as a source of knowledge for automatic theorem provers.}
The amount of knowledge available in the whole of Logipedia is
significantly larger than in any of the individual proof assistant
libraries, and as such selecting the knowledge relevant for a
particular goal might require more complex techniques than
before~\cite{Irving-deepmath}.  We will experiment with machine
learning techniques for formal proofs. This includes both techniques
for selection of relevant knowledge for a goal and for the selection
of most promising constructors in the direct proof term
construction~\cite{ZielenkiewiczSchubert2016}.

\subsubsection*{Large scale application: formal verification of C code}

The use of formal methods in the industry requires to have approaches
that apply to a wide variety of cases. For the verification of C code,
the Frama-C platform features numerous techniques. One of them relies
on automatic solvers: Frama-C-WP. Since automatic theorem provers are
built by making many choices such as which heuristics or which
algorithms to use, they display blind spots where they are less
efficient to solve some cases.  In order to overcome that, it is
necessary to use the wide variety of solvers which exist in a
portfolio manner. The Why3 tool features the ability to send problems
to lots of provers in a uniform way. However with so many tools
written by different teams involved, the meaning of the same concept
in the different tools could be different. It would lead to errors in
the overall verification results. Moreover in order to be applied more
widely, formal methods must handle more concepts such as floating
points where their exact meaning is less clear that mathematical
integers. That leads to more opportunities for two tools to interpret
differently the same concept. Finally, an industrial user sometimes
needs to add some reasoning or simplifications for their particular
concept so that it is better handled by automatic solvers. It is very
easy to make an error in those simplifications.

In order to overcome these problems, we propose to gain insurances in
the interaction of these different tools and to speed the addition of
new features to handle new cases by using proof objects:
\begin{itemize}
\item Where the industrial user needs to define the concepts he wants
  to verify in its C code, we will add the possibility to import
  concepts from Logipedia. The user gain time by not having to define
  them himself and it ensures that we have proofs for the accompanying
  lemmas.

\item Where the simplification rules written for the industrial case
  are executed in Frama-C-WP, we will instrument it to generate
  Ekstrakto input in order to produce a proof of the correction of
  these simplifications.

\item Where the problems are transformed to fit the different provers
  in Why3, we will inspire from the fine-grained encodings (previous
  paragraph) to also generate Ekstrakto proof.

\item In the end, we will gather the Dedukti or Ekstrakto proof of the
  provers, the encodings, and the simplification rules, in order to
  assemble them in a coherent whole.
\end{itemize}

This part thus combines and validates most of the previous aspects of
this theme of automatic theorem proving, but also constitutes the
challenge to instrument a large-scale formal tool.

\subsubsection*{Automatic theorem provers to increase Logipedia readiness}

Providing automation on the level of an encyclopedia of formal proofs
is both interesting and challenging. Indeed, making so much formal
systems cooperate requires a lot of proof manipulation,
transformations, gluing, that can only be reasonably done with
automation. On a higher level, being able to more efficiently write
proofs directly in Dedukti would be of interest to the community in
itself. Making this automation practicable in Logipedia raises the
question of scalability.

\paragraph*{Automatic theorem provers for Dedukti.}
Automation has been a key to the success of proof assistants, since it
allows much more efficient formalization~\cite{Hales-Developments}.
Many different techniques to provide proof assistant automation have
been developed over the years: techniques based on rewriting,
tableaux~\cite{Paulson-blast}, or even the integration of efficient
superposition-based provers for predicate logic~\cite{hurd-metis} and
Simple type theory~\cite{asperti-matita-paramodulation}. Most
recently, various machine learning techniques have been developed for
proof assistants allowing for more precise selection of relevant
knowledge~\cite{blanchette-h4qed-jfr} or even the prediction of useful
proof techniques~\cite{gauthier-tactictoe}.

The automation techniques offered by different proof assistants differ
significantly. For systems mostly based on classical predicate logic
it is often possible to provide sound and complete translations to the
most efficient automatic theorem provers for predicate logic by only
expressing the intricacies of their type
systems~\cite{kaliszyk-miz40}. This allows for straightforward native
proof reconstruction. On the other end of the spectrum, providing
efficient automation for systems based on involved foundations, such
as the constructive type theory behind Coq and its variants,
providing even a slightly useful automation is a significant
challenge. Most efficient automation still relies on translations to
external tools, however the useful translations from such logics to
the logic of SMT solvers~\cite{DBLP:conf/cpp/ArmandFGKTW11} or
predicate logic~\cite{DBLP:journals/jar/CzajkaK18} are incomplete
and often (partially) unsound. This means that to reconstruct such
proofs in the logic of the proof assistant separate intricate
components need to be developed.

As part of the project we will experiment with various kinds of proof
automation on the level of Dedukti and verify their applicability to
the different libraries imported as part of the Logipedia project.
Furthermore, we will experiment with machine learning techniques for
formal proofs. This includes both techniques for selection of relevant
knowledge for a goal and for the selection of most promising
constructors in the direct proof term
construction~\cite{ZielenkiewiczSchubert2016}. Finally we plan to look
at computational proof reconstruction, to complement the link to
automated theorem provers developed in the other parts.

\paragraph*{Automation for Logipedia.}
The aforementioned automation will be crucial to enable full proof
verification. Considering systems one at a time, a number of proof
exports are not complete, that is the actual systems do not provide
all the proof step details and internal Logipedia automation would be
necessary. More demanding, automation will be used to fill the gaps
between systems. In particular, concept alignments need to be formally
established. More generally, proof transformations coming from
encodings into Dedukti, reverse mathematics, \dots must be discharged
as automatically as possible.

As it has been the case for interactive theorem provers, automation
will also provide support for Logipedia developers and users. During
the project, many Dedukti developments shall be performed, in
particular to define theories in Dedukti. In a longer term, Dedukti
will also be used, {\em e.g.} for new proof transformations or to
align distant concepts, which will be affordable with automation.

\subsubsection*{Success}

The success of the integration of proofs coming from automated theorem
proving systems, SAT solvers, SMT solvers, and model checkers, can be
measured by the size of the proofs imported from these systems to Logipedia.

\bigskip

\noindent
{\bf \Large \underline{Large libraries}}

\medskip

We have seen how to instrument proof systems and automated theorem
provers to produce proofs in Dedukti, and how to share these proofs in
Logipedia.

Very large proofs, requiring large libraries, have been developed in
some proof systems and these large proofs and libraries are
interesting for our project in two respects. First they constitute an
excellent benchmark for the methods developed above. Then they will
constitute one of the lion's share of proofs in Logipedia for end-user
applications.

The target libraries are dedicated to particular application
areas. They provide a substantial coverage of that application area
and do so in a structured manner. This may require reworking the
libraries for better access. Any library is only as useful as its
structure and documentation.

The libraries are curated for end-user application. That is, they are
structured according to application specific ontologies that support
browsing and search. The structuring leverages the infrastructures of
and will be a stress test for the results of structuring discussed
below: is the metadata infrastructure capable of representing the
structure of large libraries?

The libraries we focus on have been selected according to the
following criteria:
\begin{itemize}
\item Relevance: the libraries should support core areas of mathematics
  and computer science.
\item Coverage: the libraries should have a wide coverage of an
  application area.
\item Maturity: the libraries should have been used in a significant
  application already.
\end{itemize}

As a result we selected the following libraries: MathComp, Coq's
revised Analysis library, the Archive of Formal Proofs, Isabelle's
revised Analysis and Probability library, GeoCoq, Flyspeck and CakeML.
Other very interesting libraries such as CompCert, seL4, and selected
Mizar and PVS libraries are left for future work.

\paragraph*{Isabelle's Archive of Formal Proofs.}
Isabelle's Archive of Formal Proofs (AFP) \cite{isabelle-afp} is a
growing user-contributed online library for Isabelle. In Feb-2020, the
AFP consisted of more than 500 entries (articles of formalized
mathematics) by 340 authors, and required approx. 60h CPU time for
checking (using many gigabytes of memory).  The purpose of this task
is to scale up the Isabelle instrumentation for Dedukti further, to
cover major parts of this library. We do not intend to restructure or
document the library further beyond what is provided by the authors of
each article and by the hierarchical dependencies among the articles.

The key challenge is scaling. The ultimate aim is to export the main
substance of the AFP without promising full coverage: some entries
with prohibitive resource requirements will be omitted. Also note that
the AFP is continuously growing at a high rate and thus a moving
target.

\paragraph*{The Isabelle Analysis \& Probability Theory Library.}
The Isabelle Analysis \& Probability Theory Library consists of more
than 200.000 lines of definitions and proofs, corresponding to almost
4000 printed pages. It covers topology, homology, multivariate,
functional and complex analysis, measure and probability theory, and a
dedicated library for ordinary differential equations. It is fair to
say that it is the most advanced machine-checked library in the area
of analysis and probability theory. The Isabelle analysis and
probability theory library is in part based on the extensive library
of the HOL Light system but has generalized it from real numbers to
appropriate algebraic structures and includes areas absent from the
HOL Light library (in particular measure theory and ordinary
differential equations). It is currently used by 60 out of 500
articles in the Archive of Formal Proofs, a user-contributed library
of Isabelle/HOL proofs from all areas of computer science and
mathematics. Moreover, it is the only existing library in this area
where proofs are structured and readable. Due to its size and
generality and because analysis and probability theory are pervasive
in any application in the engineering and natural sciences and
economics, this library is a KER of the project: it is a fundamental
enabling resource for almost any formal verification activity in these
areas, for example autonomous driving and mathematical finance. The
purpose of this task is to structure, document and develop this
library for optimal accessibility, ease of use and
comprehensiveness. The aim is a curated library for applications.
Therefore some material will need revising for ease of use and some
essential material will need to be added.

\emph{Accessibility:}
The library is a large collection of theories with a hierarchical dependency
relation. However, this structure has grown over time and does not always
reflect the abstract mathematical dependencies. In short, the structure needs
to be modularized. This requires a significant refactoring effort.
%
At the same time we need to add metadata to the source material to turn this
structured collection of theorems and proofs into a curated library at the
Dedukti level (see below). We
will be test drivers of the metadata infrastructure provided by that WP, in
particular the ability to describe ontologies for structuring in the large.
This will also entail annotating those statements in the library that
correspond to proper mathematical theorems as opposed to auxiliary lemmas
required only for the benefit of the theorem prover.
To increase accessibility we will also include links from the library into
Wikipedia and in particular in the other direction to raise the awareness
of Logipedia.

\emph{Development:}
Although the library support for integrals is extensive, it suffers
from the (necessary) coexistence of different kinds of
integrals. This complicates proofs about integrals in applications and needs
to be unified, which entails further refactoring. The rest of the
development adds further essential material:
\begin{enumerate}
  \item Fourier analysis because of its extreme important both for pure mathematics and for
engineering and physics (especially the Fourier
transform). A formalization of basic properties of the Laplace transform is already available in the AFP.
\item Stability theory for differential equations and
dynamical systems, in particular Lyapunov functions, because of their
relevance for the verification of cyber-physical systems.
\item Stochastic differential equations to model a large range of
dynamical systems with stochastic components, from physics to financial markets.
\end{enumerate}

\paragraph*{The GeoCoq library.}
The GeoCoq library consists of more than 100.000 lines of definitions
and proofs in geometry. It is mostly based on synthetic approaches,
where the axiom system is based on some geometric objects and axioms
about them, but, following Descartes and Tarski, we prove that the
analytic approach can be derived , where a field $F$ is assumed
(usually $\mathbb{R}$) and the space is defined as $F^n$. Moreover, it
contains a model, based on the analytic approach, of one of the
synthetic approaches present in the library, namely Tarski's system of
geometry, thus establishing the connection between these two
approaches in the opposite direction.

The fact that the analytic approach can be derived from the synthetic
one is called the arithmetization and coordinatization of geometry and
it represents the culminating result of both Hilbert's {\em Grundlagen der
Geometrie} and Tarski's {\em Metamathematische Methoden in der
Geometrie}. As of now, there is no other formalization of this result
inside a proof assistant, making the GeoCoq library the most advanced
machine-checked library in the area of synthetic geometry. This
formalization enables to obtain automatic proofs based on geometric
axioms using algebraic automated deduction methods, some of which will
be covered by automated theorem proving.

The main axiom system in this library is the one of Tarski, but
Hilbert's axiom system and a version of Euclid's axioms sufficient to
prove the propositions in Book 1 of Euclid's Elements are also
defined. Thanks to the proof that Tarski's axioms (except continuity)
are equivalent to Hilbert's axioms (except continuity), the
arithmetization and coordinatization of geometry are available for
both systems. In the library, the focus is not only on axiom systems
but also on axioms themselves. Eleven continuity axioms are available
and are hierarchically organised. Finally, it contains a new
refinement of Pejas' classification of parallel postulates together
with proofs of the classification of 34 versions of the parallel
postulate.

\emph{Challenges:} A lot of preliminary work has been done on the
GeoCoq library.  One of the remaining obstacles is the use of
computational steps in Coq proofs. The issue is that proofs containing
proof by reflection reach a level of complexity that currently makes
verification by Dedukti impractical, but these proofs are very
frequent in Coq.  An approach is to isolate these proofs by
reflection so that they are not perceived as simple conversion steps
in the type theory proofs, but marked as proofs to be treated by an
automatic tool. The coherent logic theorem prover should handle part
of these proofs. The other ones should be handled by specific provers
for geometry, as they correspond to proofs obtained by the Gr\"obner
basis method in GeoCoq.  Another challenge is that CoqInE, a tool
developed to translate Coq proofs into Dedukti type-checkable terms,
produces terms in a expressing of the Calculus of Inductive
Constructions into the $\lambda \Pi$-calculus modulo theory on which
Dedukti is based. Currently, it is not possible to export these
Dedukti terms to other proof assistant. However, another tool,
Universo, has been developed and paves the way for the export of these
terms.  The last challenge is that part of GeoCoq relies on the
mathcomp library. 

\emph{Refinements:}
The pragmatic approach that has been adopted so far has motivated a
few choices that should be reconsidered once the obstacles,
encountered throughout this process, which triggered the need for
these choices will have been solved when instrumenting Coq. The solution that
has been preferred in most cases was to modify the Coq script so that
it does not rely of features, such as universe polymorphism, that were
not handled by CoqInE when the obstacles were encountered. For
example, the use of module functors was removed from GeoCoq, some
proofs produced by tactics, such as intuition, relied indirectly on
parts of the standard library of Coq that were not available with
CoqInE were reworked by hand to avoid such dependencies, or direct
dependencies were avoided. Another choice that could be reconsidered
is to use of automated theorem provers to replace the proofs by reflection.

\paragraph*{The Flyspeck library.}

The Flyspeck library
is the result of the Flyspeck
project, a formal proof of the Kepler conjecture. The statement and
proof is based on an original proof of Thomas Hales
\cite{DBLP:journals/corr/HalesABDHHKMMNNNOPRSTTTUVZ15}, and is
formalized and proved largely in HOL
Light\footnote{\url{https://github.com/flyspeck/flyspeck}}. HOL Light
has a large and rich analysis and geometry library, with some results
not formalized in any other system. This motivates the project of
importing the HOL Light library and the Flyspeck project into
Dedukti and thus into Logipedia.

\emph{Challenges:}
Proofs coming from the HOL systems, including {HOL Light}, are known to
be very large, adding to the already existing issue of the scalability of
exporting software for large libraries
\cite{DBLP:conf/tphol/Wong95,DBLP:conf/cade/ObuaS06,DBLP:conf/itp/KellerW10,
DBLP:conf/cade/Kumar13}. As an example, the size of the standard library is
1.5M in {HOL Light} files, while the same files once
exported as {Dedukti} files scale to 2G, and the whole
{HOL Light} library and the {Flyspeck} project are each as
large as 30M. The multitude of automatic tactics and their increasing
complexity and number of dependencies suggests that the generation time, size,
and rechecking time of exported proofs will not increase linearly. Scalable
export techniques {HOL Light} proofs have been investigated
\cite{KaliszykK13} and can provide a solid base to
this project; for want of significantly reducing the size of generated
{Dedukti} files, the time of export could at least be reduced.

The main milestones of this task are the further automation of the
translation from {HOL Light} to {Dedukti}, the import of the
multivariate analysis library, the import of the whole {HOL Light}
library, and the import of the {Flyspeck} in {Dedukti}.

\paragraph*{The CakeML compiler library.}

The CakeML compiler library consists of the CakeML programming language
definition, its compiler and the correctness proofs about the
compiler. This is a cutting edge compiler library and one of only two
verified compilers for real languages, the other being CompCert. Its
export to Dedukti is one of the KERs of this project. Because of the
size and importance of this library, we will approach the export from
two angles.

The first approach utilizes OpenTheory-based technology. The CakeML
compiler development lives within the HOL4 prover. HOL4 can export
definitions and proofs in the OpenTheory format, which can in turn be
translated into Dedukti. This link from HOL4 via OpenTheory to Dedukti
exists but, in its current state, fails to scale to the task of
transporting something as sizeable as the CakeML compiler
development. This part of this task will rework the route via
OpenTheory to scale better, possibly taking inspiration from an
OpenTheory-like approach that scaled well for the HOL light
prover~\cite{KaliszykK13}.

The second approach establishes a connection from HOL4 via Isabelle to
Dedukti. The basis is a promising new approach of virtualizing HOL4
inside Isabelle \cite{ImmlerRW19}. That is, the inference
kernel of HOL4 is replaced by that of Isabelle and the resulting
system produces Isabelle theorems instead of HOL4 theorems. This has
the invaluable additional benefit that tools, not just theorems, can
be shared. As a benchmark of the viability of this approach we plan to
export CakeML via Isabelle to Dedukti.

\subsubsection*{Challenges}

The two key challenges are scalability (pushing the technology to cope
with the size of proofs) and accessibility (presenting the library in
an accessible manner to end-users). This will be a stress test for the
infrastructure.

\subsubsection*{Contributions}

These libraries will populate Logipedia with mature,
application-level mathematical theories. That is, proofs about
important areas of mathematics (e.g. analysis and algebra) and
computer science (e.g. compilers) that support applications in
engineering as well as security and in particular in interdisciplinary
areas such as cyber-physical systems and autonomous vehicles.

\subsubsection*{Success}

As one of the challenges is scalability, one measure of success is how
much of the libraries we will be able to import into Logipedia. A
second measure of success is how well we are able to restructure the
libraries for end-user access and how well this structure can be
transferred to Logipedia and thus to end-users.

\bigskip

\noindent
{\bf \Large \underline{Infrastructure of the encyclopedia}}

\medskip

In the three last sections dedicated to integration, automated theorem
proving and large libraries, we have described the concepts that are
at the center of the networking activities that will populate
Logipedia with formal proofs coming from the libraries of various
systems.

Let us now turn to the development of the infrastructure itself, with
a focus on accessibility. It is not sufficient to provide a
trans-national and virtual access to make such an infrastructure
usable. We must also provide an ergonomic web interface, a package
distribution system, a search engine, a structure of encyclopedia
that rests upon an enrichment of the data with metadata.

{\color{red} To do}

Web site

Packages

Web site for students

\subsubsection*{Success}

\bigskip

\noindent
{\bf \Large \underline{Source Level Structure and Metadata}}

\medskip

Like programmes, formal proofs exist at two levels: the user-written
source code and the kernel representation.

The expression of theories in Dedukti focuses on integrating the low
level representation obtained by instrumenting the kernels.  Thus, a
single architecture such as Dedukti can cross-verify proofs from any
number of proof assistants.

But this eliminates a lot of salient information that is present in
the original sources, including
\begin{itemize}
\item library structure in the form of documents, sections, or
  theories,

\item high-level statements and operators (which proof assistants
  typically elaborate when compiling the sources into,

\item extra-logical statements such as comments, metadata annotations,
  or processing instructions, the kernel representation).
\end{itemize}

So, we need to complement the current expression of proofs in Dedukti
and Logipedia, by integrating source level representations.

We will survey the most commonly used and most
interoperability-relevant language features and add those to Dedukti.
These will complement the existing language features in such a way
that encodings in Dedukti can choose flexibly when and how source
level features are preserved.  We thus will annotate the kernel level
representations with source level representations.  We will develop a
semantic web-style ontological representations of the libraries.
These encodings heavily abstract from the content of the libraries and
are not meant to capture either its syntax or its semantics, but they
are tremendously useful for shallow interoperability such as
dependency management, navigation, and search and querying.  For
example, searching for a particular theorem based on some information
where it might be used can be enough to find it.  We will develop a
reference ontology for formal libraries and extend our encodings to
make use of it.

This method works particularly well in connection with the alignments
developed below. Because, in its simplest form, alignment is just a
binary relation between identifier in different libraries that mean
the same thing, it can be integrated with this ontology directly.  It
then enables queries that span multiple libraries, e.g., to find all
theorem about a certain concept that have been in some provers but not
others.

These data structures developed will be used
critically in development of search engines for Logipedia.  This
includes (i) the use of source level syntax in queries that is matched
against corresponding source syntax in the libraries as well as (ii)
semantic web-style queries in languages like SPARQL.

\subsubsection*{Success}

{\color{red} To do}

\bigskip

\noindent
{\bf \Large \underline{New theories, new systems}}

\medskip

Finally, after discussing the concepts related to the networking activities
needed to collect the content of Logipedia and the infrastructure development
needed to make it accessible, let us turn to joint research projects. 

We have considered above systems that were already at LIL 1, that is
systems that implement a theory that we know how to express in Dedukti.
There are other systems, that are still at level 0, that is that we
do not know how to express. Part of our project is to work on such
systems, to understand how to express their theory in Dedukti.

These systems range from specialized systems such as Minlog
used in academia to mainstream systems such as PVS or TLA$^+$ that
have been used in major scientific or industrial projects. They
provide specific features that are relevant to their target domains
and are therefore preferred by users for developing theories and
proofs in these areas.  The objective is to bring them (at least) to
LIL $2$ in order to prepare the grounds for a longer-term integration
into Logipedia.

There are various motivations for including these systems in
Logipedia. First, being able to export theories and proofs developed
in these systems makes them available to those systems with more
advanced Logipedia integration levels. For example, accessing Mizar's
rich library of formalized mathematics would be beneficial to
developers of machine-checked mathematical results. Second, the
smaller systems have been used for specific tasks such as reasoning
about the meta-theory of programme calculi, but they do not provide rich
general-purpose standard libraries, say, about integer or real
arithmetic as the more mature systems. Finally, for some of the
systems only one implementation able to check the proofs exists. As
such, in the long run we can cross verify the proofs stemming from the
target systems.  Neither goal will be fully achieved at the end of the
project since higher Logipedia integration levels are necessary for
exporting or importing full libraries. Nevertheless, the work carried
out here is a necessary stepping stone and enabler
for making Logipedia a universally accepted encyclopedia of formal
proofs.

\paragraph*{Homotopy type theory (HoTT).}
Introduced by Vladimir Voevodsky, Michael Warren and others, HoTT aims at
connecting type theory and homotopy theory, the study of abstract shapes. It is
considered as an alternative to set theory for providing foundations for all
areas of mathematics. In particular, paths and homotopies are primitive objects
rather than mere sets of points, and isomorphic mathematical structures cannot
be discriminated---unlike set theory where accidental identifications can arise
depending on how structures are represented.

In its simplest form, HoTT is an extension of Martin-L\"of type theory by the
univalence axiom. More modern forms include Cubical type theory (CTT,
\cite{cohen:cubical}) that is both constructive (i.e., proving the existence of
an object provides an algorithm for effectively computing the object) and where
univalence is a theorem. All type-theoretic proof systems have very small steps
that are omitted in proofs (e.g., trivial equalities such as $1+1=2$). In the
context of HoTT, these are called coherence conditions. Two-level type theory
(2LTT~\cite{annenkov:two-level}) was introduced as a stepping stone where the
small proof steps are made explicit, and we will start by expressing 2LTT in
Dedukti and then move to CTT. This will allow us to gain experience with
the extent to which coherence conditions can be supported as rewriting steps in
$\lambda\Pi$-calculus modulo theories, and which conditions require proof
elaboration.

\paragraph*{Minlog.}
The
Minlog\footnote{\url{http://www.mathematik.uni-muenchen.de/~logik/minlog/index.php}}
system is based on the Curry-Howard correspondence between constructive proofs
in natural deduction and terms in typed $\lambda$-calculus and supports
extracting programmes from proofs. It has also been extended for extracting
programmes from proofs in classical logic. Minlog supports function definitions by
computation and rewrite rules and identifies terms with a common reduct. Its
main applications are in constructive analysis~\cite{miyamoto:real}: we extract
computational content from a proof of a mathematical statement in an extension
of Gödel's system $T$ and generate a formal proof that the extracted term
realizes the statement, viewed as a specification.

Real numbers are best represented as streams of (signed) digits, and handling
such infinite objects requires co-induction, which gives rise to co-recursion in
the extracted terms. Minlog accommodates co-recursion via partial functionals in
the Scott-Ershow model $\mathcal{C}$ and restricts variables to extensional
objects of type level at most $1$, where extensionality and totality coincide.

Besides extracting programmes from proofs in constructive analysis, other notable
applications of Minlog include the unification of $\lambda$-calculus and
combinatory logic, embedding classical logic in minimal implicational logic, and
the extraction of the DPLL algorithm underlying propositional Satisfiability
solving from a proof.

In this project, we aim to lift Minlog from LIL $0$ to (at least) LIL $3$.
This is facilitated by the fact that the foundations of Minlog and Dedukti are
rather close (both have proof terms and deduction modulo); the main challenges
are support for co-induction and co-recursion. We plan to export programme
extraction to Dedukti through realizability and also aim for being able to
import a subset of Dedukti proofs into Minlog in order to benefit from the
libraries available in Logipedia.

\paragraph*{Mizar.}
Mizar\footnote{\url{http://mizar.org}}~\cite{banczerek:mizar} is one of the
earliest proof assistants. It was initially created as a typesetting system for
mathematics with proof checking functionality added later. The language is
designed to be as close as possible to the traditional language of mathematics
where basic elements are represented as nouns, whereas their properties and
operations can be represented using adjectives, predicates, and functors, as
well as a hierarchy of soft types. Proofs are expressed in a declarative way
based on natural deduction.

The Mizar community maintains a centralized data base, the Mizar Mathematical
Library. Based on the axioms of Tarski-Grothendieck set theory, it provides a
framework on top of which new theories can be built in the form of ``articles''.
The library constitutes one of the largest collections of formal mathematics with
a number of results that are not present in other systems. Apart from purely
mathematical formalizations, the system has been used for developing formal
descriptions of theoretical models of computation, cryptography, digital circuit
design, and software verification. The Mizar library provides a data base for
training automated theorem provers and other projects based on machine learning
techniques.

The Mizar proof checker differs from other proof assistants because the
semantics of a proof step corresponds to the notion of ``obviousness'' to a
human mathematician. Mizar contains modules that check different aspects of the
correctness of Mizar articles in a compiler-like manner, from purely syntactic
concerns all the way to logical completeness and consistency. Mizar has not been
designed to produce proof objects, and much background knowledge (e.g., stemming
from soft types) is used implicitly. The main challenges of encoding Mizar in
Dedukti will be to represent the different forms of Mizar definitions in a
uniform way and to instrument the system to make this implicit knowledge
explicit and allow Mizar proofs to be checked by Dedukti.

\paragraph*{PVS.}
PVS\footnote{\url{https://pvs.csl.sri.com}}~\cite{owre:pvs} is a specification
system and proof assistant that is based on Simple type theory with a rich type
system including predicate sub-typing. This feature is used extensively in PVS,
for example for defining a hierarchy of numerical types. Predicate sub-typing
makes type checking in PVS undecidable, and PVS emits corresponding proof
obligations in the form of type checking constraints that require
interactive proof by the user. For proof checking, PVS provides a rich set of
automated tactics and back-end provers but does not generate explicit proof
objects.

PVS has been used in numerous academic and industrial projects for
verifying hardware systems, fault-tolerant algorithms or dynamically
linked Java programmes, among others. It is particularly known for its
applications to avionics systems and airborne conflict detection and
avoidance systems, e.g.\ \cite{munoz:unmanned}, by Rockwell Collins,
NASA, and the National Institute of Aerospace.

Our objective in this project is the translation of a substantial part of the
PVS Prelude (the standard library) into Logipedia. A core of the PVS logic,
restricted to a fragment with decidable type checking, has been defined in
$\lambda\Pi$-calculus modulo theory~\cite{gilbert:extending}. The main
challenges for achieving our objective are to effectively extend this fragment
to full PVS, in a way that identifies superficially different proofs of the
inhabitedness of sub-types, and to instrument PVS in order to generate proof
traces that can be checked by Dedukti.

\paragraph*{TLA$^+$.}
\tlaplus~\cite{lamport:specifying} is a specification language based on
Zermelo-Fraenkel set theory and the Temporal Logic of Actions, a dialect of
linear-time temporal logic. It is intended for the precise description of
discrete systems and in particular of distributed algorithms and systems.
\tlaplus has seen substantial adoption by companies working on distributed and
cloud systems, e.g.~\cite{newcombe:amazon-cacm}. The two main software tools for
analyzing and verifying \tlaplus specifications are the model checker
TLC~\cite{yu:model-checking} and
TLA$^+$\footnote{\url{https://tla.msr-inria.inria.fr/tlaps/content/Home.html}},
the \tlaplus Proof System~\cite{cousineau:tla-proofs}.

\tlaplus proofs are written in a declarative, hierarchical style based on
natural deduction that allows the user to break down the proof of high-level
theorems into lower-level steps. The proof obligations corresponding to the
leaves of this proof tree are discharged by automated prover back-ends,
including SMT solvers, the Zenon tableau prover, an encoding of \tlaplus set
theory as an object logic Isabelle/\tlaplus in the logical framework Isabelle,
and a decision procedure for linear-time temporal logic.

Our objective in this project is to lift the non-temporal fragment of
TLA$^+$ (which covers the overwhelming majority of proof obligations
arising in system verification) from LIL $0$ to LIL $2$. The main
challenges are encoding the untyped set theory underlying \tlaplus in
$\lambda\Pi$-calculus modulo theory and to instrument TLA$^+$ in order
to export proof traces that can be checked in Dedukti, starting with
the Zenon and Isabelle back-ends.

One of the case studies will be the specification of one of the software
systems developed within MED-EL, which requires an ad-hoc modification of
instances of the pre-defined process. Every new change to the process might
contain fewer, more or different steps that should be performed by an end user
(by a patient), but the requirement is that the already running instances
should get updated according to the new process definition. The formal
verification of this system should guarantee that the process executed by the
particular MED-ELs user, is never out of sync, meaning, it does not contain
inconsistencies in data presented and generated by the user.

\subsubsection*{Challenges}

As explained above, each of the systems considered poses some specific
challenges that will be addressed in the tasks corresponding to the
individual systems. We list some cross-cutting research challenges
that this project will allow us to address synergistically.

\paragraph*{Rich syntactic and semantic features.}
Several systems provide support for representing and reasoning about
binders, definitional constructions, or have rich type systems that
are not easily encoded in the $\lambda\Pi$-calculus modulo theory that
underlies Dedukti.  Although the details vary between the systems,
there are commonalities that should be addressed jointly. For example,
Abella and Minlog have strong support for co-induction and co-recursion
for which foundational support should be provided in Dedukti. Mizar
and (to a lesser degree) TLA$^+$ make use of soft types for assisting
proof search, and PVS emphasizes predicate sub-typing, which is also
used in the SMT encoding used in TLA$^+$.

\paragraph*{Set-theoretic vs.\ type-theoretic foundations.}
Abella, HoTT, and Minlog are based on type theories with explicit proof objects
and are in this respect close to Dedukti, although the details differ. PVS has a
type system reminiscent of Simple type theory but adds predicate
sub-typing, whereas Mizar and TLA$^+$ are based on classical set theories. The
three latter systems do not provide proof objects and require instrumentation
for providing traces that can then be interpreted by Dedukti. In all cases, a
crucial point to consider for defining an encoding into $\lambda\Pi$-calculus
modulo theory is that of exploiting Dedukti's rewrite capabilities for
controlling the size of proof objects.

\paragraph*{Automatic proof tools.}
Many of the systems considered call upon automatic back-ends for proof
search and in many cases, these do not natively produce traces. Even
when they do, they are not in a form that is suited for replay in
Dedukti. There are two ways how we will try to tackle the problem of
missing proofs. First, we will attempt to instrument these tools in
order to record and export proof traces, and there are clear
opportunities for sharing notions of proof traces and concrete proof
formats in order to avoid duplicate effort. Second, we will use native
Dedukti automation to re-create the missing proof parts.

\subsubsection*{Success}

The main measure of success for this work is provided by the Logipedia
integration levels of the considered systems, and the fragments for which
theories and proofs can be exported. A secondary measure is the overhead
incurred by the encodings and instrumentations and thus the complexity of proof
checking by Dedukti.

\bigskip

\noindent
{\bf \Large \underline{Proof engineering}}

\medskip

The development of formal reasoning tools has led to an ever-growing
corpus of mechanized mathematical theories. As these libraries span a
wide spectrum of proof assistants, the underlying logical foundations
of these systems and libraries can different greatly. For example,
some proof assistants use set theory as a foundation while others use
Simple type theory, Martin-L\"of's type theory, the Calculus of
Constructions, etc.  In order to combine the power of different proof
assistants and their respective communities, it is critically
important that these different logical foundations can be related to
each other in ways that ensure their interoperability.  Even when one
has successfully aligned these different logical foundations, one must
also align the many different mathematical concepts and theorems that
are in the libraries produced by the various proof assistants. Thus,
successful interoperability of both proof assistants and formalized
mathematical libraries starts with identifying the correspondences
between the concepts underpinning these formalizations, i.e., their
alignment.

In the simplest case, an alignment is a pair of symbol identifiers
from two different libraries such that both symbols are ``the same
informal mathematical concept''. For example, although real numbers
can be defined by Cauchy sequences, Dedekind cuts, and also stream
representations due to co-recursion, all such different formalizations
introduce essentially the same mathematical concept. More precisely,
following the work of \site{Fau} and \site{Inn}
\cite{GKKMR:alignments:17} and the alignment-based mediator developed
by \site{Fau} in the OpenDreamKit project \cite{ODK:mitm:18}, we
consider an alignment to be a pair of
\begin{itemize}
  \item two identifiers $c,c’$ (usually from two different libraries
    $L,L’$), which are considered to represent the same mathematical
    concept,
  \item additional data that governs how $L$-expressions using $c$ and
    $L’$-expressions using $c’$ correspond to each other.
\end{itemize}
The source of alignments can vary and include manual collections
\cite{MRLR:alignments:17} and automatic, machine-learned collections
\cite{align_kaliszyk}.

Even when the various different proof assistants are all exporting their
proofs in Dedukti, the proper alignment of the theories underpinning
those systems is an essential task: for examples, simply taking the
union of all the different theories can create an inconsistent and,
therefore, useless combination.
%
Alignments can also be useful for joining formalizations done within a
single prover, since different formal libraries developed within a
single system can introduce different definitions of the same
mathematical concept (each more suitable to the library where it is
introduced).

To explain the need for concept alignment, we can compare this aspect
of the Logipedia project to the virtualization of computer operating
systems. In such virtualized systems, it is not sufficient for the
user to be able to run system A inside system B, she also needs to
have access to some device of the host system as device of the guest
system. For example, all operating systems have the concept of file
systems, keyboards, mice, etc, but they are all using technically
different definitions of these concepts.  For virtualization to
succeed, significant work is required to build bridges that formally
connect the different versions of these concepts.

Our goals are first to understand the concept of
alignment, then to identify alignments across the various proof
assistants, and then to build tools to bridge these alignments. We
will develop standards, tools, and techniques that will allow concepts
to be aligned so that formal proof developments from one proof
assistant can be meaningfully used in other systems.

\subsubsection*{Content}

Concept alignment can happen at different levels:
\begin{itemize}
\item Alignments of logical foundations. just like Euclidean and
  non-Euclidean geometries are not completely independent theories but
  differ just from a few axioms, the various type theories or set theories 
implemented in the proof systems have some axioms in common. 
We want to divide these theories in clusters,
according to their key parameters, and then to build a
web of syntactic translations between systems. We do not expect full
back-and-forth translations, as some systems are well-known to be
proof-theoretically stronger than others, but we will seek to
establish suitable equiprovability results for fragments of the
relevant languages.

\item Alignments of theorem proving objects deals with aligning named concepts such as types,
  constants, and theorems.
\item Alignment of proofs deals with aligning tactics and
  inference rules that have similar effects on the state of proof
  development.
\end{itemize}
We will primarily focus on the alignment of
logics and theorem proving objects (the first two of these levels).

% The following is used instead of \subsubsection* in the alignment WP.
\newcommand{\parag}[1]{\medskip \noindent {\bf #1}} 
% To revert, uncomment the following line
%\newcommand{\parag}[1]{\subsubsection*{#1}}

\subsubsection*{Aligning logical foundations}

In order to align logical foundations we will develop novel ways of
relating theories represented in Dedukti, building on extensive
experience of the team in type theory, computer-assisted
formalisation, proof theory and reverse mathematics. We will
articulate our work according to the following key parameters for
distinguishing between theories: classical vs constructive logic,
predicative vs impredicative definitions, treatments of equality, and
presence and absence of certain additional axioms, e.g., the axiom of
choice.

One of the major goals of this task will be the formulation and
representation in Dedukti of a core logical system that is both common
to all logical systems and supportive of modular analysis of its
extensions. We believe that such a logical system can be obtained as a
type-theoretic version of geometric logic, a certain well-behaved
fragment of predicate logic.  Such a treatment of geometric logic
will be of interest because we
believe that, over theories in such a logic, we can transform
classical proofs, using the law of excluded middle and the axiom of
choice, into constructive proofs.  This transformation will not also be
a theoretical result but also have practical value since we 
will seek to obtain feasible algorithms to
transform proofs based on classical principles to constructive proofs.
This effort will add a new dimension to a long-standing development of
proof theory that is still very much unexplored.

A further reason for the interest in this type theory is that it could
provide, with a sequence of extensions, a new setting for the
development of reverse mathematics in a type-theoretic setting. This
setting is in contrast to the standard reverse mathematics developed
in the context of second-order arithmetic. Again, the presence of
proof terms (expressed using Dedukti) adds a new dimension to a
well-established topic.

One consequence of this work is that it can make features of one
system available in other systems. For example, Minlog implements the
refined Dragalin-Friedman $A$-translation, so that classical proofs in
a certain class can be constructivised, which means that programmes can
be extracted from such transformed proofs.  We would like to make this
feature of Minlog available to other proof assistants through Dedukti.
In order to carry it out, we will study both-directed encodings
between a subsystem of Dedukti and Minlog.

\paragraph*{Aligning theorem proving objects (case studies).}
Since sets, functions, relations and numbers are ubiquitous in all
formalized mathematics, we will pay special attention to their
alignment. Additionally, geometry is a very interesting case study,
since it is traditionally introduced in many quite different ways
(both analytically, using various number fields, and synthetically,
using different axiom systems), and we shall pay special attention
to aligning different existing formalizations of geometry.

There are several ways to define numbers. For the natural numbers,
one can, for example, choose a definition based on Peano’s axioms which
relies on a unary representation of numbers which is suitable for
proving theorems. But for computing, a definition based on a binary
representation is better. For real numbers and functions, their
definitions often differ even between different libraries of the same
proof assistant.  Real numbers, for example, can be defined using 
Bishop style modulated Cauchy sequence, regular Cauchy sequence,
Dedekind cut, and stream representations using co-recursion.
Similarly, geometry can be defined synthetically using Hilbert’s,
Euclid’s or Tarski’s axioms or analytically using coordinate systems and
algebra.  Therefore, our first step will be to import into Dedukti the
equivalence theorems between these different axiomatizations. In
practice we want to use a theorem $\Gamma \vdash_S T$ expressed in
system $S$ based on some axioms $\Gamma$ in a system $S’$ based on
different axioms $\Gamma'$ and different definitions.

\paragraph*{Automated theory alignment.}
Although alignments can be manually discovered and described, the
sheer size of existing formal libraries forces us to develop automated
support for finding and organizing alignments.


We propose a workflow based on database and semantic web
methodologies. Specifically, we aim to build an automatic alignment
engine on top of an ontology framework that defines mappings between
base concepts belonging to different theories. Relying on a graph of
dependencies that indicates how the theories are structured within a
given library, we set to propagate the alignments with the help of a
certified matching-based engine. The engine will provide a way of
inferring new alignments, based on the alignment of the primitive
concepts and on a set of rules that will describe further
derivations. In order to enable flexible alignments, we are prepared
to allow not only exact matchings, but also matchings with respect to
a given similarity measure, as in \cite{???}.

An alternative method for discovering ontologies of alignments is to
use unsupervised learning methods to find alignments between proof
assistant repositories. For this, heuristic algorithms and dynamic
processes have been tried in the past \cite{???}.  As part of the
project we intend to also try to use unsupervised machine translation
algorithms \cite{???} to directly find correspondences between
statements and their constituent constants and types in proof
assistant libraries imported in Logipedia.

\paragraph*{Alignment-based services.}
This task is concerned with the implementation of useful services
enabled by the discovered alignments. Each service implemented on top
of Dedukti---such as accessing a library via browsing or searching or
interoperability and reuse provided by translations across
libraries---will benefit from working up-to-alignments. For example, a
user of system $L$ can go to Logipedia to find a theorem about $t’$
about concept $c’$, defined in $L’$ that is missing in the library of
$L$. Even if $L$ contains the corresponding concept $c$, because $t’$
is stated and proved in the logic of $L’$, it will use a different
definition of $c’$ in $L’$ than the one of $c$ in $L$.  Therefore,
$t’$ cannot be directly used in $L$. By translating $t’$ along the
alignment $(c,c’)$, it induces a theorem $t$ that can be used in
$L$. The details are subtle both in theory and in practice, and this
task will explore how to scale up the alignments to provide strong
services.

This task will leverage this simple idea in multiple different
ways. Firstly, we will use alignments to search across libraries. In
its simplest form, this involves a simple search interface into which
the user enters the term $t$ and the system finds $t’$ because it uses
the alignment $\langle c,c’\rangle$. Secondly, we will develop
translation services that use alignments to port proofs across
libraries. This translation will allow porting the proof of $t’$ in
$L’$ to a (potential) proof of $t$ in $L$.


\subsubsection*{Challenges}

Often there are many notions that are tightly connected but not
equivalent.

For example, some definitions are only special cases of the others,
sometimes there are definitions valid in any dimension and some that
are valid only in some dimension, and sometimes definitions differ
only in the treatment of some corner cases (e.g., Hilbert's axioms of
geometry use the strict betweenness while Tarski's axioms use the
non-strict betweenness relation).  Although alignments of such
definitions are also needed, they are very hard to detect and use. An
automatic prover would fail to formally relate such a pair of concepts
since they are not equivalent: all the same, such a pair can usually
be related by an ``equivalent-up-to'' some special cases.  While some
equivalences are trivial, others equivalences can be expected to be
proved automatically.

\subsubsection*{Contribution}

The alignment on concepts between large libraries of formal proofs has
never been attempted before.  Doing so requires a joint research
activity of a large number of European researchers.  Alignment based
services will significantly
facilitate access to the available body of formalized mathematical and
computer science knowledge.

\subsubsection*{Success}

One of the final products of this work is alignment aware services
that enable both searching and browsing modulo alignment as well as
automated translation of theorem proving objects and proofs.  However,
before such an ambitious goal can be achieved, many lessons must be
learned and several milestones must be achieved.
\begin{itemize}
\item A very precise definition of alignments should be given and a
  format (and an ontology) for describing and organizing alignments
  must be described.
\item Proof theoretical results relating different logical foundations
  must be obtained and an efficient algorithm for translating
  statements from one logical system to another must be implemented.
\item Alignment of basic mathematical objects (sets, functions,
  relations, numbers) must be ensured.
\item A method for automated detection of alignments must be devised,
  implemented and applied on a large corpus of formal libraries.
\item A case study of geometry must show that it is possible to align
  large formal theories developed in different theorem provers, based
  on different approaches (synthetic vs. analytic).
\end{itemize}

%%% Local Variables:
%%%   mode: latex
%%%   ispell-local-dictionary: "english"
%%% TeX-master: "propB"
%%% End:
